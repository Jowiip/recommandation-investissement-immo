{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d89235e-dccb-4410-87ba-504a6bfbc673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Vérification des fichiers DVF départementaux...\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_75.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_77.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_78.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_91.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_92.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_93.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_94.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dvf_95.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "\n",
      "🔍 Vérification des bases complémentaires...\n",
      "\n",
      "🔹 INSEE - Données socio-économiques\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/insee_dossier_complet.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 DPE - Diagnostic énergétique\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/dpe_logement.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Airparif - Qualité de l’air\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/air_parif_communes.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Bruitparif - Niveau sonore\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/bruitsparifs_communes.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Encadrement des loyers (Paris)\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/encadrement_des_loyers_paris.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Délinquance - Sécurité\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/delinquance_communes.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Transports - Île-de-France\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/transports_idf.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Éducation - Annuaire établissements\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/fr-en-annuaire-education.csv\n",
      "❌ Fichier introuvable.\n",
      "\n",
      "🔹 Éducation - Principaux établissements\n",
      "----------------------------------------------------\n",
      "📄 Test du fichier : data/fr-en-principaux-etablissement.csv\n",
      "❌ Fichier introuvable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Fonction utilitaire ---\n",
    "def test_import(nom_fichier, sep=',', nrows=5):\n",
    "    \"\"\"\n",
    "    Teste le chargement d'un fichier CSV :\n",
    "    - Vérifie s'il existe\n",
    "    - Essaie de lire les premières lignes\n",
    "    - Affiche la taille, les colonnes, et un extrait\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(f\"📄 Test du fichier : {nom_fichier}\")\n",
    "\n",
    "    if not os.path.exists(nom_fichier):\n",
    "        print(\"❌ Fichier introuvable.\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(nom_fichier, sep=sep, nrows=nrows, low_memory=False)\n",
    "        print(f\"✅ Chargement réussi ({df.shape[0]} lignes × {df.shape[1]} colonnes)\")\n",
    "        print(\"   ➜ Colonnes :\", list(df.columns[:10]))\n",
    "        print(df.head(2), \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur lors du chargement : {e}\\n\")\n",
    "\n",
    "\n",
    "dvf_files = [f\"data/dvf_{dep}.csv\" for dep in [\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"]]\n",
    "\n",
    "print(\"🔍 Vérification des fichiers DVF départementaux...\\n\")\n",
    "for fichier in dvf_files:\n",
    "    test_import(fichier, sep=\",\")\n",
    "\n",
    "autres_bases = [\n",
    "    {\"nom\": \"INSEE - Données socio-économiques\", \"path\": \"data/insee_dossier_complet.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"DPE - Diagnostic énergétique\", \"path\": \"data/dpe_logement.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"Airparif - Qualité de l’air\", \"path\": \"data/air_parif_communes.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Bruitparif - Niveau sonore\", \"path\": \"data/bruitsparifs_communes.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Encadrement des loyers (Paris)\", \"path\": \"data/encadrement_des_loyers_paris.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"Délinquance - Sécurité\", \"path\": \"data/delinquance_communes.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"Transports - Île-de-France\", \"path\": \"data/transports_idf.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Éducation - Annuaire établissements\", \"path\": \"data/fr-en-annuaire-education.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Éducation - Principaux établissements\", \"path\": \"data/fr-en-principaux-etablissement.csv\", \"sep\": \";\"}\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Vérification des bases complémentaires...\\n\")\n",
    "for base in autres_bases:\n",
    "    print(f\"🔹 {base['nom']}\")\n",
    "    test_import(base[\"path\"], sep=base[\"sep\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0b50a-5e73-4fd4-9ed0-e77e6d46f7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fichiers DVF trouvés : 7\n",
      "   - data/clean/dvf_75_clean.csv\n",
      "   - data/clean/dvf_77_clean.csv\n",
      "   - data/clean/dvf_91_clean.csv\n",
      "   - data/clean/dvf_92_clean.csv\n",
      "   - data/clean/dvf_93_clean.csv\n",
      "   - data/clean/dvf_94_clean.csv\n",
      "   - data/clean/dvf_95_clean.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Où écrire le fichier fusionné ?\n",
    "# Choisis l'un des deux chemins de sortie (dé-commente celui que tu veux garder) :\n",
    "# out_path = Path(\"data/raw/dvf_idf.csv\")   # si tu as ce dossier\n",
    "out_path = Path(\"dvf_idf.csv\")              # à la racine du projet (même dossier que tes dvf_75.csv, etc.)\n",
    "\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Trouver automatiquement les fichiers DVF départementaux\n",
    "here = Path(\".\")\n",
    "dvf_files = list(here.glob(\"dvf_*.csv\"))\n",
    "if not dvf_files:\n",
    "    # si rien au niveau racine, on cherche en profondeur (sous-dossiers)\n",
    "    dvf_files = list(here.rglob(\"dvf_*.csv\"))\n",
    "\n",
    "print(\"📂 Fichiers DVF trouvés :\", len(dvf_files))\n",
    "for f in dvf_files:\n",
    "    print(\"   -\", f.relative_to(here))\n",
    "\n",
    "if not dvf_files:\n",
    "    raise FileNotFoundError(\"Aucun fichier 'dvf_*.csv' trouvé. Vérifie les noms/chemins.\")\n",
    "\n",
    "# 3) Lecture robuste (encodage + séparateur)\n",
    "def robust_read(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\";\", encoding=\"latin-1\", low_memory=False)\n",
    "        except Exception:\n",
    "            # dernier essai: on tente séparateur virgule\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "            except Exception:\n",
    "                return pd.read_csv(path, sep=\",\", encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "dfs = []\n",
    "for f in dvf_files:\n",
    "    df = robust_read(f)\n",
    "    # Normalisation minimale utile pour DVF\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "# 4) Fusion\n",
    "dvf_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"✅ Fusion réalisée :\", dvf_all.shape)\n",
    "\n",
    "# 5) Sauvegarde\n",
    "dvf_all.to_csv(out_path, index=False)\n",
    "print(\"💾 Fichier fusionné sauvegardé sous :\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af704c13-2614-4f48-aef5-2fce8f4629ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"🔍 Test air_parif_communes.csv\")\n",
    "\n",
    "try:\n",
    "    df_air = pd.read_csv(\n",
    "        \"data/air_parif_communes.csv\",\n",
    "        sep=',',\n",
    "        encoding='utf-8',\n",
    "        engine='python'\n",
    "    )\n",
    "    print(\"✅ Chargement réussi :\", df_air.shape)\n",
    "    display(df_air.head(5))\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Erreur UTF-8 :\", e)\n",
    "    print(\"🔁 Nouvelle tentative avec encodage latin-1 et séparateur ';'\")\n",
    "    try:\n",
    "        df_air = pd.read_csv(\n",
    "            \"data/air_parif_communes.csv\",\n",
    "            sep=';',\n",
    "            encoding='latin-1',\n",
    "            engine='python'\n",
    "        )\n",
    "        print(\"✅ Chargement réussi (latin-1) :\", df_air.shape)\n",
    "        display(df_air.head(5))\n",
    "    except Exception as e2:\n",
    "        print(\"❌ Toujours erreur :\", e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee13506-bc05-48fc-bd11-8b3bdcb70032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 1 : SETUP + VERIFICATIONS AUTOMATIQUES ===\n",
    "# Objectif : vérifier l'environnement, préparer les dossiers, détecter les CSV, et tester une lecture légère\n",
    "\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"✅ {pkg} déjà installé\")\n",
    "    except ImportError:\n",
    "        print(f\"⏳ Installation de {pkg}…\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        print(f\"✅ {pkg} installé\")\n",
    "\n",
    "# 1) Dépendances minimales\n",
    "for p in [\"pandas\", \"numpy\"]:\n",
    "    ensure(p)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "print(\"\\n📦 Versions\")\n",
    "print(\" - Python :\", sys.version.split()[0])\n",
    "print(\" - pandas :\", pd.__version__)\n",
    "print(\" - numpy  :\", np.__version__)\n",
    "\n",
    "# 2) Dossiers projet\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n📁 Dossiers\")\n",
    "print(\" - Projet :\", BASE_DIR)\n",
    "print(\" - Données :\", DATA_DIR)\n",
    "print(\" - Nettoyés:\", CLEAN_DIR)\n",
    "\n",
    "# 3) Détection automatique de TOUS les CSV (sauf ceux déjà nettoyés)\n",
    "FILES = {\n",
    "    f.stem.replace(\"-\", \"_\"): f\n",
    "    for f in DATA_DIR.glob(\"*.csv\")\n",
    "    if not str(f).startswith(str(CLEAN_DIR))\n",
    "}\n",
    "\n",
    "print(\"\\n🔎 CSV détectés dans 'data/' (hors 'data/clean/'):\")\n",
    "if not FILES:\n",
    "    print(\"   ⚠️ Aucun fichier .csv trouvé dans data/. Place tes fichiers ici, puis relance cette cellule.\")\n",
    "else:\n",
    "    for k, p in sorted(FILES.items()):\n",
    "        print(f\"   • {k:35s} → {p.name}\")\n",
    "\n",
    "# 4) Mini test de lecture (robuste) sur chaque CSV : 2 lignes max\n",
    "def test_preview(path: Path):\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\"]\n",
    "    seps = [\";\", \",\", \"\\t\", \"|\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, encoding=enc, nrows=2, )\n",
    "                # Heuristique anti faux-positif : si 1 colonne unique très longue -> mauvais séparateur\n",
    "                if df.shape[1] == 1:\n",
    "                    continue\n",
    "                return True, enc, sep, df.columns.tolist()[:6]\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "    return False, None, None, str(last_err)\n",
    "\n",
    "print(\"\\n🧪 Test de lecture rapide (2 lignes) :\")\n",
    "if FILES:\n",
    "    ok_all = True\n",
    "    for k, p in sorted(FILES.items()):\n",
    "        ok, enc, sep, info = test_preview(p)\n",
    "        if ok:\n",
    "            print(f\"   ✅ {p.name:35s} | enc='{enc}', sep='{sep}' | colonnes: {info}\")\n",
    "        else:\n",
    "            ok_all = False\n",
    "            print(f\"   ❌ {p.name:35s} | lecture impossible (dernier message: {info})\")\n",
    "    if ok_all:\n",
    "        print(\"\\n🎉 ETAPE 1 OK : environnement prêt, dossiers en place, CSV détectés et lisibles.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ ETAPE 1 PARTIELLE : certains fichiers ne se lisent pas en preview. On pourra les traiter au cas par cas au nettoyage.\")\n",
    "else:\n",
    "    print(\"   (aucun fichier à tester)\")\n",
    "\n",
    "# 5) Expose les variables pour les étapes suivantes\n",
    "globals()[\"DATA_DIR\"] = DATA_DIR\n",
    "globals()[\"CLEAN_DIR\"] = CLEAN_DIR\n",
    "globals()[\"FILES\"] = FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a950e-64f2-41dc-bf63-b3312ff5a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 2 : DÉCLARATION AUTOMATIQUE DES FICHIERS + SCHÉMAS DE NETTOYAGE ===\n",
    "# Objectif : identifier tous les fichiers de données dans \"data/\", définir des schémas adaptés,\n",
    "# et préparer le pipeline pour le nettoyage (étape 4).\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Réutilisation des dossiers de l'étape 1\n",
    "DATA_RAW = DATA_DIR\n",
    "DATA_CLEAN = CLEAN_DIR\n",
    "\n",
    "# === 1️⃣ Détection automatique des fichiers CSV ===\n",
    "FILES = {\n",
    "    f.stem.replace(\"-\", \"_\"): f\n",
    "    for f in DATA_RAW.glob(\"*.csv\")\n",
    "    if not str(f).startswith(str(DATA_CLEAN))  # on ignore les fichiers déjà nettoyés\n",
    "}\n",
    "\n",
    "print(\"📂 Fichiers détectés automatiquement :\")\n",
    "for key, path in sorted(FILES.items()):\n",
    "    print(f\"   • {key:30s} → {path.name}\")\n",
    "print(f\"\\nTotal : {len(FILES)} fichiers détectés dans {DATA_RAW}\\n\")\n",
    "\n",
    "\n",
    "# === 2️⃣ Schémas personnalisés pour certaines bases connues ===\n",
    "SCHEMAS_CUSTOM = {\n",
    "    \"dvf_idf\": {\n",
    "        \"rename\": {\n",
    "            \"valeur_fonciere\": \"valeur_fonciere\",\n",
    "            \"surface_reelle_bati\": \"surface_reelle_bati\",\n",
    "            \"type_local\": \"type_local\",\n",
    "            \"nombre_pieces_principales\": \"nb_pieces\",\n",
    "            \"date_mutation\": \"date_mutation\",\n",
    "            \"commune\": \"commune\",\n",
    "        },\n",
    "        \"dtype\": {\n",
    "            \"valeur_fonciere\": \"float\",\n",
    "            \"surface_reelle_bati\": \"float\",\n",
    "            \"nb_pieces\": \"Int64\",\n",
    "        },\n",
    "        \"parse_dates\": [\"date_mutation\"],\n",
    "        \"drop_dupes_on\": [\"code_commune\", \"date_mutation\", \"valeur_fonciere\", \"surface_reelle_bati\"],\n",
    "    },\n",
    "    \"delinquance_communes\": {\n",
    "        \"rename\": {\n",
    "            \"faits_total\": \"faits_total\",\n",
    "            \"population\": \"population\",\n",
    "            \"annee\": \"annee\",\n",
    "            \"commune\": \"commune\",\n",
    "            \"taux_criminalite\": \"taux_criminalite\",\n",
    "        },\n",
    "        \"dtype\": {\n",
    "            \"faits_total\": \"Int64\",\n",
    "            \"population\": \"Int64\",\n",
    "            \"annee\": \"Int64\",\n",
    "        },\n",
    "        \"drop_dupes_on\": [\"code_commune\", \"annee\"],\n",
    "    },\n",
    "    \"air_parif_communes\": {\n",
    "        \"rename\": {\"indice_airparif\": \"indice_airparif\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {\"indice_airparif\": \"float\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"dpe_logement\": {\n",
    "        \"rename\": {\"date_visite_diagnostiqueur\": \"date_dpe\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"insee_dossier_complet\": {\n",
    "        \"rename\": {\n",
    "            \"revenu_median\": \"revenu_median\",\n",
    "            \"taux_chomage\": \"taux_chomage\",\n",
    "            \"population\": \"population\",\n",
    "            \"commune\": \"commune\",\n",
    "        },\n",
    "        \"dtype\": {\"revenu_median\": \"float\", \"taux_chomage\": \"float\", \"population\": \"Int64\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"transports_idf\": {\n",
    "        \"rename\": {\"nb_arrets\": \"nb_arrets\", \"score_connectivite\": \"score_connectivite\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {\"nb_arrets\": \"Int64\", \"score_connectivite\": \"float\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# === 3️⃣ Schéma par défaut pour toutes les autres bases ===\n",
    "DEFAULT_SCHEMA = {\n",
    "    \"rename\": {},\n",
    "    \"dtype\": {},\n",
    "    \"drop_dupes_on\": [\"code_commune\"],\n",
    "}\n",
    "\n",
    "# === 4️⃣ Génération automatique des schémas ===\n",
    "SCHEMAS = {}\n",
    "for key in FILES.keys():\n",
    "    SCHEMAS[key] = SCHEMAS_CUSTOM.get(key, DEFAULT_SCHEMA)\n",
    "\n",
    "print(\"📘 Schémas générés :\")\n",
    "for k in sorted(SCHEMAS.keys()):\n",
    "    base_type = \"🎯 personnalisé\" if k in SCHEMAS_CUSTOM else \"⚙️  générique\"\n",
    "    print(f\"   {k:30s} → {base_type}\")\n",
    "print(f\"\\nTotal : {len(SCHEMAS)} schémas chargés\\n\")\n",
    "\n",
    "\n",
    "# === 5️⃣ Vérification rapide de cohérence ===\n",
    "missing = [k for k in FILES if k not in SCHEMAS]\n",
    "if missing:\n",
    "    print(\"⚠️ Bases sans schéma associé :\", missing)\n",
    "else:\n",
    "    print(\"✅ Toutes les bases détectées ont un schéma associé (automatique ou personnalisé).\")\n",
    "\n",
    "# --- Expose les variables pour les étapes suivantes\n",
    "globals()[\"FILES\"] = FILES\n",
    "globals()[\"SCHEMAS\"] = SCHEMAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b676b-1654-4236-95b3-9490c6784d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 3 : NETTOYAGE AUTOMATIQUE ULTRA OPTIMISÉ (GROS FICHIERS) ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import csv, time, sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "CHUNK_SIZE = 30000  # plus petit = plus fluide\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "print(\"🚀 Mode optimisation activé : lecture par petits paquets + écriture directe\\n\")\n",
    "\n",
    "def normalize_colnames(cols):\n",
    "    def _norm(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        s = s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "        s = \"_\".join([token for token in s.split(\"_\") if token])\n",
    "        return s\n",
    "    return [_norm(c) for c in cols]\n",
    "\n",
    "def normalize_commune_name(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    df = df.copy()\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c or \"nom\" == c), None)\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def detect_separator(path):\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            sample = f.read(2048)\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "        if sep not in [\";\", \",\", \"\\t\", \"|\"]:\n",
    "            sep = \";\"\n",
    "        return sep\n",
    "    except Exception:\n",
    "        return \";\"\n",
    "\n",
    "def clean_one_lightweight(key, path, schema):\n",
    "    print(f\"\\n🧩 === {key.upper()} ===\")\n",
    "    print(f\"📄 Lecture du fichier : {path.name}\")\n",
    "\n",
    "    sep = detect_separator(path)\n",
    "    out_path = DATA_CLEAN / f\"{key}_clean.csv\"\n",
    "    first_chunk = True\n",
    "    total_lines = 0\n",
    "    start = time.time()\n",
    "\n",
    "    try:\n",
    "        reader = pd.read_csv(\n",
    "            path, sep=sep, encoding=\"utf-8\", chunksize=CHUNK_SIZE,\n",
    "            engine=\"python\", on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "        for chunk in tqdm(reader, desc=f\"{key}\", unit=\"chunk\"):\n",
    "            total_lines += len(chunk)\n",
    "            chunk.columns = normalize_colnames(chunk.columns)\n",
    "            chunk = add_commune_keys(chunk)\n",
    "\n",
    "            rename_map = {old: new for old, new in schema.get(\"rename\", {}).items() if old in chunk.columns}\n",
    "            chunk = chunk.rename(columns=rename_map)\n",
    "\n",
    "            for col, dtype in schema.get(\"dtype\", {}).items():\n",
    "                if col in chunk.columns:\n",
    "                    try:\n",
    "                        if dtype == \"Int64\":\n",
    "                            chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\").astype(\"Int64\")\n",
    "                        elif dtype == \"float\":\n",
    "                            chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            chunk.to_csv(out_path, index=False, mode=\"a\", header=first_chunk)\n",
    "            first_chunk = False\n",
    "\n",
    "        elapsed = round(time.time() - start, 2)\n",
    "        print(f\"✅ {key} terminé ({total_lines:,} lignes) en {elapsed}s\")\n",
    "        return {\"base\": key, \"lignes\": total_lines, \"fichier\": str(out_path)}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur sur {key}: {e}\")\n",
    "        return {\"base\": key, \"lignes\": 0, \"fichier\": str(path), \"erreur\": str(e)}\n",
    "\n",
    "\n",
    "# --- Boucle principale ---\n",
    "results = []\n",
    "for key, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        print(f\"⚠️ Fichier manquant : {path}\")\n",
    "        continue\n",
    "    schema = SCHEMAS.get(key, {\"rename\": {}, \"dtype\": {}})\n",
    "    res = clean_one_lightweight(key, path, schema)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Résumé global ---\n",
    "print(\"\\n📊 === RÉSUMÉ GLOBAL DU NETTOYAGE ===\")\n",
    "summary = pd.DataFrame(results)\n",
    "display(summary)\n",
    "\n",
    "print(\"\\n✅ Étape 3 terminée (mode léger). Aucune surcharge mémoire détectée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6764e73-5d14-4072-a391-8271fde82dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 4 : NETTOYAGE COMPLET ET CONTRÔLÉ ===\n",
    "# Objectif : lire, uniformiser, nettoyer et sauvegarder toutes les bases de données.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Dossiers\n",
    "DATA_RAW = Path(\"data\")         # 🔧 fichiers sources\n",
    "DATA_CLEAN = DATA_RAW / \"clean\" # 🔧 fichiers nettoyés\n",
    "DATA_CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Nettoyage du dossier clean avant traitement\n",
    "if DATA_CLEAN.exists():\n",
    "    print(\"🧽 Nettoyage du dossier 'data/clean'...\")\n",
    "    for f in DATA_CLEAN.glob(\"*.csv\"):\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Impossible de supprimer {f.name} : {e}\")\n",
    "\n",
    "# --- Fichiers à traiter\n",
    "FILES = {\n",
    "    \"dvf\": DATA_RAW / \"dvf_idf.csv\",\n",
    "    \"delinquance\": DATA_RAW / \"delinquance_communes.csv\",\n",
    "    \"air\": DATA_RAW / \"air_parif_communes.csv\",\n",
    "    \"encadrement\": DATA_RAW / \"encadrement_loyers_idf.csv\",\n",
    "    \"insee\": DATA_RAW / \"insee_dossier.csv\",\n",
    "    \"dpe\": DATA_RAW / \"dpe_logements.csv\",\n",
    "    \"transport\": DATA_RAW / \"transports_idf.csv\",\n",
    "    \"rne\": DATA_RAW / \"fr-en-annuaire-education.csv\",\n",
    "}\n",
    "\n",
    "print(\"📂 Vérification des fichiers présents :\")\n",
    "for key, path in FILES.items():\n",
    "    print(f\"{key:12s} → {path} {'✅' if path.exists() else '❌'}\")\n",
    "\n",
    "# --- Fonctions utilitaires\n",
    "def normalize_colnames(cols):\n",
    "    \"\"\"Uniformise les noms de colonnes (minuscules, sans accents, snake_case).\"\"\"\n",
    "    def _norm(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        s = s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "        s = \"_\".join([token for token in s.split(\"_\") if token])\n",
    "        return s\n",
    "    return [_norm(c) for c in cols]\n",
    "\n",
    "def normalize_commune_name(s):\n",
    "    \"\"\"Normalise les noms de communes pour éviter les erreurs de casse ou d'accents.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    \"\"\"Crée les colonnes code_commune et commune_std pour les jointures futures.\"\"\"\n",
    "    df = df.copy()\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c or c == \"nom\"), None)\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def remove_empty_columns(df):\n",
    "    \"\"\"Supprime les colonnes entièrement vides.\"\"\"\n",
    "    before = df.shape[1]\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    after = df.shape[1]\n",
    "    if before != after:\n",
    "        print(f\"🧹 {before - after} colonnes entièrement vides supprimées\")\n",
    "    return df\n",
    "\n",
    "def add_paris_mapping_if_missing(df, key):\n",
    "    \"\"\"Ajoute les codes communes manquants pour les fichiers BruitParif/Paris.\"\"\"\n",
    "    if key not in [\"bruitsparif\", \"bruitsparifs\", \"bruitsparifs_communes\"]:\n",
    "        return df\n",
    "    if \"code_commune\" in df.columns and df[\"code_commune\"].notna().sum() > 0:\n",
    "        return df\n",
    "\n",
    "    paris_mapping = {\n",
    "        \"1er\": 75101, \"2e\": 75102, \"3e\": 75103, \"4e\": 75104, \"5e\": 75105, \"6e\": 75106,\n",
    "        \"7e\": 75107, \"8e\": 75108, \"9e\": 75109, \"10e\": 75110, \"11e\": 75111, \"12e\": 75112,\n",
    "        \"13e\": 75113, \"14e\": 75114, \"15e\": 75115, \"16e\": 75116, \"17e\": 75117,\n",
    "        \"18e\": 75118, \"19e\": 75119, \"20e\": 75120\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].astype(str).str.contains(\"arrondissement|paris\", case=False).any():\n",
    "            df[\"code_commune\"] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.extract(r\"(\\d+)\")\n",
    "                .astype(float)\n",
    "                .astype(\"Int64\")\n",
    "                .map(paris_mapping)\n",
    "            )\n",
    "            df[\"commune_std\"] = \"PARIS\"\n",
    "            print(\"🏙️ Codes communes ajoutés via mapping Paris\")\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# --- Schémas de nettoyage\n",
    "SCHEMAS = {\n",
    "    \"dvf\": {\"rename\": {\"valeur_fonciere\": \"valeur_fonciere\", \"surface_reelle_bati\": \"surface_reelle_bati\",\n",
    "                       \"type_local\": \"type_local\", \"nombre_pieces_principales\": \"nb_pieces\",\n",
    "                       \"date_mutation\": \"date_mutation\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"valeur_fonciere\": \"float\", \"surface_reelle_bati\": \"float\", \"nb_pieces\": \"Int64\"},\n",
    "            \"parse_dates\": [\"date_mutation\"],\n",
    "            \"drop_dupes_on\": [\"code_commune\", \"date_mutation\", \"valeur_fonciere\", \"surface_reelle_bati\"]},\n",
    "    \"delinquance\": {\"rename\": {\"faits_total\": \"faits_total\", \"population\": \"population\", \"annee\": \"annee\",\n",
    "                               \"commune\": \"commune\", \"taux_criminalite\": \"taux_criminalite\"},\n",
    "                    \"dtype\": {\"faits_total\": \"Int64\", \"population\": \"Int64\", \"annee\": \"Int64\"},\n",
    "                    \"drop_dupes_on\": [\"code_commune\", \"annee\"]},\n",
    "    \"air\": {\"rename\": {\"indice_airparif\": \"indice_airparif\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"indice_airparif\": \"float\"},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"encadrement\": {\"rename\": {\"zone_encadree\": \"zone_encadree\", \"loyer_ref\": \"loyer_ref\",\n",
    "                               \"loyer_majoré\": \"loyer_majore\", \"commune\": \"commune\"},\n",
    "                    \"dtype\": {},\n",
    "                    \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"insee\": {\"rename\": {\"revenu_median\": \"revenu_median\", \"taux_chomage\": \"taux_chomage\",\n",
    "                         \"pop_18_29\": \"pop_18_29\", \"population\": \"population\",\n",
    "                         \"logements_vacants\": \"logements_vacants\", \"logements_totaux\": \"logements_totaux\",\n",
    "                         \"commune\": \"commune\"},\n",
    "              \"dtype\": {\"revenu_median\": \"float\", \"taux_chomage\": \"float\", \"pop_18_29\": \"Int64\",\n",
    "                        \"population\": \"Int64\", \"logements_vacants\": \"Int64\", \"logements_totaux\": \"Int64\"},\n",
    "              \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"dpe\": {\"rename\": {\"nb_a\": \"nb_a\", \"nb_b\": \"nb_b\", \"nb_c\": \"nb_c\", \"nb_d\": \"nb_d\", \"nb_e\": \"nb_e\",\n",
    "                       \"nb_f\": \"nb_f\", \"nb_g\": \"nb_g\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {k: \"Int64\" for k in [\"nb_a\", \"nb_b\", \"nb_c\", \"nb_d\", \"nb_e\", \"nb_f\", \"nb_g\"]},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"transport\": {\"rename\": {\"nb_arrets\": \"nb_arrets\", \"score_connectivite\": \"score_connectivite\", \"commune\": \"commune\"},\n",
    "                  \"dtype\": {\"nb_arrets\": \"Int64\", \"score_connectivite\": \"float\"},\n",
    "                  \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"rne\": {\"rename\": {\"nb_etabs_sup\": \"nb_etabs_sup\", \"superficie_km2\": \"superficie_km2\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"nb_etabs_sup\": \"Int64\", \"superficie_km2\": \"float\"},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "}\n",
    "\n",
    "# --- Fonction principale de nettoyage\n",
    "def clean_one(key, path, schema):\n",
    "    print(f\"\\n🧩 === {key.upper()} ===\")\n",
    "    print(f\"📄 Lecture du fichier : {path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, sep=\";\", encoding=\"latin-1\", low_memory=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "    print(f\"   ➤ {df.shape[0]} lignes, {df.shape[1]} colonnes avant nettoyage\")\n",
    "\n",
    "    # Normalisation et renommage\n",
    "    df.columns = normalize_colnames(df.columns)\n",
    "    rename_map = {old: new for old, new in schema.get(\"rename\", {}).items() if old in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Ajout des clés communes et suppression des colonnes vides\n",
    "    df = add_commune_keys(df)\n",
    "    df = remove_empty_columns(df)\n",
    "    df = add_paris_mapping_if_missing(df, key)\n",
    "\n",
    "    # Typage\n",
    "    for col, dtype in schema.get(\"dtype\", {}).items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == \"Int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "                elif dtype == \"float\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Typage impossible pour {col}: {e}\")\n",
    "\n",
    "    # Suppression des doublons\n",
    "    subset = [c for c in schema.get(\"drop_dupes_on\", []) if c in df.columns]\n",
    "    if subset:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        print(f\"🧹 {before - len(df)} doublons supprimés sur {subset}\")\n",
    "\n",
    "    # Export sécurisé\n",
    "    out = DATA_CLEAN / f\"{key}_clean.csv\"\n",
    "    try:\n",
    "        df.to_csv(out, index=False, encoding=\"utf-8\")\n",
    "        print(f\"✅ Exporté vers {out} ({df.shape[0]} lignes, {df.shape[1]} colonnes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l’export de {out} : {e}\")\n",
    "\n",
    "    return {\"base\": key, \"lignes\": df.shape[0], \"colonnes\": df.shape[1], \"fichier\": str(out)}\n",
    "\n",
    "# --- Boucle principale de traitement\n",
    "results = []\n",
    "for key, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        print(f\"❌ Fichier manquant : {path}\")\n",
    "        continue\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.read(1024)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Fichier {path.name} inaccessible : {e}\")\n",
    "        continue\n",
    "\n",
    "    schema = SCHEMAS.get(key, {\"rename\": {}, \"dtype\": {}})\n",
    "    res = clean_one(key, path, schema)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Résumé global\n",
    "print(\"\\n📊 === RÉSUMÉ GLOBAL DU NETTOYAGE ===\")\n",
    "if results:\n",
    "    summary = pd.DataFrame(results)\n",
    "    summary[\"taille_Ko\"] = summary[\"fichier\"].apply(lambda x: round(os.path.getsize(x) / 1024, 1))\n",
    "    display(summary)\n",
    "    summary.to_csv(DATA_CLEAN / \"rapport_validation.csv\", index=False)\n",
    "    print(f\"📝 Rapport sauvegardé dans {DATA_CLEAN / 'rapport_validation.csv'}\")\n",
    "else:\n",
    "    print(\"⚠️ Aucun fichier n’a été traité.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcce6f-c74b-4e28-9fe8-e72c023eece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import unicodedata, csv\n",
    "\n",
    "DATA_RAW   = Path(\"data\")          # dvf_75.csv, dvf_77.csv, etc.\n",
    "DATA_CLEAN = Path(\"data/clean\")\n",
    "DATA_CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_colnames(cols):\n",
    "    def _n(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        return s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    return [_n(c) for c in cols]\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    df = df.copy()\n",
    "    if \"code_commune\" in df.columns:\n",
    "        df[\"code_commune\"] = (\n",
    "            df[\"code_commune\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "        )\n",
    "    if \"nom_commune\" in df.columns:\n",
    "        s = df[\"nom_commune\"].astype(str)\n",
    "        s = s.str.normalize(\"NFKD\").str.encode(\"ascii\",\"ignore\").str.decode(\"ascii\")\n",
    "        s = s.str.upper().str.replace(\"-\", \" \", regex=False).str.replace(\"'\", \" \", regex=False)\n",
    "        df[\"commune_std\"] = s.str.split().str.join(\" \")\n",
    "    return df\n",
    "\n",
    "def read_dvf_robuste(src):\n",
    "    try:\n",
    "        return pd.read_csv(src, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            src, sep=\",\", encoding=\"utf-8\",\n",
    "            engine=\"python\", quotechar='\"', on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "# DVF bruts à traiter\n",
    "raw_dvf = sorted([p for p in DATA_RAW.glob(\"dvf_*.csv\") if \"_clean\" not in p.name])\n",
    "print(f\"📂 DVF bruts détectés ({len(raw_dvf)}):\")\n",
    "for p in raw_dvf: print(\"  -\", p.name)\n",
    "\n",
    "for src in raw_dvf:\n",
    "    dest = DATA_CLEAN / (src.stem + \"_clean.csv\")\n",
    "    print(f\"\\n🧩 {src.name} → {dest.name}\")\n",
    "\n",
    "    df = read_dvf_robuste(src)\n",
    "    df.columns = normalize_colnames(df.columns)\n",
    "    df = add_commune_keys(df)\n",
    "\n",
    "    # ✅ Correction : lineterminator (sans underscore)\n",
    "    df.to_csv(\n",
    "        dest, index=False, encoding=\"utf-8\",\n",
    "        sep=\",\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    chk = pd.read_csv(dest, sep=\",\", nrows=3)\n",
    "    print(f\"   ✅ OK : {chk.shape[1]} colonnes | Aperçu colonnes : {list(chk.columns)[:6]}\")\n",
    "\n",
    "print(\"\\n🎉 Recréation des DVF _clean terminée (fichiers sains et sans guillemets parasites).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cfc766-f6d3-4ed0-a409-5dbee840f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# === Fonction pour détecter automatiquement le séparateur ===\n",
    "def detect_separator(path, sample_size=4096):\n",
    "    \"\"\"Détecte le séparateur le plus probable dans un fichier CSV.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(sample_size)\n",
    "        sniffer = csv.Sniffer()\n",
    "        sep = sniffer.sniff(sample).delimiter\n",
    "        if sep not in [\",\", \";\", \"\\t\", \"|\"]:\n",
    "            sep = \";\"\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "\n",
    "# === Fonction principale de visualisation ===\n",
    "def check_all_clean_files(folder=\"data/clean\", nrows=5, max_cols=10):\n",
    "    \"\"\"Affiche un aperçu des fichiers CSV nettoyés (séparateur, colonnes, aperçu des données).\"\"\"\n",
    "    clean_dir = Path(folder)\n",
    "    clean_files = list(clean_dir.glob(\"*_clean.csv\"))\n",
    "\n",
    "    if not clean_files:\n",
    "        raise FileNotFoundError(f\"⚠️ Aucun fichier '_clean.csv' trouvé dans {folder}/\")\n",
    "\n",
    "    print(f\"📂 {len(clean_files)} fichiers trouvés dans {folder}/\\n\")\n",
    "\n",
    "    for f in clean_files:\n",
    "        sep = detect_separator(f)\n",
    "        print(f\"=== {f.name} ===\")\n",
    "        print(f\"   🔹 Séparateur détecté : '{sep}'\")\n",
    "\n",
    "        try:\n",
    "            # Lecture de l'en-tête pour connaître le nombre total de colonnes\n",
    "            header = pd.read_csv(f, sep=sep, nrows=0, encoding=\"utf-8\", low_memory=False)\n",
    "            nb_cols = len(header.columns)\n",
    "\n",
    "            # Lecture partielle : jusqu’à 10 colonnes maximum, ou moins si le fichier en a moins\n",
    "            cols_to_read = list(range(min(max_cols, nb_cols)))\n",
    "            df = pd.read_csv(f, sep=sep, nrows=nrows, usecols=cols_to_read, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "            print(f\"   🔹 Colonnes affichées : {len(df.columns)} sur {nb_cols} totales\")\n",
    "            display(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erreur de lecture sur {f.name}: {e}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "\n",
    "# === Appel de la fonction ===\n",
    "check_all_clean_files(\"data/clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926801-077d-4553-a5f6-ca06b7458ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ac28f-d0ee-449e-83ba-8b68d163a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 5 : TEST DU NETTOYAGE DES BASES DE DONNÉES ===\n",
    "# Objectif : vérifier la cohérence, la propreté et la structure de chaque base clean\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Dossier contenant les bases nettoyées\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Liste des fichiers à vérifier\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonctions utilitaires\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def detect_separator(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def test_clean_file(path):\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    sep = detect_separator(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture : {e}\\n\")\n",
    "        return None\n",
    "\n",
    "    print(f\"🔹 Lignes : {len(df)}, Colonnes : {len(df.columns)}, Séparateur : '{sep}'\")\n",
    "\n",
    "    # 1️⃣ Harmonisation de format\n",
    "    if \"code_commune\" in df.columns:\n",
    "        df[\"code_commune\"] = df[\"code_commune\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    if \"commune\" in df.columns:\n",
    "        df[\"commune\"] = df[\"commune\"].apply(normalize_text)\n",
    "\n",
    "    # 2️⃣ Doublons\n",
    "    dups = df.duplicated().sum()\n",
    "    print(f\"   🔁 Doublons détectés : {dups}\")\n",
    "\n",
    "    # 3️⃣ Valeurs manquantes\n",
    "    missing = df.isna().mean().round(2)\n",
    "    top_missing = missing[missing > 0].sort_values(ascending=False).head(5)\n",
    "    if not top_missing.empty:\n",
    "        print(\"   ⚠️ Colonnes avec NaN :\", dict(top_missing))\n",
    "    else:\n",
    "        print(\"   ✅ Aucune valeur manquante significative\")\n",
    "\n",
    "    # 4️⃣ Uniformisation noms colonnes\n",
    "    normalized_cols = [unicodedata.normalize(\"NFKD\", c).encode(\"ascii\", \"ignore\").decode(\"utf-8\").lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    if df.columns.tolist() != normalized_cols:\n",
    "        print(\"   ⚙️ Correction potentielle des noms de colonnes incohérents\")\n",
    "\n",
    "    # 5️⃣ Types de données\n",
    "    print(\"   📊 Types détectés :\")\n",
    "    print(df.dtypes.head())\n",
    "\n",
    "    # 6️⃣ Normalisation du texte\n",
    "    if \"commune\" in df.columns:\n",
    "        uniques = df[\"commune\"].nunique()\n",
    "        print(f\"   🧾 Communes uniques : {uniques}\")\n",
    "\n",
    "    # 7️⃣ Caractères parasites\n",
    "    example_str = df.select_dtypes(include=\"object\").astype(str).apply(lambda x: x.str.contains(\"[€,$,\\t,;]\", regex=True)).any()\n",
    "    if example_str.any():\n",
    "        print(\"   ⚠️ Caractères parasites détectés dans certaines colonnes texte\")\n",
    "\n",
    "    # 8️⃣ Clés communes\n",
    "    if \"code_commune\" in df.columns:\n",
    "        valid_keys = df[\"code_commune\"].notna().sum()\n",
    "        print(f\"   🧩 Clé 'code_commune' présente ({valid_keys} valeurs valides)\")\n",
    "\n",
    "    # 9️⃣ Contrôle de cohérence simple (ex : surface > 0)\n",
    "    if \"surface_reelle_bati\" in df.columns:\n",
    "        negatives = (df[\"surface_reelle_bati\"] <= 0).sum()\n",
    "        if negatives > 0:\n",
    "            print(f\"   ⚠️ {negatives} valeurs de surface non valides (≤0)\")\n",
    "        else:\n",
    "            print(\"   ✅ Toutes les surfaces sont positives\")\n",
    "\n",
    "    if set([\"valeur_fonciere\", \"surface_reelle_bati\"]).issubset(df.columns):\n",
    "        df[\"prix_m2\"] = df[\"valeur_fonciere\"] / df[\"surface_reelle_bati\"]\n",
    "        mean_price = df[\"prix_m2\"].mean(skipna=True)\n",
    "        print(f\"   💶 Prix moyen estimé au m² : {round(mean_price,2)}\")\n",
    "\n",
    "    # 10️⃣ Aperçu visuel\n",
    "    print(\"\\n   🧾 Aperçu :\")\n",
    "    display(df.head(5).iloc[:, :min(8, len(df.columns))])\n",
    "    print(\"-\" * 120)\n",
    "    return df\n",
    "\n",
    "# --- Lancement du test sur toutes les bases clean\n",
    "for f in files:\n",
    "    test_clean_file(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ad7c7-bd09-4bca-9a04-cccda983cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 1 : DÉTECTION DES ERREURS DE FORMAT ===\n",
    "# Objectif : détecter les problèmes de lecture, encodage ou séparateur sur toutes les bases clean\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les bases clean\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Recherche de tous les fichiers .csv dans le dossier\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction de test de format\n",
    "def detect_format_issues(path):\n",
    "    report = {\"fichier\": path.name, \"ok\": True, \"erreur\": None, \"colonnes\": 0, \"lignes\": 0, \"sep\": None}\n",
    "\n",
    "    try:\n",
    "        # Détection automatique du séparateur\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(4096)\n",
    "        try:\n",
    "            sep = csv.Sniffer().sniff(sample).delimiter\n",
    "        except Exception:\n",
    "            sep = \";\"\n",
    "        report[\"sep\"] = sep\n",
    "\n",
    "        # Lecture test sur 100 premières lignes\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", nrows=100, low_memory=False)\n",
    "        report[\"colonnes\"] = df.shape[1]\n",
    "        report[\"lignes\"] = len(df)\n",
    "\n",
    "        # Test de cohérence de structure\n",
    "        if df.shape[1] < 3:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"erreur\"] = \"Trop peu de colonnes (mauvais séparateur ou structure)\"\n",
    "        elif df.shape[0] == 0:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"erreur\"] = \"Fichier vide ou mal encodé\"\n",
    "\n",
    "    except Exception as e:\n",
    "        report[\"ok\"] = False\n",
    "        report[\"erreur\"] = str(e)\n",
    "\n",
    "    return report\n",
    "\n",
    "# --- Lancement du test pour toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    print(f\"🔎 Vérification de {f.name} ...\")\n",
    "    res = detect_format_issues(f)\n",
    "    results.append(res)\n",
    "    if not res[\"ok\"]:\n",
    "        print(f\"   ⚠️ Erreur détectée : {res['erreur']}\")\n",
    "    else:\n",
    "        print(f\"   ✅ OK ({res['colonnes']} colonnes, {res['lignes']} lignes, sep='{res['sep']}')\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# --- Résumé global\n",
    "df_report = pd.DataFrame(results)\n",
    "print(\"\\n📊 === RÉSUMÉ DES ERREURS DE FORMAT ===\")\n",
    "display(df_report)\n",
    "\n",
    "# --- Statistiques générales\n",
    "total = len(df_report)\n",
    "ok = df_report[\"ok\"].sum()\n",
    "ko = total - ok\n",
    "\n",
    "print(f\"✅ Fichiers valides : {ok}/{total}\")\n",
    "print(f\"⚠️ Fichiers avec erreurs : {ko}/{total}\")\n",
    "\n",
    "if ko > 0:\n",
    "    print(\"\\n🧾 Liste des fichiers problématiques :\")\n",
    "    display(df_report.loc[df_report[\"ok\"] == False, [\"fichier\", \"erreur\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60cc24-5e93-4471-acca-1f6743d12b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NETTOYAGE GLOBAL + SUPPRESSION DES DOUBLONS (EN PLACE) ===\n",
    "# Objectif :\n",
    "# 1️⃣ Supprimer les fichiers temporaires dans data/clean/\n",
    "# 2️⃣ Supprimer les doublons dans tous les fichiers clean et réécrire directement le fichier\n",
    "# 3️⃣ Fournir un rapport complet et propre\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les bases nettoyées\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "print(\"🔍 Étape 1 : Nettoyage du dossier 'data/clean'...\\n\")\n",
    "\n",
    "# --- Étape 1 : supprimer les fichiers temporaires\n",
    "suffixes_a_supprimer = [\"_nodup.csv\", \"_fixed.csv\", \"_clean_fixed.csv\"]\n",
    "\n",
    "suppr = 0\n",
    "for f in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    if any(suffix in f.name for suffix in suffixes_a_supprimer):\n",
    "        f.unlink()\n",
    "        suppr += 1\n",
    "\n",
    "print(f\"🧹 {suppr} fichiers temporaires supprimés.\")\n",
    "print(\"✅ Dossier 'data/clean' revenu à l'état initial (seuls les *_clean.csv sont conservés).\\n\")\n",
    "\n",
    "# --- Étape 2 : suppression des doublons dans chaque fichier\n",
    "print(\"⚙️ Étape 2 : Suppression des doublons (fichiers réécrits en place)...\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détection automatique du séparateur CSV probable.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def clean_duplicates_in_place(path):\n",
    "    \"\"\"Supprime les doublons et réécrit le fichier directement.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    removed = before - after\n",
    "    pct = round((removed / before) * 100, 2) if before > 0 else 0\n",
    "\n",
    "    # Réécriture directe du fichier\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    print(f\"✅ {path.name} : {removed} doublons supprimés ({pct}%) — Fichier mis à jour\")\n",
    "    return {\"fichier\": path.name, \"lignes_avant\": before, \"lignes_apres\": after, \"doublons_supprimes\": removed, \"pct\": pct}\n",
    "\n",
    "# --- Application du correctif\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_duplicates_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Étape 3 : Synthèse globale\n",
    "print(\"\\n📊 === SYNTHÈSE FINALE DU NETTOYAGE ===\")\n",
    "df_res = pd.DataFrame(results)\n",
    "display(df_res)\n",
    "\n",
    "total_removed = df_res[\"doublons_supprimes\"].sum()\n",
    "print(f\"\\n🧾 Total de doublons supprimés : {total_removed}\")\n",
    "print(\"🎯 Tous les fichiers ont été nettoyés et mis à jour sans duplication.\")\n",
    "print(\"✅ Tu peux maintenant relancer ton test de nettoyage pour confirmer qu'il n'y a plus de doublons.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cadff-a448-4a16-9c4b-5e83a6ad259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 2 : DÉTECTION DES DOUBLONS ===\n",
    "# Objectif : repérer les lignes répétées dans toutes les bases clean\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# --- Dossier contenant les bases nettoyées\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Recherche de tous les fichiers CSV\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def check_duplicates(path):\n",
    "    \"\"\"Analyse un fichier CSV et détecte les doublons.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return {\"fichier\": path.name, \"ok\": False, \"erreur\": str(e), \"doublons\": None}\n",
    "\n",
    "    # --- Détection des doublons\n",
    "    nb_total = len(df)\n",
    "    nb_doublons = df.duplicated().sum()\n",
    "    pct = round((nb_doublons / nb_total) * 100, 2) if nb_total > 0 else 0\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    print(f\"🔹 Lignes totales : {nb_total}\")\n",
    "    print(f\"🔁 Doublons détectés : {nb_doublons} ({pct} %)\")\n",
    "    \n",
    "    if nb_doublons > 0:\n",
    "        print(\"🧾 Exemple de doublons :\")\n",
    "        display(df[df.duplicated()].head(5))\n",
    "    else:\n",
    "        print(\"✅ Aucun doublon trouvé\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    return {\"fichier\": path.name, \"ok\": True, \"lignes\": nb_total, \"doublons\": nb_doublons, \"pourcentage\": pct}\n",
    "\n",
    "# --- Analyse de tous les fichiers\n",
    "results = []\n",
    "for f in files:\n",
    "    res = check_duplicates(f)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_doublons = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DES DOUBLONS ===\")\n",
    "display(df_doublons)\n",
    "\n",
    "# --- Statistiques globales\n",
    "total = len(df_doublons)\n",
    "ok_files = (df_doublons[\"doublons\"] == 0).sum()\n",
    "print(f\"✅ Fichiers sans doublon : {ok_files}/{total}\")\n",
    "print(f\"⚠️ Fichiers contenant des doublons : {total - ok_files}/{total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667f4d0-a998-4998-8ecb-2d60e556a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRECTIF ÉTAPE 3 : SUPPRESSION DES COLONNES VIDES OU TROP INCOMPLÈTES ===\n",
    "# Objectif :\n",
    "# - Supprimer les colonnes 100% NaN ou avec plus de 50% de valeurs manquantes\n",
    "# - Réécrire les fichiers clean existants (pas de duplication)\n",
    "# - Fournir un rapport clair de la réduction des colonnes\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction utilitaire pour détecter le séparateur\n",
    "def detect_separator(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Nettoyage et réécriture en place\n",
    "def clean_missing_columns_in_place(path, seuil=0.5):\n",
    "    \"\"\"Supprime les colonnes vides ou avec plus de `seuil` de NaN.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = len(df.columns)\n",
    "    before_rows = len(df)\n",
    "\n",
    "    # --- Calcul du taux de valeurs manquantes\n",
    "    missing_ratio = df.isna().mean()\n",
    "\n",
    "    # --- Colonnes à supprimer\n",
    "    empty_cols = list(missing_ratio[missing_ratio == 1.0].index)\n",
    "    incomplete_cols = list(missing_ratio[missing_ratio > seuil].index)\n",
    "    to_drop = set(empty_cols + incomplete_cols)\n",
    "\n",
    "    # --- Suppression\n",
    "    df = df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "    after_cols = len(df.columns)\n",
    "    removed_cols = before_cols - after_cols\n",
    "    pct_removed = round((removed_cols / before_cols) * 100, 2) if before_cols > 0 else 0\n",
    "\n",
    "    # --- Réécriture du fichier propre\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"✅ Colonnes supprimées : {removed_cols} ({pct_removed}%)\")\n",
    "    if removed_cols > 0:\n",
    "        print(f\"   🧹 {list(to_drop)[:10]}{' ...' if len(to_drop) > 10 else ''}\")\n",
    "    else:\n",
    "        print(\"   ✅ Aucune colonne supprimée\")\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name} ({after_cols} colonnes conservées, {before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_avant\": before_cols,\n",
    "        \"colonnes_apres\": after_cols,\n",
    "        \"supprimées\": removed_cols,\n",
    "        \"pct_supprimées\": pct_removed\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_missing_columns_in_place(f, seuil=0.5)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU NETTOYAGE DES COLONNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_removed = df_summary[\"supprimées\"].sum()\n",
    "print(f\"🧾 Total de colonnes supprimées : {total_removed}\")\n",
    "print(\"🎯 Tous les fichiers ont été mis à jour directement sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ad30f-31db-42d2-9477-81d60432d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 3 : ANALYSE DES VALEURS MANQUANTES ===\n",
    "# Objectif :\n",
    "# - Identifier les colonnes comportant des NaN ou cellules vides\n",
    "# - Afficher des statistiques claires pour chaque base\n",
    "# - Ne pas générer de fichier sur le disque\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés (déjà sans doublons)\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def analyze_missing_values(path):\n",
    "    \"\"\"Analyse le taux de valeurs manquantes d'un fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return {\"fichier\": path.name, \"ok\": False, \"erreur\": str(e)}\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    print(f\"🔹 {len(df)} lignes, {len(df.columns)} colonnes\")\n",
    "\n",
    "    # --- Calcul des NaN par colonne\n",
    "    missing_ratio = df.isna().mean().round(3)\n",
    "    missing_ratio = missing_ratio[missing_ratio > 0].sort_values(ascending=False)\n",
    "\n",
    "    # --- Statistiques globales\n",
    "    nb_col_nan = len(missing_ratio)\n",
    "    taux_moyen_nan = round(missing_ratio.mean() * 100, 2) if nb_col_nan > 0 else 0\n",
    "    taux_median_nan = round(missing_ratio.median() * 100, 2) if nb_col_nan > 0 else 0\n",
    "\n",
    "    if nb_col_nan > 0:\n",
    "        print(f\"⚠️ {nb_col_nan} colonnes avec des valeurs manquantes ({taux_moyen_nan}% en moyenne)\")\n",
    "        print(\"🔝 Top 10 des colonnes les plus incomplètes :\")\n",
    "        display(missing_ratio.head(10))\n",
    "    else:\n",
    "        print(\"✅ Aucune valeur manquante détectée dans ce fichier.\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes\": len(df.columns),\n",
    "        \"lignes\": len(df),\n",
    "        \"nb_colonnes_nan\": nb_col_nan,\n",
    "        \"taux_moyen_nan(%)\": taux_moyen_nan,\n",
    "        \"taux_median_nan(%)\": taux_median_nan,\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = analyze_missing_values(f)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_nan = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE GLOBALE DES VALEURS MANQUANTES ===\")\n",
    "display(df_nan)\n",
    "\n",
    "# --- Indicateurs globaux\n",
    "nb_total = len(df_nan)\n",
    "nb_sans_nan = (df_nan[\"nb_colonnes_nan\"] == 0).sum()\n",
    "nb_avec_nan = nb_total - nb_sans_nan\n",
    "taux_moyen_global = round(df_nan[\"taux_moyen_nan(%)\"].mean(), 2)\n",
    "\n",
    "print(f\"✅ Bases sans NaN : {nb_sans_nan}/{nb_total}\")\n",
    "print(f\"⚠️ Bases avec NaN : {nb_avec_nan}/{nb_total}\")\n",
    "print(f\"📈 Taux moyen global de NaN sur l'ensemble des fichiers : {taux_moyen_global}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914b4df-f826-44e2-8475-436cd2339cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 4 : UNIFORMISATION DES NOMS DE COLONNES ===\n",
    "# Objectif :\n",
    "#   - Vérifier et corriger les noms de colonnes (minuscules, sans accents, underscores)\n",
    "#   - Supprimer les caractères spéciaux et doublons\n",
    "#   - Réécrire directement les fichiers nettoyés\n",
    "#   - Fournir un résumé clair avant/après\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détection automatique du séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_column_name(name):\n",
    "    \"\"\"Normalise un nom de colonne : minuscules, pas d'accents, underscores, caractères alphanumériques uniquement.\"\"\"\n",
    "    name = str(name).strip().lower()\n",
    "    name = \"\".join(c for c in unicodedata.normalize(\"NFKD\", name) if not unicodedata.combining(c))\n",
    "    name = name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "    name = \"\".join(ch for ch in name if ch.isalnum() or ch == \"_\")\n",
    "    name = \"_\".join([tok for tok in name.split(\"_\") if tok])  # supprime les underscores multiples\n",
    "    return name\n",
    "\n",
    "def clean_column_names_in_place(path):\n",
    "    \"\"\"Uniformise les noms de colonnes et réécrit le fichier en place.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.columns.tolist()\n",
    "\n",
    "    # --- Normalisation des noms de colonnes\n",
    "    normalized_cols = [normalize_column_name(c) for c in before_cols]\n",
    "    df.columns = normalized_cols\n",
    "\n",
    "    # --- Gestion des doublons éventuels\n",
    "    if len(set(df.columns)) < len(df.columns):\n",
    "        seen = {}\n",
    "        new_cols = []\n",
    "        for c in df.columns:\n",
    "            if c not in seen:\n",
    "                seen[c] = 1\n",
    "                new_cols.append(c)\n",
    "            else:\n",
    "                seen[c] += 1\n",
    "                new_cols.append(f\"{c}_{seen[c]}\")\n",
    "        df.columns = new_cols\n",
    "        print(\"⚠️ Doublons de colonnes détectés et corrigés automatiquement.\")\n",
    "\n",
    "    # --- Sauvegarde du fichier corrigé\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    # --- Résumé\n",
    "    after_cols = df.columns.tolist()\n",
    "    changes = {b: a for b, a in zip(before_cols, after_cols) if b != a}\n",
    "    print(f\"✅ Colonnes renommées : {len(changes)} / {len(before_cols)}\")\n",
    "    if len(changes) > 0:\n",
    "        print(f\"   🧾 Exemples : {list(changes.items())[:5]}{' ...' if len(changes) > 5 else ''}\")\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name} ({len(df.columns)} colonnes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": len(df.columns),\n",
    "        \"colonnes_renommees\": len(changes),\n",
    "        \"pct_renommees\": round((len(changes) / len(df.columns)) * 100, 2) if len(df.columns) > 0 else 0\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_column_names_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU NETTOYAGE DES NOMS DE COLONNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_renamed = df_summary[\"colonnes_renommees\"].sum()\n",
    "print(f\"🧾 Total de colonnes renommées : {total_renamed}\")\n",
    "print(\"🎯 Tous les fichiers ont été mis à jour directement sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acb6f1-47bb-4b43-a094-a28e56106127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 5 (corrigée) : CONTRÔLE ET CORRECTION DES TYPES DE DONNÉES ===\n",
    "# Correction : suppression de 'infer_datetime_format' et suppression des warnings répétitifs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Désactivation des warnings inutiles de pandas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "\n",
    "# --- Dossier contenant les fichiers clean\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def infer_and_fix_types_in_place(path):\n",
    "    \"\"\"Analyse et corrige les types de données d’un fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_types = df.dtypes.copy()\n",
    "    conversions = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Tentative de conversion des chaînes vers des types utiles\n",
    "        if series.dtype == \"object\":\n",
    "            # 1️⃣ Conversion numérique\n",
    "            num_series = pd.to_numeric(series.astype(str).str.replace(\",\", \".\").str.replace(\" \", \"\"), errors=\"coerce\")\n",
    "            ratio_num = num_series.notna().mean()\n",
    "            if ratio_num > 0.9:  # au moins 90 % de valeurs numériques\n",
    "                df[col] = num_series\n",
    "                conversions[col] = \"float\"\n",
    "                continue\n",
    "\n",
    "            # 2️⃣ Conversion date (version corrigée)\n",
    "            try:\n",
    "                date_series = pd.to_datetime(series, errors=\"coerce\")\n",
    "                ratio_date = date_series.notna().mean()\n",
    "                if ratio_date > 0.9:\n",
    "                    df[col] = date_series\n",
    "                    conversions[col] = \"datetime\"\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 3️⃣ Conversion float → Int64 quand possible\n",
    "        if df[col].dtype == \"float64\" and df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "            conversions[col] = \"Int64\"\n",
    "\n",
    "    # Sauvegarde du fichier corrigé\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    after_types = df.dtypes\n",
    "    changed = {col: (before_types[col], after_types[col]) for col in df.columns if before_types[col] != after_types[col]}\n",
    "\n",
    "    print(f\"✅ Colonnes converties : {len(conversions)}\")\n",
    "    if len(conversions) > 0:\n",
    "        print(f\"   🧾 Exemples : {list(conversions.items())[:5]}{' ...' if len(conversions) > 5 else ''}\")\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": len(df.columns),\n",
    "        \"colonnes_converties\": len(conversions),\n",
    "        \"types_changés\": len(changed)\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = infer_and_fix_types_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU CONTRÔLE DES TYPES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_converted = df_summary[\"colonnes_converties\"].sum()\n",
    "print(f\"🧾 Total de colonnes converties : {total_converted}\")\n",
    "print(\"🎯 Tous les fichiers ont été vérifiés et mis à jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d1a5b-7db0-4544-9a3e-7a384c9c6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 6 : NORMALISATION DES TEXTES ===\n",
    "# Objectif :\n",
    "#   - Nettoyer les colonnes texte pour supprimer les accents, symboles et espaces parasites\n",
    "#   - Uniformiser la casse (majuscule cohérente)\n",
    "#   - Réécrire les fichiers directement\n",
    "#   - Fournir un résumé clair du nettoyage\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_text(value):\n",
    "    \"\"\"Nettoie et normalise les chaînes de texte.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    value = str(value).strip()\n",
    "\n",
    "    # Supprimer les accents\n",
    "    value = \"\".join(c for c in unicodedata.normalize(\"NFKD\", value) if not unicodedata.combining(c))\n",
    "\n",
    "    # Supprimer les symboles et caractères spéciaux\n",
    "    value = re.sub(r\"[€%$;'\\t\\n\\r]\", \" \", value)\n",
    "\n",
    "    # Remplacer tirets et apostrophes par espace\n",
    "    value = re.sub(r\"[-']\", \" \", value)\n",
    "\n",
    "    # Supprimer les espaces multiples\n",
    "    value = re.sub(r\"\\s+\", \" \", value)\n",
    "\n",
    "    # Mise en majuscule pour les textes courts (ex: communes, catégories)\n",
    "    if len(value) <= 40:\n",
    "        value = value.upper()\n",
    "    else:\n",
    "        value = value.capitalize()\n",
    "\n",
    "    return value.strip()\n",
    "\n",
    "def normalize_text_columns_in_place(path):\n",
    "    \"\"\"Normalise les colonnes texte d’un fichier CSV et réécrit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.shape[1]\n",
    "    before_rows = df.shape[0]\n",
    "\n",
    "    # Colonnes texte à traiter\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"🧾 Colonnes texte détectées : {len(text_cols)} / {before_cols}\")\n",
    "\n",
    "    if len(text_cols) > 0:\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].apply(normalize_text)\n",
    "\n",
    "        # Réécriture directe du fichier\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "        print(f\"✅ Colonnes texte normalisées ({len(text_cols)} colonnes traitées)\")\n",
    "    else:\n",
    "        print(\"ℹ️ Aucune colonne texte à normaliser.\")\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name} ({before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": before_cols,\n",
    "        \"colonnes_texte\": len(text_cols)\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = normalize_text_columns_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU NETTOYAGE TEXTUEL ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_text_cols = df_summary[\"colonnes_texte\"].sum()\n",
    "print(f\"🧾 Total de colonnes texte normalisées : {total_text_cols}\")\n",
    "print(\"🎯 Tous les fichiers ont été normalisés et mis à jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86881bce-bf55-482b-a7c0-f5c8bf9b7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 7 : SUPPRESSION DES CARACTÈRES PARASITES ===\n",
    "# Objectif :\n",
    "#   - Supprimer les caractères invisibles ou non imprimables dans les champs texte\n",
    "#   - Nettoyer les guillemets, tabulations, espaces spéciaux (\\xa0, \\t, etc.)\n",
    "#   - Réécrire les fichiers directement\n",
    "#   - Fournir un rapport clair de nettoyage\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def clean_parasites_in_place(path):\n",
    "    \"\"\"Supprime les caractères parasites dans les colonnes texte et réécrit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.shape[1]\n",
    "    before_rows = df.shape[0]\n",
    "\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"🧾 Colonnes texte détectées : {len(text_cols)} / {before_cols}\")\n",
    "\n",
    "    # --- Fonction de nettoyage cellule\n",
    "    def clean_cell(value):\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "        value = str(value)\n",
    "\n",
    "        # Supprime les espaces non standard et caractères de contrôle\n",
    "        value = re.sub(r\"[\\x00-\\x1F\\x7F\\xa0\\u200b\\r\\t]\", \" \", value)\n",
    "\n",
    "        # Supprime guillemets et doubles séparateurs\n",
    "        value = value.replace('\"', \"\").replace(\"'\", \"\")\n",
    "\n",
    "        # Supprime les espaces multiples\n",
    "        value = re.sub(r\"\\s+\", \" \", value)\n",
    "\n",
    "        return value.strip()\n",
    "\n",
    "    # --- Application sur toutes les colonnes texte\n",
    "    if len(text_cols) > 0:\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].apply(clean_cell)\n",
    "\n",
    "        # Sauvegarde directe du fichier propre\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "        print(f\"✅ Colonnes nettoyées : {len(text_cols)} (fichier réécrit)\")\n",
    "    else:\n",
    "        print(\"ℹ️ Aucun champ texte à nettoyer.\")\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name} ({before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": before_cols,\n",
    "        \"colonnes_texte\": len(text_cols)\n",
    "    }\n",
    "\n",
    "# --- Application du nettoyage à toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_parasites_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU NETTOYAGE DES CARACTÈRES PARASITES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_cleaned = df_summary[\"colonnes_texte\"].sum()\n",
    "print(f\"🧾 Total de colonnes texte nettoyées : {total_cleaned}\")\n",
    "print(\"🎯 Tous les fichiers ont été nettoyés et mis à jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912e0c-ea31-4956-9c3e-8b6b35948522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 8 : CRÉATION ET VÉRIFICATION DES CLÉS COMMUNES ===\n",
    "# Objectif :\n",
    "#   - Créer / uniformiser les colonnes code_commune et commune_std\n",
    "#   - Normaliser les noms de communes\n",
    "#   - Vérifier la cohérence et réécrire les fichiers directement\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Utilitaires de normalisation\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_commune_name(name):\n",
    "    \"\"\"Nettoie et standardise le nom de la commune.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return np.nan\n",
    "    name = str(name).strip().upper()\n",
    "    name = \"\".join(c for c in unicodedata.normalize(\"NFKD\", name) if not unicodedata.combining(c))\n",
    "    name = re.sub(r\"[-']\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "def extract_code_commune(value):\n",
    "    \"\"\"Extrait un code INSEE sur 5 chiffres si possible.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    value = str(value)\n",
    "    match = re.search(r\"\\d{5}\", value)\n",
    "    return match.group(0) if match else np.nan\n",
    "\n",
    "def ensure_commune_keys(path):\n",
    "    \"\"\"Ajoute / vérifie code_commune et commune_std puis réécrit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = len(df.columns)\n",
    "    before_rows = len(df)\n",
    "\n",
    "    # --- Détection de colonnes similaires existantes\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c and \"std\" not in c), None)\n",
    "\n",
    "    # --- Création / nettoyage du code_commune\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].apply(extract_code_commune)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "\n",
    "    # --- Création / nettoyage du nom de commune\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "\n",
    "    # --- Rapport\n",
    "    valid_codes = df[\"code_commune\"].notna().sum()\n",
    "    valid_names = df[\"commune_std\"].notna().sum()\n",
    "    print(f\"🧩 code_commune : {valid_codes} valeurs valides / {before_rows}\")\n",
    "    print(f\"🏙️ commune_std : {valid_names} valeurs valides / {before_rows}\")\n",
    "\n",
    "    # --- Réécriture directe du fichier\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"✅ Fichier mis à jour : {path.name} ({len(df.columns)} colonnes, {before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"code_commune_valide\": valid_codes,\n",
    "        \"commune_std_valide\": valid_names,\n",
    "        \"total_lignes\": before_rows\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = ensure_commune_keys(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DES CLÉS COMMUNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "# --- Indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "avg_code_rate = round((df_summary[\"code_commune_valide\"] / df_summary[\"total_lignes\"]).mean() * 100, 2)\n",
    "avg_name_rate = round((df_summary[\"commune_std_valide\"] / df_summary[\"total_lignes\"]).mean() * 100, 2)\n",
    "\n",
    "print(f\"✅ Moyenne de complétude du code_commune : {avg_code_rate}%\")\n",
    "print(f\"✅ Moyenne de complétude du commune_std : {avg_name_rate}%\")\n",
    "print(\"🎯 Tous les fichiers ont été mis à jour avec des clés de jointure cohérentes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dca695-1ad8-4f49-9de7-b96841af3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 9 : CONTRÔLE ET COHÉRENCE DES DONNÉES ===\n",
    "# Objectif :\n",
    "#   - Vérifier la cohérence logique des valeurs numériques\n",
    "#   - Supprimer les lignes incohérentes (surfaces <= 0, valeurs foncières négatives, etc.)\n",
    "#   - Fournir un rapport clair et réécrire les fichiers en place\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def control_coherence_in_place(path):\n",
    "    \"\"\"Vérifie et corrige les incohérences de cohérence logique des données.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_rows = len(df)\n",
    "    anomalies = {}\n",
    "\n",
    "    # --- 1️⃣ Surfaces négatives ou nulles\n",
    "    if \"surface_reelle_bati\" in df.columns:\n",
    "        anomalies[\"surface<=0\"] = (df[\"surface_reelle_bati\"] <= 0).sum(skipna=True)\n",
    "        df = df[df[\"surface_reelle_bati\"] > 0]\n",
    "\n",
    "    # --- 2️⃣ Valeurs foncières négatives ou nulles\n",
    "    if \"valeur_fonciere\" in df.columns:\n",
    "        anomalies[\"valeur<=0\"] = (df[\"valeur_fonciere\"] <= 0).sum(skipna=True)\n",
    "        df = df[df[\"valeur_fonciere\"] > 0]\n",
    "\n",
    "    # --- 3️⃣ Prix au m² irréalistes (si applicable)\n",
    "    if set([\"valeur_fonciere\", \"surface_reelle_bati\"]).issubset(df.columns):\n",
    "        df[\"prix_m2\"] = df[\"valeur_fonciere\"] / df[\"surface_reelle_bati\"]\n",
    "        anomalies[\"prix_m2<500\"] = (df[\"prix_m2\"] < 500).sum(skipna=True)\n",
    "        anomalies[\"prix_m2>50000\"] = (df[\"prix_m2\"] > 50000).sum(skipna=True)\n",
    "        df = df[(df[\"prix_m2\"] >= 500) & (df[\"prix_m2\"] <= 50000)]\n",
    "\n",
    "    # --- 4️⃣ Valeurs négatives générales sur d’autres indicateurs\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_cols:\n",
    "        if col not in [\"valeur_fonciere\", \"surface_reelle_bati\", \"prix_m2\"]:\n",
    "            negatives = (df[col] < 0).sum(skipna=True)\n",
    "            if negatives > 0:\n",
    "                anomalies[f\"{col}<0\"] = negatives\n",
    "                df = df[df[col] >= 0]\n",
    "\n",
    "    # --- Calcul des suppressions\n",
    "    after_rows = len(df)\n",
    "    removed = before_rows - after_rows\n",
    "    pct_removed = round((removed / before_rows) * 100, 2) if before_rows > 0 else 0\n",
    "\n",
    "    # --- Sauvegarde du fichier propre\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"✅ {removed} lignes incohérentes supprimées ({pct_removed}%)\")\n",
    "    if anomalies:\n",
    "        print(\"📊 Détails anomalies détectées :\", anomalies)\n",
    "    print(f\"   🔸 Fichier mis à jour : {path.name} ({after_rows} lignes restantes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"lignes_avant\": before_rows,\n",
    "        \"lignes_apres\": after_rows,\n",
    "        \"supprimees\": removed,\n",
    "        \"pct_supprimees\": pct_removed\n",
    "    }\n",
    "\n",
    "# --- Application à toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = control_coherence_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\n📊 === SYNTHÈSE DU CONTRÔLE DE COHÉRENCE ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_removed = df_summary[\"supprimees\"].sum()\n",
    "print(f\"🧾 Total de lignes incohérentes supprimées : {total_removed}\")\n",
    "print(\"🎯 Tous les fichiers ont été nettoyés et mis à jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a99e5a-18cf-4c8f-8ba9-57d886d66682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 10 : VÉRIFICATION STRUCTURELLE ET FORMAT FINAL ===\n",
    "# Objectif :\n",
    "#   - Vérifier la structure, le format et la propreté finale de chaque fichier clean\n",
    "#   - Contrôler encodage, séparateur, cohérence colonnes / lignes, taille et caractères parasites\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction de détection du séparateur\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Vérification structurelle\n",
    "def verify_structure(path):\n",
    "    report = {\n",
    "        \"fichier\": path.name,\n",
    "        \"encodage\": \"UTF-8\",\n",
    "        \"sep\": None,\n",
    "        \"colonnes\": 0,\n",
    "        \"lignes\": 0,\n",
    "        \"caractères_parasites\": False,\n",
    "        \"taille_ko\": round(os.path.getsize(path) / 1024, 2),\n",
    "        \"statut\": \"✅ Conforme\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        report[\"sep\"] = sep\n",
    "\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "        report[\"colonnes\"] = len(df.columns)\n",
    "        report[\"lignes\"] = len(df)\n",
    "\n",
    "        # Vérification des critères de base\n",
    "        if report[\"colonnes\"] < 5 or report[\"lignes\"] < 10:\n",
    "            report[\"statut\"] = \"⚠️ Structure suspecte (trop peu de données)\"\n",
    "\n",
    "        # Recherche de caractères parasites\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "        if any(char in content for char in [\"\\x00\", \"\\x1f\", \"\\xa0\", \"\\u200b\", \"; ;\", \", ,\"]):\n",
    "            report[\"caractères_parasites\"] = True\n",
    "            report[\"statut\"] = \"⚠️ Caractères parasites détectés\"\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        report[\"encodage\"] = \"⚠️ Non UTF-8\"\n",
    "        report[\"statut\"] = \"⚠️ Encodage incorrect\"\n",
    "    except Exception as e:\n",
    "        report[\"statut\"] = f\"❌ Erreur de lecture : {e}\"\n",
    "\n",
    "    return report\n",
    "\n",
    "# --- Application de la vérification à toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = verify_structure(f)\n",
    "    results.append(res)\n",
    "    print(f\"🔎 Vérification {f.name} → {res['statut']} ({res['colonnes']} colonnes, {res['lignes']} lignes)\")\n",
    "\n",
    "# --- Synthèse globale\n",
    "df_final = pd.DataFrame(results)\n",
    "print(\"\\n📊 === RAPPORT FINAL DE VALIDATION ===\")\n",
    "display(df_final)\n",
    "\n",
    "# --- Statistiques globales\n",
    "nb_total = len(df_final)\n",
    "nb_valid = (df_final[\"statut\"].str.contains(\"✅\")).sum()\n",
    "nb_warnings = (df_final[\"statut\"].str.contains(\"⚠️\")).sum()\n",
    "nb_errors = (df_final[\"statut\"].str.contains(\"❌\")).sum()\n",
    "\n",
    "print(f\"\\n✅ Fichiers conformes : {nb_valid}/{nb_total}\")\n",
    "print(f\"⚠️ Fichiers à vérifier : {nb_warnings}/{nb_total}\")\n",
    "print(f\"❌ Fichiers en erreur : {nb_errors}/{nb_total}\")\n",
    "\n",
    "if nb_errors == 0 and nb_warnings == 0:\n",
    "    print(\"\\n🎯 Tous les fichiers sont prêts pour l’analyse finale et la fusion inter-bases !\")\n",
    "else:\n",
    "    print(\"\\n📌 Certains fichiers nécessitent une vérification manuelle avant l’étape d’analyse.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca4f76-19d3-441f-a057-74c34eb013bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RÉCAPITULATIF GLOBAL DES BASES APRÈS NETTOYAGE ===\n",
    "# Objectif :\n",
    "#   - Afficher nombre de lignes / colonnes par base\n",
    "#   - Montrer un aperçu (5 lignes × 8 colonnes)\n",
    "#   - Vérifier cohérence et propreté globale\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Lecture et résumé global\n",
    "summaries = []\n",
    "\n",
    "for path in files:\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "        # Informations de base\n",
    "        lignes, colonnes = df.shape\n",
    "        nan_rate = round(df.isna().mean().mean() * 100, 2)\n",
    "        numeric_cols = len(df.select_dtypes(include=[\"number\"]).columns)\n",
    "        text_cols = len(df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "        summaries.append({\n",
    "            \"Fichier\": path.name,\n",
    "            \"Lignes\": lignes,\n",
    "            \"Colonnes\": colonnes,\n",
    "            \"Colonnes numériques\": numeric_cols,\n",
    "            \"Colonnes texte\": text_cols,\n",
    "            \"Taux NaN (%)\": nan_rate\n",
    "        })\n",
    "\n",
    "        print(f\"=== {path.name} ===\")\n",
    "        print(f\"🔹 {lignes} lignes | {colonnes} colonnes\")\n",
    "        print(f\"   📊 {numeric_cols} numériques | {text_cols} texte | NaN moyen : {nan_rate}%\")\n",
    "        print(\"   🔸 Aperçu des premières colonnes :\")\n",
    "        display(df.head(5).iloc[:, :min(8, colonnes)])  # 5 lignes × 8 colonnes max\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de lire {path.name} : {e}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# --- Synthèse finale\n",
    "print(\"\\n📊 === SYNTHÈSE GLOBALE DES BASES ===\")\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "display(df_summary.sort_values(\"Fichier\").reset_index(drop=True))\n",
    "\n",
    "# --- Quelques indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "total_lignes = df_summary[\"Lignes\"].sum()\n",
    "total_colonnes = df_summary[\"Colonnes\"].sum()\n",
    "\n",
    "print(f\"\\n📈 Nombre total de bases : {nb_total}\")\n",
    "print(f\"📊 Total de lignes (toutes bases confondues) : {total_lignes:,}\")\n",
    "print(f\"🧱 Total de colonnes cumulées : {total_colonnes}\")\n",
    "print(\"🎯 Vérification visuelle effectuée : tu peux maintenant confirmer que tout est cohérent avant les fusions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c3233-b551-4e5c-9660-7033165f0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RÉCAPITULATIF GLOBAL DES BASES APRÈS NETTOYAGE ===\n",
    "# Objectif :\n",
    "#   - Afficher nombre de lignes / colonnes par base\n",
    "#   - Montrer un aperçu (5 lignes × 8 colonnes)\n",
    "#   - Vérifier cohérence et propreté globale\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoyés\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"📂 {len(files)} fichiers détectés dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"Détecte automatiquement le séparateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Lecture et résumé global\n",
    "summaries = []\n",
    "\n",
    "for path in files:\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "        # Informations de base\n",
    "        lignes, colonnes = df.shape\n",
    "        nan_rate = round(df.isna().mean().mean() * 100, 2)\n",
    "        numeric_cols = len(df.select_dtypes(include=[\"number\"]).columns)\n",
    "        text_cols = len(df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "        summaries.append({\n",
    "            \"Fichier\": path.name,\n",
    "            \"Lignes\": lignes,\n",
    "            \"Colonnes\": colonnes,\n",
    "            \"Colonnes numériques\": numeric_cols,\n",
    "            \"Colonnes texte\": text_cols,\n",
    "            \"Taux NaN (%)\": nan_rate\n",
    "        })\n",
    "\n",
    "        print(f\"=== {path.name} ===\")\n",
    "        print(f\"🔹 {lignes} lignes | {colonnes} colonnes\")\n",
    "        print(f\"   📊 {numeric_cols} numériques | {text_cols} texte | NaN moyen : {nan_rate}%\")\n",
    "        print(\"   🔸 Aperçu des premières colonnes :\")\n",
    "        display(df.head(5).iloc[:, :min(8, colonnes)])  # 5 lignes × 8 colonnes max\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible de lire {path.name} : {e}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# --- Synthèse finale\n",
    "print(\"\\n📊 === SYNTHÈSE GLOBALE DES BASES ===\")\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "display(df_summary.sort_values(\"Fichier\").reset_index(drop=True))\n",
    "\n",
    "# --- Quelques indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "total_lignes = df_summary[\"Lignes\"].sum()\n",
    "total_colonnes = df_summary[\"Colonnes\"].sum()\n",
    "\n",
    "print(f\"\\n📈 Nombre total de bases : {nb_total}\")\n",
    "print(f\"📊 Total de lignes (toutes bases confondues) : {total_lignes:,}\")\n",
    "print(f\"🧱 Total de colonnes cumulées : {total_colonnes}\")\n",
    "print(\"🎯 Vérification visuelle effectuée : tu peux maintenant confirmer que tout est cohérent avant les fusions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5559b-5b46-47e4-b792-42c33ac71e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf83502-c91d-410a-8085-576d1ffe5477",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m         name \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m         dfs[name] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, file,low_memory\u001b[38;5;241m=\u001b[39m\u001b[43mfalse\u001b[49m))\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Charg√© : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dfs[name])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lignes)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Charger la table de r√©f√©rence si elle existe\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "sns.set(style=\"whitegrid\")\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dossier contenant les fichiers clean\n",
    "data_dir = \"data/clean\"  # adapte si ton dossier diff√®re\n",
    "\n",
    "# Chargement automatique de toutes les bases *_clean.csv\n",
    "dfs = {}\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\"_clean.csv\"):\n",
    "        name = file.replace(\".csv\", \"\")\n",
    "        dfs[name] = pd.read_csv(os.path.join(data_dir, file))\n",
    "        print(f\"‚úÖ Charg√© : {name} ({len(dfs[name])} lignes)\")\n",
    "\n",
    "# Charger la table de r√©f√©rence si elle existe\n",
    "if \"communes_ref_clean\" in dfs:\n",
    "    communes_ref = dfs[\"communes_ref_clean\"]\n",
    "    print(f\"\\nüìç communes_ref pr√™te : {len(communes_ref)} communes.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è communes_ref non trouv√©e dans les fichiers clean.\")\n",
    "\n",
    "print(\"‚úÖ Librairies import√©es avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ed2b74-a4f6-4881-8797-c8bb46e86ee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Chargement des bases de donn√©es nettoy√©es\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/clean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m clean_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      5\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m clean_files:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Chargement des bases de donn√©es nettoy√©es\n",
    "data_folder = \"data/clean\"\n",
    "clean_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "dfs = {}\n",
    "for f in clean_files:\n",
    "    path = os.path.join(data_folder, f)\n",
    "    name = f.replace(\".csv\", \"\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", sep=\",\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"latin-1\", sep=\";\", low_memory=False)\n",
    "    dfs[name] = df\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name.upper()} : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "    print(list(df.columns[:8]))\n",
    "    display(df.head(3))\n",
    "\n",
    "possible_keys = [\"code_commune\", \"code_insee\", \"Code_commune\", \"nom_commune\", \"Commune\", \"ville\"]\n",
    "for name, df in dfs.items():\n",
    "    found = [col for col in df.columns if col in possible_keys]\n",
    "    print(f\"{name}: {found if found else 'aucune cl√© de jointure trouv√©e'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6308aff8-47ec-4d6e-aedd-10019684175f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# √âtape 2 ‚Äî Harmonisation des cl√©s\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Normalisation des cl√©s de jointure\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdfs\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_commune\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m alt \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode_commune\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_insee\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommune\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnom_commune\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mville\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "# √âtape 2 ‚Äî Harmonisation des cl√©s\n",
    "\n",
    "# Normalisation des cl√©s de jointure\n",
    "for name, df in dfs.items():\n",
    "    if \"code_commune\" not in df.columns:\n",
    "        for alt in [\"Code_commune\", \"code_insee\", \"Commune\", \"nom_commune\", \"ville\"]:\n",
    "            if alt in df.columns:\n",
    "                df.rename(columns={alt: \"code_commune\"}, inplace=True)\n",
    "                break\n",
    "    if \"code_commune\" in df.columns:\n",
    "        df[\"code_commune\"] = df[\"code_commune\"].astype(str).str.zfill(5)\n",
    "        dfs[name] = df\n",
    "\n",
    "# V√©rification de la pr√©sence et de la coh√©rence de la cl√©\n",
    "for name, df in dfs.items():\n",
    "    if \"code_commune\" in df.columns:\n",
    "        print(f\"\\n{name.upper()} : cl√© d√©tect√©e\")\n",
    "        print(f\" - Type : {df['code_commune'].dtype}\")\n",
    "        print(f\" - Nb valeurs uniques : {df['code_commune'].nunique()}\")\n",
    "        print(f\" - Nb lignes : {len(df)}\")\n",
    "        print(f\" - Doublons potentiels : {len(df) - df['code_commune'].nunique()}\")\n",
    "        display(df[['code_commune']].head(5))\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {name.upper()} : aucune cl√© 'code_commune' d√©tect√©e\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13259055-4433-4f7a-a70b-adce7b7c9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, itertools, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _strip_accents(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_insee(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\.0$\", \"\", s)\n",
    "    s = re.sub(r\"\\D\", \"\", s)\n",
    "    if not re.fullmatch(r\"\\d{5}\", s): return None\n",
    "    return s\n",
    "\n",
    "def norm_postal(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = re.sub(r\"\\D\", \"\", str(x).strip())\n",
    "    if not re.fullmatch(r\"\\d{5}\", s): return None\n",
    "    return s\n",
    "\n",
    "def norm_commune(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = _strip_accents(str(x).upper().strip())\n",
    "    s = re.sub(r\"[\\s'\\-]\", \"\", s)\n",
    "    return s if s and not s.isdigit() else None\n",
    "\n",
    "def norm_year(x):\n",
    "    if pd.isna(x): return None\n",
    "    m = re.search(r\"\\b(19\\d{2}|20\\d{2})\\b\", str(x))\n",
    "    if not m: return None\n",
    "    y = int(m.group(1))\n",
    "    return y if 1900 <= y <= 2030 else None\n",
    "\n",
    "def norm_lat(x):\n",
    "    try: return round(float(x), 3)\n",
    "    except: return None\n",
    "\n",
    "def norm_lon(x):\n",
    "    try: return round(float(x), 3)\n",
    "    except: return None\n",
    "\n",
    "# ignorer explicitement ces colonnes\n",
    "IGNORE_NAME_PAT = re.compile(r\"(?:^|_)(id|identifiant|mutation|numero|uuid|hash)(?:_|$)\", re.I)\n",
    "\n",
    "def guess_type(colname, s):\n",
    "    name = colname.lower()\n",
    "\n",
    "    if IGNORE_NAME_PAT.search(name):\n",
    "        return None\n",
    "\n",
    "    if any(k in name for k in [\"insee\",\"code_commune\",\"codeinsee\"]):\n",
    "        return \"insee\"\n",
    "    if any(k in name for k in [\"cp\",\"code_postal\",\"postal\"]):\n",
    "        return \"postal\"\n",
    "    if any(k in name for k in [\"commune\",\"ville\",\"nom_ville\",\"nomcommune\"]):\n",
    "        return \"commune\"\n",
    "    if any(k in name for k in [\"annee\",\"year\",\"date\"]):\n",
    "        return \"year\"\n",
    "    if any(k in name for k in [\"lat\",\"latitude\"]):\n",
    "        return \"lat\"\n",
    "    if any(k in name for k in [\"lon\",\"lng\",\"longitude\"]):\n",
    "        return \"lon\"\n",
    "    return None\n",
    "\n",
    "def apply_norm(typ, series):\n",
    "    if typ == \"insee\":   return series.map(norm_insee)\n",
    "    if typ == \"postal\":  return series.map(norm_postal)\n",
    "    if typ == \"commune\": return series.map(norm_commune)\n",
    "    if typ == \"year\":    return series.map(norm_year)\n",
    "    if typ == \"lat\":     return series.map(norm_lat)\n",
    "    if typ == \"lon\":     return series.map(norm_lon)\n",
    "    return None\n",
    "\n",
    "profiles = {}\n",
    "MIN_UNIQUES = 20\n",
    "\n",
    "for dname, df in dfs.items():\n",
    "    prof = {}\n",
    "    for col in df.columns:\n",
    "        typ = guess_type(col, df[col])\n",
    "        if typ is None: \n",
    "            continue\n",
    "        normed = apply_norm(typ, df[col])\n",
    "        if normed is None:\n",
    "            continue\n",
    "        vals = pd.Series(normed).dropna().unique()\n",
    "        nuniq = len(vals)\n",
    "        if nuniq >= MIN_UNIQUES:\n",
    "            prof[col] = (typ, normed, set(vals), nuniq)\n",
    "    if prof:\n",
    "        profiles[dname] = prof\n",
    "\n",
    "def jaccard(a, b):\n",
    "    inter = a & b\n",
    "    uni = a | b\n",
    "    return (len(inter) / len(uni) if len(uni) else 0.0, len(inter))\n",
    "\n",
    "rows = []\n",
    "for (d1, p1), (d2, p2) in itertools.combinations(profiles.items(), 2):\n",
    "    for c1, (t1, s1, set1, n1) in p1.items():\n",
    "        for c2, (t2, s2, set2, n2) in p2.items():\n",
    "            same_type = (t1 == t2)\n",
    "            allowed_mix = ({t1,t2} in [ {\"commune\",\"insee\"}, {\"commune\",\"postal\"} ])\n",
    "            if not (same_type or allowed_mix):\n",
    "                continue\n",
    "            jac, inter = jaccard(set1, set2)\n",
    "            cov1 = inter / n1 if n1 else 0\n",
    "            cov2 = inter / n2 if n2 else 0\n",
    "\n",
    "            if inter >= 50 or (jac >= 0.05 and cov1 >= 0.1 and cov2 >= 0.1):\n",
    "                score = 0.6*jac + 0.2*cov1 + 0.2*cov2\n",
    "                rows.append({\n",
    "                    \"dataset_left\": d1, \"col_left\": c1, \"type_left\": t1, \"unique_left\": n1,\n",
    "                    \"dataset_right\": d2, \"col_right\": c2, \"type_right\": t2, \"unique_right\": n2,\n",
    "                    \"intersection\": inter, \"jaccard\": round(jac,4),\n",
    "                    \"cov_left\": round(cov1,4), \"cov_right\": round(cov2,4),\n",
    "                    \"match_score\": round(score,4)\n",
    "                })\n",
    "\n",
    "report = pd.DataFrame(rows).sort_values([\"match_score\",\"intersection\"], ascending=[False, False]).head(50)\n",
    "report.reset_index(drop=True, inplace=True)\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b095f-67cb-489d-851e-5f78c32c8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pipeline Int√©gration & Analyse ‚Äî version avec checks de progression ===\n",
    "import re, unicodedata, pandas as pd, numpy as np\n",
    "\n",
    "print(\"‚ñ∂Ô∏è D√©marrage pipeline...\")\n",
    "\n",
    "def strip_accents(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_commune_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = strip_accents(str(s).upper().strip())\n",
    "    s = s.replace(\"‚Äô\",\"'\").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[\\s'-]\", \"\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{1})\\b\", r\"PARIS0\\1\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{2})\\b\", r\"PARIS\\1\", s)\n",
    "    return s or None\n",
    "\n",
    "def clean_insee(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\.0$\", \"\", s)\n",
    "    s = re.sub(r\"\\D\", \"\", s)\n",
    "    return s if re.fullmatch(r\"\\d{5}\", s) else None\n",
    "\n",
    "def best_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def add_norm_name(df, name_cols=(\"nom_commune\",\"Commune\",\"commune\",\"ville\",\"LIBGEO\")):\n",
    "    col = best_col(df, name_cols)\n",
    "    return df.assign(_nom_norm=df[col].map(norm_commune_name) if col else None)\n",
    "\n",
    "def add_insee(df, insee_cols=(\"code_commune\",\"code_insee\",\"Code_commune\",\"ninsee\",\"codeinsee\")):\n",
    "    col = best_col(df, insee_cols)\n",
    "    return df.assign(_insee=df[col].map(clean_insee) if col else None)\n",
    "\n",
    "def coverage_pct(keys, ref_keys):\n",
    "    if len(keys) == 0: return 0.0\n",
    "    return round(keys.dropna().isin(ref_keys).mean()*100, 2)\n",
    "\n",
    "success_steps = []\n",
    "\n",
    "try:\n",
    "    assert 'dfs' in globals() and isinstance(dfs, dict) and len(dfs) > 0, \"Le dict 'dfs' doit √™tre pr√©sent (bases charg√©es).\"\n",
    "    print(f\"‚úÖ dfs d√©tect√© ({len(dfs)} bases)\")\n",
    "    success_steps.append(\"dfs_ok\")\n",
    "except AssertionError as e:\n",
    "    print(\"‚ùå\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f252313-dcf6-4a9c-b77d-71fea4bdea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) LIAISON ‚Äî Cr√©ation du pivot communes_ref (robuste) ===\n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "def _strip_acc(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def _norm_commune_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = _strip_acc(str(s).upper().strip())\n",
    "    s = s.replace(\"‚Äô\",\"'\").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[\\s'-]\", \"\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{1})\\b\", r\"PARIS0\\1\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{2})\\b\", r\"PARIS\\1\", s)\n",
    "    return s or None\n",
    "\n",
    "def _norm_insee_series(s):\n",
    "    return (s.astype(str)\n",
    "             .str.strip()\n",
    "             .str.replace(r\"\\.0$\",\"\",regex=True)\n",
    "             .str.replace(r\"\\D\",\"\",regex=True)\n",
    "             .where(lambda x: x.str.fullmatch(r\"\\d{5}\").fillna(False)))\n",
    "\n",
    "def _build_communes_ref_from(df):\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"Code_commune\",\"ninsee\",\"codeinsee\"] if c in df.columns), None)\n",
    "    name_col = next((c for c in [\"nom_commune\",\"Commune\",\"commune\",\"ville\",\"LIBGEO\"] if c in df.columns), None)\n",
    "    out = pd.DataFrame()\n",
    "    if code_col is not None:\n",
    "        out[\"code_commune\"] = _norm_insee_series(df[code_col])\n",
    "    if name_col is not None:\n",
    "        out[\"_nom_norm\"] = df[name_col].map(_norm_commune_name)\n",
    "    if \"code_commune\" not in out:\n",
    "        out[\"code_commune\"] = None\n",
    "    out = out.dropna(subset=[\"code_commune\"]).drop_duplicates(\"code_commune\")\n",
    "    return out\n",
    "\n",
    "print(\"‚ñ∂Ô∏è √âtape 1 ‚Äî S√©lection du pivot\")\n",
    "print(\"Jeux dispo :\", list(dfs.keys()))\n",
    "\n",
    "cand_air   = [k for k in dfs if \"air_parif_communes\" in k.lower()]\n",
    "cand_insee = [k for k in dfs if \"insee\" in k.lower()]\n",
    "cand_dvf   = sorted([k for k in dfs if re.match(r\"^dvf_\\d{2}_clean$\", k, flags=re.I)])\n",
    "\n",
    "if cand_air:\n",
    "    pivot_key = cand_air[0]\n",
    "    print(f\"‚úÖ Pivot forc√© (Airparif) : {pivot_key}\")\n",
    "    communes_ref = _build_communes_ref_from(dfs[pivot_key])\n",
    "elif cand_insee:\n",
    "    pivot_key = cand_insee[0]\n",
    "    print(f\"‚úÖ Pivot INSEE : {pivot_key}\")\n",
    "    communes_ref = _build_communes_ref_from(dfs[pivot_key])\n",
    "elif cand_dvf:\n",
    "    print(\"‚úÖ Pivot DVF = concat de tous les d√©partements\")\n",
    "    tmp = pd.concat([dfs[k] for k in cand_dvf], ignore_index=True)\n",
    "    communes_ref = _build_communes_ref_from(tmp)\n",
    "else:\n",
    "    raise ValueError(\"Aucun pivot exploitable trouv√© parmi dfs.\")\n",
    "\n",
    "communes_ref = communes_ref.dropna(subset=[\"code_commune\"]).drop_duplicates(\"code_commune\")\n",
    "nb_codes = communes_ref[\"code_commune\"].nunique()\n",
    "print(f\"üìå communes_ref pr√™te : {nb_codes} codes uniques\")\n",
    "display(communes_ref.head(10))\n",
    "\n",
    "assert nb_codes > 0, \"communes_ref vide.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f3639-f886-4ff1-9a61-f2ce3f22a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dvf_keys = sorted([k for k in dfs if re.match(r\"^dvf_\\d{2}_clean$\", k, flags=re.I)])\n",
    "print(\"DVF trouv√©es :\", dvf_keys)\n",
    "\n",
    "for k in dvf_keys:\n",
    "    df = dfs[k]\n",
    "    print(f\"\\n‚Äî {k}\")\n",
    "    print(\"Colonnes dispo (extrait) :\", [c for c in df.columns if c.lower() in \n",
    "          {\"code_commune\",\"code_insee\",\"commune\",\"nom_commune\",\"ville\",\"surface_reelle_bati\",\"valeur_fonciere\"}])\n",
    "    for col in [\"code_commune\",\"code_insee\",\"commune\",\"nom_commune\",\"ville\"]:\n",
    "        if col in df.columns:\n",
    "            nnz = df[col].notna().sum()\n",
    "            ex = df[col].dropna().astype(str).head(3).tolist()\n",
    "            print(f\"  {col}: {nnz} non-nuls, ex: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546dc7f-764e-4cf2-ba5b-bb87741b2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, re, unicodedata\n",
    "\n",
    "def strip_acc(s):\n",
    "    if not isinstance(s,str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_commune_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = strip_acc(str(s).upper().strip())\n",
    "    s = s.replace(\"‚Äô\",\"'\").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[\\s'-]\", \"\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{1})\\b\", r\"PARIS0\\1\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{2})\\b\", r\"PARIS\\1\", s)\n",
    "    return s or None\n",
    "\n",
    "def clean_insee(x):\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\.0$\",\"\", s)\n",
    "    s = re.sub(r\"\\D\",\"\", s)\n",
    "    return s if re.fullmatch(r\"\\d{5}\", s) else None\n",
    "\n",
    "# concat minimal DVF avec nom+code quand pr√©sent\n",
    "pairs = []\n",
    "for k in dvf_keys:\n",
    "    df = dfs[k].copy()\n",
    "    # choix d'un champ nom dispo\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in df.columns), None)\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"Code_commune\"] if c in df.columns), None)\n",
    "    if name_col is None or code_col is None: \n",
    "        continue\n",
    "    tmp = df[[name_col, code_col]].dropna()\n",
    "    if tmp.empty: \n",
    "        continue\n",
    "    tmp[\"_nom_norm\"] = tmp[name_col].map(norm_commune_name)\n",
    "    tmp[\"code_commune\"] = tmp[code_col].map(clean_insee)\n",
    "    tmp = tmp.dropna(subset=[\"_nom_norm\",\"code_commune\"]).drop_duplicates([\"code_commune\",\"_nom_norm\"])\n",
    "    pairs.append(tmp[[\"_nom_norm\",\"code_commune\"]])\n",
    "\n",
    "if pairs:\n",
    "    dvf_name_map = pd.concat(pairs, ignore_index=True).drop_duplicates(\"code_commune\")\n",
    "    # merge pour enrichir communes_ref\n",
    "    if \"_nom_norm\" in communes_ref.columns:\n",
    "        communes_ref.drop(columns=[\"_nom_norm\"], inplace=True)\n",
    "    communes_ref = communes_ref.merge(dvf_name_map, on=\"code_commune\", how=\"left\")\n",
    "    print(f\"üìù communes_ref enrichi en noms via DVF ‚Äî couverture noms: {communes_ref['_nom_norm'].notna().mean()*100:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Aucun couple nom+code exploitable trouv√© dans DVF pour enrichir le pivot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442f29c-3e2c-4a41-bffa-fb7535d647c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Enrichir communes_ref avec des noms depuis √âducation puis Transports ===\n",
    "import re, unicodedata, pandas as pd\n",
    "\n",
    "def _strip_acc(s):\n",
    "    if not isinstance(s,str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_commune_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = _strip_acc(str(s).upper().strip())\n",
    "    s = s.replace(\"‚Äô\",\"'\").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[\\s'-]\", \"\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{1})\\b\", r\"PARIS0\\1\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{2})\\b\", r\"PARIS\\1\", s)\n",
    "    return s or None\n",
    "\n",
    "def clean_insee(x):\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\.0$\",\"\", s)\n",
    "    s = re.sub(r\"\\D\",\"\", s)\n",
    "    return s if re.fullmatch(r\"\\d{5}\", s) else None\n",
    "\n",
    "def extract_nom_code(df):\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\",\"LIBGEO\"] if c in df.columns), None)\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"Code_commune\",\"ninsee\",\"codeinsee\"] if c in df.columns), None)\n",
    "    if not name_col or not code_col:\n",
    "        return None\n",
    "    tmp = df[[name_col, code_col]].dropna()\n",
    "    if tmp.empty: return None\n",
    "    tmp[\"_nom_norm\"]   = tmp[name_col].map(norm_commune_name)\n",
    "    tmp[\"code_commune\"]= tmp[code_col].map(clean_insee)\n",
    "    tmp = tmp.dropna(subset=[\"_nom_norm\",\"code_commune\"]).drop_duplicates(\"code_commune\")\n",
    "    return tmp[[\"_nom_norm\",\"code_commune\"]]\n",
    "\n",
    "before = communes_ref[\"_nom_norm\"].notna().mean()*100 if \"_nom_norm\" in communes_ref.columns else 0.0\n",
    "\n",
    "edu_keys = [k for k in dfs if \"annuaire_education\" in k.lower() or \"etablissement\" in k.lower()]\n",
    "if edu_keys:\n",
    "    edu_map = extract_nom_code(dfs[edu_keys[0]])\n",
    "    if edu_map is not None:\n",
    "        communes_ref = communes_ref.merge(edu_map, on=\"code_commune\", how=\"left\") if \"_nom_norm\" not in communes_ref.columns \\\n",
    "                       else communes_ref.merge(edu_map.rename(columns={\"_nom_norm\":\"_nom_norm_edu\"}), on=\"code_commune\", how=\"left\")\n",
    "        if \"_nom_norm_edu\" in communes_ref.columns:\n",
    "            communes_ref[\"_nom_norm\"] = communes_ref[\"_nom_norm\"].fillna(communes_ref[\"_nom_norm_edu\"])\n",
    "            communes_ref.drop(columns=[\"_nom_norm_edu\"], inplace=True)\n",
    "\n",
    "tr_keys = [k for k in dfs if \"transport\" in k.lower() or \"idf\" in k.lower()]\n",
    "if tr_keys:\n",
    "    tr_map = extract_nom_code(dfs[tr_keys[0]])\n",
    "    if tr_map is not None:\n",
    "        communes_ref = communes_ref.merge(tr_map.rename(columns={\"_nom_norm\":\"_nom_norm_tr\"}), on=\"code_commune\", how=\"left\")\n",
    "        communes_ref[\"_nom_norm\"] = communes_ref[\"_nom_norm\"].fillna(communes_ref[\"_nom_norm_tr\"])\n",
    "        communes_ref.drop(columns=[\"_nom_norm_tr\"], inplace=True)\n",
    "\n",
    "after = communes_ref[\"_nom_norm\"].notna().mean()*100 if \"_nom_norm\" in communes_ref.columns else 0.0\n",
    "print(f\"üìù Couverture noms dans communes_ref : {before:.1f}% ‚Üí {after:.1f}%\")\n",
    "display(communes_ref.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d6739-425e-47c3-b313-12108ddcc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âTAPE 2 ‚Äî FUSION COMPL√àTE & LIAISON MULTI-BDD\n",
    "# Version finale avec matching flou (similarit√©)\n",
    "# ============================================================\n",
    "import pandas as pd, re, numpy as np, unicodedata\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ============================================================\n",
    "# üîß FONCTIONS UTILITAIRES\n",
    "# ============================================================\n",
    "def strip_acc(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def norm_commune_name(s):\n",
    "    if pd.isna(s): return None\n",
    "    s = strip_acc(str(s).upper().strip())\n",
    "    s = s.replace(\"‚Äô\",\"'\").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[\\s'-]\", \"\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{1})\\b\", r\"PARIS0\\1\", s)\n",
    "    s = re.sub(r\"\\bPARIS\\s?(\\d{2})\\b\", r\"PARIS\\1\", s)\n",
    "    return s or None\n",
    "\n",
    "def clean_insee(x):\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\.0$\", \"\", s)\n",
    "    s = re.sub(r\"\\D\", \"\", s)\n",
    "    return s if re.fullmatch(r\"\\d{5}\", s) else None\n",
    "\n",
    "def coverage_pct(keys, ref):\n",
    "    if len(keys) == 0: return 0.0\n",
    "    return round(keys.dropna().isin(ref).mean()*100, 2)\n",
    "\n",
    "def safe_num_cols(df, exclude):\n",
    "    return [c for c in df.columns if c not in exclude and df[c].dtype.kind in \"biufc\"]\n",
    "\n",
    "def build_nom2code(communes_ref):\n",
    "    \"\"\"Mapping nom_norm -> code_commune avec index unique\"\"\"\n",
    "    df = communes_ref.dropna(subset=[\"_nom_norm\",\"code_commune\"]).copy()\n",
    "    df = df.sort_values(\"code_commune\").drop_duplicates(\"_nom_norm\")\n",
    "    return df.set_index(\"_nom_norm\")[\"code_commune\"]\n",
    "\n",
    "def enrich_codes_by_similarity(df, communes_ref, nom_col=\"nom_commune\", seuil=0.8):\n",
    "    \"\"\"Matching flou pour trouver un code_commune via similarit√©\"\"\"\n",
    "    if nom_col not in df.columns: \n",
    "        return df\n",
    "    noms_ref = list(communes_ref[\"_nom_norm\"].dropna().unique())\n",
    "    map_nom2code = communes_ref.set_index(\"_nom_norm\")[\"code_commune\"].to_dict()\n",
    "\n",
    "    df[\"_nom_norm\"] = df[nom_col].map(norm_commune_name)\n",
    "    df[\"code_commune_fuzzy\"] = None\n",
    "\n",
    "    for i, nom in enumerate(df[\"_nom_norm\"]):\n",
    "        if pd.isna(nom): \n",
    "            continue\n",
    "        match = get_close_matches(nom, noms_ref, n=1, cutoff=seuil)\n",
    "        if match:\n",
    "            df.at[i, \"code_commune_fuzzy\"] = map_nom2code.get(match[0])\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# üîó INITIALISATION\n",
    "# ============================================================\n",
    "ref_codes = set(communes_ref[\"code_commune\"])\n",
    "print(f\"üìå communes_ref pr√™te : {len(ref_codes)} codes uniques | \"\n",
    "      f\"{round(communes_ref['_nom_norm'].notna().mean()*100,1)}% noms renseign√©s.\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ DVF ‚Äî Agr√©gation robuste\n",
    "# ============================================================\n",
    "dvf_keys = sorted([k for k in dfs if re.match(r\"^dvf_\\d{2}_clean$\", k, flags=re.I)])\n",
    "dvf_parts = []\n",
    "for k in dvf_keys:\n",
    "    df = dfs[k].copy()\n",
    "    base_cols = [c for c in [\"valeur_fonciere\",\"surface_reelle_bati\"] if c in df.columns]\n",
    "    code_cols = [c for c in [\"code_commune\",\"code_insee\",\"Code_commune\"] if c in df.columns]\n",
    "    name_cols = [c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in df.columns]\n",
    "    use_cols = list(dict.fromkeys(base_cols + code_cols + name_cols))\n",
    "    dvf_parts.append(df[use_cols])\n",
    "\n",
    "dvf_all = pd.concat(dvf_parts, ignore_index=True)\n",
    "dvf_all[\"code_commune\"] = None\n",
    "for c in [\"code_commune\",\"code_insee\",\"Code_commune\"]:\n",
    "    if c in dvf_all.columns:\n",
    "        dvf_all[\"code_commune\"] = dvf_all[\"code_commune\"].fillna(dvf_all[c].map(clean_insee))\n",
    "\n",
    "name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in dvf_all.columns), None)\n",
    "if name_col and \"_nom_norm\" in communes_ref.columns:\n",
    "    dvf_all[\"_nom_norm\"] = dvf_all[name_col].map(norm_commune_name)\n",
    "    map_nom2code = build_nom2code(communes_ref)\n",
    "    dvf_all.loc[dvf_all[\"code_commune\"].isna(), \"code_commune\"] = dvf_all[\"_nom_norm\"].map(map_nom2code)\n",
    "\n",
    "for c in [\"valeur_fonciere\",\"surface_reelle_bati\"]:\n",
    "    if c in dvf_all.columns:\n",
    "        dvf_all[c] = pd.to_numeric(dvf_all[c], errors=\"coerce\")\n",
    "\n",
    "dvf_ok = dvf_all.dropna(subset=[\"code_commune\",\"valeur_fonciere\",\"surface_reelle_bati\"])\n",
    "dvf_ok = dvf_ok[\n",
    "    (dvf_ok[\"surface_reelle_bati\"].between(8,400)) &\n",
    "    (dvf_ok[\"valeur_fonciere\"].between(10_000,10_000_000))\n",
    "]\n",
    "agg_dvf = (dvf_ok.groupby(\"code_commune\", as_index=False)\n",
    "           .agg(somme_valeur=(\"valeur_fonciere\",\"sum\"),\n",
    "                somme_surface=(\"surface_reelle_bati\",\"sum\"),\n",
    "                n_ventes=(\"surface_reelle_bati\",\"size\")))\n",
    "agg_dvf[\"prix_m2\"] = agg_dvf[\"somme_valeur\"] / agg_dvf[\"somme_surface\"]\n",
    "print(f\"‚úÖ DVF agr√©g√© : {agg_dvf.shape} | prix_m2 non-nuls: {round(agg_dvf['prix_m2'].notna().mean()*100,2)}%\")\n",
    "base = agg_dvf.copy()\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ AIRPARIF ‚Äî Pollution\n",
    "# ============================================================\n",
    "air_key = [k for k in dfs if \"air_parif_communes\" in k.lower()]\n",
    "if air_key:\n",
    "    air = dfs[air_key[0]].copy()\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"ninsee\",\"codeinsee\"] if c in air.columns), None)\n",
    "    air[\"code_commune\"] = air[code_col].map(clean_insee)\n",
    "    num_cols = safe_num_cols(air, {\"code_commune\"})\n",
    "    air_agg = air.dropna(subset=[\"code_commune\"]).groupby(\"code_commune\", as_index=False)[num_cols].mean()\n",
    "    base = base.merge(air_agg, on=\"code_commune\", how=\"left\")\n",
    "    print(\"‚úÖ base + Airparif :\", base.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ DPE ‚Äî √ânergie\n",
    "# ============================================================\n",
    "dpe_key = [k for k in dfs if \"dpe\" in k.lower()]\n",
    "if dpe_key:\n",
    "    dpe = dfs[dpe_key[0]].copy()\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"Code_commune\",\"ninsee\",\"codeinsee\"] if c in dpe.columns), None)\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in dpe.columns), None)\n",
    "    dpe[\"code_commune\"] = dpe[code_col].map(clean_insee) if code_col else None\n",
    "    if dpe[\"code_commune\"].isna().any() and name_col and \"_nom_norm\" in communes_ref.columns:\n",
    "        dpe[\"_nom_norm\"] = dpe[name_col].map(norm_commune_name)\n",
    "        map_nom2code = build_nom2code(communes_ref)\n",
    "        dpe.loc[dpe[\"code_commune\"].isna(), \"code_commune\"] = dpe[\"_nom_norm\"].map(map_nom2code)\n",
    "    class_cols = [c for c in dpe.columns if \"classe\" in c.lower()]\n",
    "    if class_cols:\n",
    "        cc = class_cols[0]\n",
    "        dpe[\"_is_FG\"] = dpe[cc].astype(str).str.upper().str.strip().isin([\"F\",\"G\"])\n",
    "        dpe_agg = (dpe.dropna(subset=[\"code_commune\"])\n",
    "                   .groupby(\"code_commune\", as_index=False)\n",
    "                   .agg(nb_dpe=(\"code_commune\",\"size\"),\n",
    "                        part_FG=(\"_is_FG\",\"mean\")))\n",
    "        base = base.merge(dpe_agg, on=\"code_commune\", how=\"left\")\n",
    "        print(\"‚úÖ base + DPE :\", base.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ √âDUCATION\n",
    "# ============================================================\n",
    "edu_keys = [k for k in dfs if \"annuaire_education\" in k.lower() or \"etablissement\" in k.lower()]\n",
    "if edu_keys:\n",
    "    edu = dfs[edu_keys[0]].copy()\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in edu.columns), None)\n",
    "    if name_col and \"_nom_norm\" in communes_ref.columns:\n",
    "        edu[\"_nom_norm\"] = edu[name_col].map(norm_commune_name)\n",
    "        map_nom2code = build_nom2code(communes_ref)\n",
    "        edu[\"code_commune\"] = edu[\"_nom_norm\"].map(map_nom2code)\n",
    "        edu = enrich_codes_by_similarity(edu, communes_ref, nom_col=name_col)\n",
    "        edu[\"code_commune\"] = edu[\"code_commune\"].fillna(edu[\"code_commune_fuzzy\"])\n",
    "        edu.drop(columns=[\"code_commune_fuzzy\"], inplace=True)\n",
    "        edu_agg = edu.dropna(subset=[\"code_commune\"]).groupby(\"code_commune\", as_index=False).size().rename(columns={\"size\":\"nb_etabs\"})\n",
    "        base = base.merge(edu_agg, on=\"code_commune\", how=\"left\")\n",
    "        print(\"‚úÖ base + √âducation :\", base.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ TRANSPORTS\n",
    "# ============================================================\n",
    "tr_keys = [k for k in dfs if \"transport\" in k.lower() or \"idf\" in k.lower()]\n",
    "if tr_keys:\n",
    "    tr = dfs[tr_keys[0]].copy()\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in tr.columns), None)\n",
    "    if name_col and \"_nom_norm\" in communes_ref.columns:\n",
    "        tr[\"_nom_norm\"] = tr[name_col].map(norm_commune_name)\n",
    "        map_nom2code = build_nom2code(communes_ref)\n",
    "        tr[\"code_commune\"] = tr[\"_nom_norm\"].map(map_nom2code)\n",
    "        tr = enrich_codes_by_similarity(tr, communes_ref, nom_col=name_col)\n",
    "        tr[\"code_commune\"] = tr[\"code_commune\"].fillna(tr[\"code_commune_fuzzy\"])\n",
    "        tr.drop(columns=[\"code_commune_fuzzy\"], inplace=True)\n",
    "        if \"mode\" in tr.columns:\n",
    "            weights = {\"RER\":3,\"TRAIN\":3,\"METRO\":2,\"TRAM\":1,\"BUS\":0.5}\n",
    "            tr[\"_w\"] = tr[\"mode\"].astype(str).str.upper().map(weights).fillna(0.5)\n",
    "            tr_agg = tr.groupby(\"code_commune\", as_index=False)[\"_w\"].sum().rename(columns={\"_w\":\"indice_connectivite\"})\n",
    "        else:\n",
    "            tr_agg = tr.groupby(\"code_commune\", as_index=False).size().rename(columns={\"size\":\"nb_arrets\"})\n",
    "        base = base.merge(tr_agg, on=\"code_commune\", how=\"left\")\n",
    "        print(\"‚úÖ base + Transports :\", base.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ D√âLINQUANCE\n",
    "# ============================================================\n",
    "del_keys = [k for k in dfs if \"delin\" in k.lower()]\n",
    "if del_keys:\n",
    "    crim = dfs[del_keys[0]].copy()\n",
    "    code_col = next((c for c in [\"code_commune\",\"code_insee\",\"Code_commune\",\"ninsee\",\"codeinsee\"] if c in crim.columns), None)\n",
    "    name_col = next((c for c in [\"nom_commune\",\"commune\",\"ville\",\"Commune\"] if c in crim.columns), None)\n",
    "    crim[\"code_commune\"] = crim[code_col].map(clean_insee) if code_col else None\n",
    "    if crim[\"code_commune\"].isna().any() and name_col and \"_nom_norm\" in communes_ref.columns:\n",
    "        crim[\"_nom_norm\"] = crim[name_col].map(norm_commune_name)\n",
    "        map_nom2code = build_nom2code(communes_ref)\n",
    "        crim.loc[crim[\"code_commune\"].isna(), \"code_commune\"] = crim[\"_nom_norm\"].map(map_nom2code)\n",
    "    crim = enrich_codes_by_similarity(crim, communes_ref, nom_col=name_col)\n",
    "\n",
    "    # si la colonne floue existe, on fusionne les infos\n",
    "    if \"code_commune_fuzzy\" in crim.columns:\n",
    "        crim[\"code_commune\"] = crim[\"code_commune\"].fillna(crim[\"code_commune_fuzzy\"])\n",
    "        crim.drop(columns=[\"code_commune_fuzzy\"], inplace=True)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Aucun enrichissement fuzzy pour D√©linquance (colonne absente).\")\n",
    "\n",
    "    if {\"faits_total\",\"population\"}.issubset(crim.columns):\n",
    "        crim_agg = (crim.dropna(subset=[\"code_commune\"])\n",
    "                    .groupby(\"code_commune\", as_index=False)\n",
    "                    .agg(faits_total=(\"faits_total\",\"sum\"),\n",
    "                         population=(\"population\",\"max\")))\n",
    "        crim_agg[\"taux_criminalite_pour_1k\"] = (crim_agg[\"faits_total\"]/crim_agg[\"population\"])*1000\n",
    "        base = base.merge(crim_agg[[\"code_commune\",\"taux_criminalite_pour_1k\"]], on=\"code_commune\", how=\"left\")\n",
    "        print(\"‚úÖ base + D√©linquance :\", base.shape)\n",
    "\n",
    "# ============================================================\n",
    "# 7Ô∏è‚É£ EXPORT & BILAN FINAL\n",
    "# ============================================================\n",
    "drop_cols = [c for c in [\"somme_valeur\",\"somme_surface\"] if c in base.columns]\n",
    "if drop_cols:\n",
    "    base.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "print(\"\\n===== BILAN FINAL =====\")\n",
    "print(f\"Communes finales: {base['code_commune'].nunique()} | Lignes: {len(base)}\")\n",
    "for col in [\"prix_m2\",\"part_FG\",\"zone_encadree\",\"indice_connectivite\",\"taux_criminalite_pour_1k\"]:\n",
    "    if col in base.columns:\n",
    "        print(f\"{col}: {round(base[col].notna().mean()*100,2)}% non-nuls\")\n",
    "\n",
    "base.to_csv(\"data/master_table.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Export ‚Üí data/master_table.csv\")\n",
    "display(base.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f7ecb-4980-48e2-a099-d46390be4093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# √âTAPE 3 ‚Äî ANALYSE & INDICATEURS COMPOSITES (robuste)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚ñ∂Ô∏è Chargement du master‚Ä¶\")\n",
    "base = pd.read_csv(\"data/master_table.csv\", dtype={\"code_commune\": str})\n",
    "print(f\"‚úÖ {len(base)} lignes, {base['code_commune'].nunique()} communes\\n\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def to_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def scale_0_100_robust(series, higher_is_better=True):\n",
    "    \"\"\"Min‚ÄìMax avec garde-fous:\n",
    "       - <3 valeurs non-null -> NaN\n",
    "       - s√©rie constante -> 50 pour tous (ou 100 si higher_is_better et valeur == 0 pour un \"risque\" type part_FG)\n",
    "       - coupe aux percentiles 2‚Äì98 pour limiter les outliers\n",
    "    \"\"\"\n",
    "    s = series.astype(float).copy()\n",
    "    n = s.notna().sum()\n",
    "    if n < 3:\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "\n",
    "    # clamp aux percentiles\n",
    "    lo, hi = np.nanpercentile(s, [2, 98])\n",
    "    s = s.clip(lo, hi)\n",
    "\n",
    "    vmin, vmax = np.nanmin(s), np.nanmax(s)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax):\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "\n",
    "    if np.isclose(vmin, vmax, atol=1e-12):\n",
    "        # si tout identique -> 50 neutre\n",
    "        # cas utile: part_FG = 0 partout => 100 (meilleur) quand higher_is_better=False\n",
    "        if higher_is_better:\n",
    "            return pd.Series(50.0, index=s.index)\n",
    "        else:\n",
    "            # valeur faible = mieux ‚áí tout 100 si constant\n",
    "            return pd.Series(100.0, index=s.index)\n",
    "\n",
    "    if higher_is_better:\n",
    "        scaled = 100 * (s - vmin) / (vmax - vmin)\n",
    "    else:\n",
    "        scaled = 100 * (vmax - s) / (vmax - vmin)\n",
    "    return scaled\n",
    "\n",
    "def safe_col(df, name, default=np.nan):\n",
    "    return df[name] if name in df.columns else pd.Series(default, index=df.index)\n",
    "\n",
    "# ---------- Pr√©paration ----------\n",
    "num_cols = [\"prix_m2\",\"n_ventes\",\"part_FG\",\"no2\",\"pm10\",\"o3\",\n",
    "            \"nb_etabs\",\"indice_connectivite\",\"nb_arrets\",\n",
    "            \"taux_criminalite_pour_1k\",\"population\",\"nb_dpe\"]\n",
    "base = to_numeric(base, [c for c in num_cols if c in base.columns])\n",
    "\n",
    "# Densit√© d'√©tablissements si population dispo\n",
    "if \"population\" in base.columns and base[\"population\"].notna().any():\n",
    "    base[\"etabs_par_10k\"] = (safe_col(base, \"nb_etabs\", 0) / base[\"population\"]) * 10_000\n",
    "else:\n",
    "    base[\"etabs_par_10k\"] = np.nan\n",
    "\n",
    "# ---------- Scores unitaires ----------\n",
    "# March√©\n",
    "base[\"score_prix\"]      = scale_0_100_robust(safe_col(base, \"prix_m2\"), higher_is_better=False)  # moins cher = mieux\n",
    "base[\"score_liquidite\"] = scale_0_100_robust(safe_col(base, \"n_ventes\"), higher_is_better=True)\n",
    "base[\"score_marche\"]    = 0.6*base[\"score_prix\"] + 0.4*base[\"score_liquidite\"]\n",
    "\n",
    "# √ânergie\n",
    "base[\"score_energie\"] = 100 - scale_0_100_robust(safe_col(base, \"part_FG\"), higher_is_better=True)\n",
    "if \"nb_dpe\" in base.columns:\n",
    "    boost = scale_0_100_robust(base[\"nb_dpe\"], higher_is_better=True) * 0.1  # +10% max si gros volume DPE\n",
    "    base[\"score_energie\"] = np.clip(base[\"score_energie\"]*0.9 + boost, 0, 100)\n",
    "\n",
    "# Pollution (plus bas = mieux)\n",
    "poll_inputs = [c for c in [\"no2\",\"pm10\",\"o3\"] if c in base.columns]\n",
    "if poll_inputs:\n",
    "    base[\"score_pollution\"] = scale_0_100_robust(base[poll_inputs].mean(axis=1), higher_is_better=False)\n",
    "else:\n",
    "    base[\"score_pollution\"] = np.nan\n",
    "\n",
    "# √âducation / attractivit√©\n",
    "base[\"score_education\"] = scale_0_100_robust(\n",
    "    base[\"etabs_par_10k\"].fillna(safe_col(base, \"nb_etabs\")), higher_is_better=True\n",
    ")\n",
    "\n",
    "# Transports\n",
    "if \"indice_connectivite\" in base.columns and base[\"indice_connectivite\"].notna().any():\n",
    "    base[\"score_transport\"] = scale_0_100_robust(base[\"indice_connectivite\"], higher_is_better=True)\n",
    "elif \"nb_arrets\" in base.columns and base[\"nb_arrets\"].notna().any():\n",
    "    base[\"score_transport\"] = scale_0_100_robust(base[\"nb_arrets\"], higher_is_better=True)\n",
    "else:\n",
    "    base[\"score_transport\"] = np.nan\n",
    "\n",
    "# S√©curit√©\n",
    "if \"taux_criminalite_pour_1k\" in base.columns and base[\"taux_criminalite_pour_1k\"].notna().any():\n",
    "    base[\"score_securite\"] = 100 - scale_0_100_robust(base[\"taux_criminalite_pour_1k\"], higher_is_better=True)\n",
    "else:\n",
    "    base[\"score_securite\"] = np.nan\n",
    "\n",
    "# Encadrement (p√©nalit√©)\n",
    "penality = (base[\"zone_encadree\"].fillna(False).astype(bool).map({True: -10.0, False: 0.0})\n",
    "            if \"zone_encadree\" in base.columns else 0.0)\n",
    "\n",
    "# ---------- Score global (pond√©rations auto-normalis√©es sur ce qui existe) ----------\n",
    "weights = {\n",
    "    \"score_marche\":    0.35,\n",
    "    \"score_transport\": 0.15,\n",
    "    \"score_energie\":   0.15,\n",
    "    \"score_education\": 0.10,\n",
    "    \"score_securite\":  0.15,\n",
    "    \"score_pollution\": 0.10,\n",
    "}\n",
    "present = {k:v for k,v in weights.items() if k in base.columns and base[k].notna().any()}\n",
    "w_sum = sum(present.values()) or 1.0\n",
    "present = {k: v/w_sum for k,v in present.items()}\n",
    "\n",
    "base[\"score_global\"] = 0.0\n",
    "for k, w in present.items():\n",
    "    # remplace NaN par m√©diane de la colonne pour ne pas annuler le score\n",
    "    base[\"score_global\"] = base[\"score_global\"] + w * base[k].fillna(base[k].median())\n",
    "\n",
    "base[\"score_global\"] = np.clip(base[\"score_global\"] + penality, 0, 100)\n",
    "\n",
    "# ---------- Contr√¥les & exports ----------\n",
    "scores_cols = [\"score_marche\",\"score_transport\",\"score_energie\",\"score_education\",\n",
    "               \"score_securite\",\"score_pollution\",\"score_global\"]\n",
    "\n",
    "print(\"üîé Couverture des scores (non-nuls) :\")\n",
    "for c in scores_cols:\n",
    "    if c in base.columns:\n",
    "        print(f\" - {c}: {round(base[c].notna().mean()*100,1)}%\")\n",
    "\n",
    "top = base.sort_values(\"score_global\", ascending=False).head(10)\n",
    "bottom = base.sort_values(\"score_global\", ascending=True).head(10)\n",
    "\n",
    "print(\"\\nüèÜ TOP 10 (toutes communes disponibles)\"); \n",
    "display(top[[\"code_commune\",\"prix_m2\",\"score_global\"] + [c for c in scores_cols if c!=\"score_global\"]])\n",
    "print(\"\\n‚ö†Ô∏è BOTTOM 10\"); \n",
    "display(bottom[[\"code_commune\",\"prix_m2\",\"score_global\"] + [c for c in scores_cols if c!=\"score_global\"]])\n",
    "\n",
    "out = \"data/master_with_scores.csv\"\n",
    "base.to_csv(out, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Export enrichi ‚Üí {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa565353-d9c4-470c-ab8b-f5dd0b7f02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üßπ Nettoyage m√©moire s√©curis√©\n",
    "# ============================================================\n",
    "import gc\n",
    "\n",
    "# Lib√©rer toutes les variables sauf les modules\n",
    "for name in list(globals().keys()):\n",
    "    if not name.startswith(\"_\") and name not in [\"gc\", \"pd\", \"np\"]:\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "print(\"‚úÖ M√©moire RAM lib√©r√©e avec succ√®s (variables temporaires supprim√©es).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af18280-5db2-450d-bb2a-a66263d2779b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

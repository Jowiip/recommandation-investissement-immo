{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d89235e-dccb-4410-87ba-504a6bfbc673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç V√©rification des fichiers DVF d√©partementaux...\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_75.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_77.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_78.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_91.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_92.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_93.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_94.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dvf_95.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "\n",
      "üîç V√©rification des bases compl√©mentaires...\n",
      "\n",
      "üîπ INSEE - Donn√©es socio-√©conomiques\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/insee_dossier_complet.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ DPE - Diagnostic √©nerg√©tique\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/dpe_logement.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ Airparif - Qualit√© de l‚Äôair\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/air_parif_communes.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ Bruitparif - Niveau sonore\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/bruitsparifs_communes.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ Encadrement des loyers (Paris)\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/encadrement_des_loyers_paris.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ D√©linquance - S√©curit√©\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/delinquance_communes.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ Transports - √éle-de-France\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/transports_idf.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ √âducation - Annuaire √©tablissements\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/fr-en-annuaire-education.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n",
      "üîπ √âducation - Principaux √©tablissements\n",
      "----------------------------------------------------\n",
      "üìÑ Test du fichier : data/fr-en-principaux-etablissement.csv\n",
      "‚ùå Fichier introuvable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Fonction utilitaire ---\n",
    "def test_import(nom_fichier, sep=',', nrows=5):\n",
    "    \"\"\"\n",
    "    Teste le chargement d'un fichier CSV :\n",
    "    - V√©rifie s'il existe\n",
    "    - Essaie de lire les premi√®res lignes\n",
    "    - Affiche la taille, les colonnes, et un extrait\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(f\"üìÑ Test du fichier : {nom_fichier}\")\n",
    "\n",
    "    if not os.path.exists(nom_fichier):\n",
    "        print(\"‚ùå Fichier introuvable.\\n\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(nom_fichier, sep=sep, nrows=nrows, low_memory=False)\n",
    "        print(f\"‚úÖ Chargement r√©ussi ({df.shape[0]} lignes √ó {df.shape[1]} colonnes)\")\n",
    "        print(\"   ‚ûú Colonnes :\", list(df.columns[:10]))\n",
    "        print(df.head(2), \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du chargement : {e}\\n\")\n",
    "\n",
    "\n",
    "dvf_files = [f\"data/dvf_{dep}.csv\" for dep in [\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"]]\n",
    "\n",
    "print(\"üîç V√©rification des fichiers DVF d√©partementaux...\\n\")\n",
    "for fichier in dvf_files:\n",
    "    test_import(fichier, sep=\",\")\n",
    "\n",
    "autres_bases = [\n",
    "    {\"nom\": \"INSEE - Donn√©es socio-√©conomiques\", \"path\": \"data/insee_dossier_complet.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"DPE - Diagnostic √©nerg√©tique\", \"path\": \"data/dpe_logement.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"Airparif - Qualit√© de l‚Äôair\", \"path\": \"data/air_parif_communes.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Bruitparif - Niveau sonore\", \"path\": \"data/bruitsparifs_communes.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"Encadrement des loyers (Paris)\", \"path\": \"data/encadrement_des_loyers_paris.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"D√©linquance - S√©curit√©\", \"path\": \"data/delinquance_communes.csv\", \"sep\": \",\"},\n",
    "    {\"nom\": \"Transports - √éle-de-France\", \"path\": \"data/transports_idf.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"√âducation - Annuaire √©tablissements\", \"path\": \"data/fr-en-annuaire-education.csv\", \"sep\": \";\"},\n",
    "    {\"nom\": \"√âducation - Principaux √©tablissements\", \"path\": \"data/fr-en-principaux-etablissement.csv\", \"sep\": \";\"}\n",
    "]\n",
    "\n",
    "print(\"\\nüîç V√©rification des bases compl√©mentaires...\\n\")\n",
    "for base in autres_bases:\n",
    "    print(f\"üîπ {base['nom']}\")\n",
    "    test_import(base[\"path\"], sep=base[\"sep\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0b50a-5e73-4fd4-9ed0-e77e6d46f7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Fichiers DVF trouv√©s : 7\n",
      "   - data/clean/dvf_75_clean.csv\n",
      "   - data/clean/dvf_77_clean.csv\n",
      "   - data/clean/dvf_91_clean.csv\n",
      "   - data/clean/dvf_92_clean.csv\n",
      "   - data/clean/dvf_93_clean.csv\n",
      "   - data/clean/dvf_94_clean.csv\n",
      "   - data/clean/dvf_95_clean.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) O√π √©crire le fichier fusionn√© ?\n",
    "# Choisis l'un des deux chemins de sortie (d√©-commente celui que tu veux garder) :\n",
    "# out_path = Path(\"data/raw/dvf_idf.csv\")   # si tu as ce dossier\n",
    "out_path = Path(\"dvf_idf.csv\")              # √† la racine du projet (m√™me dossier que tes dvf_75.csv, etc.)\n",
    "\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Trouver automatiquement les fichiers DVF d√©partementaux\n",
    "here = Path(\".\")\n",
    "dvf_files = list(here.glob(\"dvf_*.csv\"))\n",
    "if not dvf_files:\n",
    "    # si rien au niveau racine, on cherche en profondeur (sous-dossiers)\n",
    "    dvf_files = list(here.rglob(\"dvf_*.csv\"))\n",
    "\n",
    "print(\"üìÇ Fichiers DVF trouv√©s :\", len(dvf_files))\n",
    "for f in dvf_files:\n",
    "    print(\"   -\", f.relative_to(here))\n",
    "\n",
    "if not dvf_files:\n",
    "    raise FileNotFoundError(\"Aucun fichier 'dvf_*.csv' trouv√©. V√©rifie les noms/chemins.\")\n",
    "\n",
    "# 3) Lecture robuste (encodage + s√©parateur)\n",
    "def robust_read(path):\n",
    "    try:\n",
    "        return pd.read_csv(path, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, sep=\";\", encoding=\"latin-1\", low_memory=False)\n",
    "        except Exception:\n",
    "            # dernier essai: on tente s√©parateur virgule\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "            except Exception:\n",
    "                return pd.read_csv(path, sep=\",\", encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "dfs = []\n",
    "for f in dvf_files:\n",
    "    df = robust_read(f)\n",
    "    # Normalisation minimale utile pour DVF\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "# 4) Fusion\n",
    "dvf_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"‚úÖ Fusion r√©alis√©e :\", dvf_all.shape)\n",
    "\n",
    "# 5) Sauvegarde\n",
    "dvf_all.to_csv(out_path, index=False)\n",
    "print(\"üíæ Fichier fusionn√© sauvegard√© sous :\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af704c13-2614-4f48-aef5-2fce8f4629ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"üîç Test air_parif_communes.csv\")\n",
    "\n",
    "try:\n",
    "    df_air = pd.read_csv(\n",
    "        \"data/air_parif_communes.csv\",\n",
    "        sep=',',\n",
    "        encoding='utf-8',\n",
    "        engine='python'\n",
    "    )\n",
    "    print(\"‚úÖ Chargement r√©ussi :\", df_air.shape)\n",
    "    display(df_air.head(5))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Erreur UTF-8 :\", e)\n",
    "    print(\"üîÅ Nouvelle tentative avec encodage latin-1 et s√©parateur ';'\")\n",
    "    try:\n",
    "        df_air = pd.read_csv(\n",
    "            \"data/air_parif_communes.csv\",\n",
    "            sep=';',\n",
    "            encoding='latin-1',\n",
    "            engine='python'\n",
    "        )\n",
    "        print(\"‚úÖ Chargement r√©ussi (latin-1) :\", df_air.shape)\n",
    "        display(df_air.head(5))\n",
    "    except Exception as e2:\n",
    "        print(\"‚ùå Toujours erreur :\", e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee13506-bc05-48fc-bd11-8b3bdcb70032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 1 : SETUP + VERIFICATIONS AUTOMATIQUES ===\n",
    "# Objectif : v√©rifier l'environnement, pr√©parer les dossiers, d√©tecter les CSV, et tester une lecture l√©g√®re\n",
    "\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"‚úÖ {pkg} d√©j√† install√©\")\n",
    "    except ImportError:\n",
    "        print(f\"‚è≥ Installation de {pkg}‚Ä¶\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        print(f\"‚úÖ {pkg} install√©\")\n",
    "\n",
    "# 1) D√©pendances minimales\n",
    "for p in [\"pandas\", \"numpy\"]:\n",
    "    ensure(p)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "print(\"\\nüì¶ Versions\")\n",
    "print(\" - Python :\", sys.version.split()[0])\n",
    "print(\" - pandas :\", pd.__version__)\n",
    "print(\" - numpy  :\", np.__version__)\n",
    "\n",
    "# 2) Dossiers projet\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CLEAN_DIR = DATA_DIR / \"clean\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nüìÅ Dossiers\")\n",
    "print(\" - Projet :\", BASE_DIR)\n",
    "print(\" - Donn√©es :\", DATA_DIR)\n",
    "print(\" - Nettoy√©s:\", CLEAN_DIR)\n",
    "\n",
    "# 3) D√©tection automatique de TOUS les CSV (sauf ceux d√©j√† nettoy√©s)\n",
    "FILES = {\n",
    "    f.stem.replace(\"-\", \"_\"): f\n",
    "    for f in DATA_DIR.glob(\"*.csv\")\n",
    "    if not str(f).startswith(str(CLEAN_DIR))\n",
    "}\n",
    "\n",
    "print(\"\\nüîé CSV d√©tect√©s dans 'data/' (hors 'data/clean/'):\")\n",
    "if not FILES:\n",
    "    print(\"   ‚ö†Ô∏è Aucun fichier .csv trouv√© dans data/. Place tes fichiers ici, puis relance cette cellule.\")\n",
    "else:\n",
    "    for k, p in sorted(FILES.items()):\n",
    "        print(f\"   ‚Ä¢ {k:35s} ‚Üí {p.name}\")\n",
    "\n",
    "# 4) Mini test de lecture (robuste) sur chaque CSV : 2 lignes max\n",
    "def test_preview(path: Path):\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\"]\n",
    "    seps = [\";\", \",\", \"\\t\", \"|\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        for sep in seps:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, encoding=enc, nrows=2, )\n",
    "                # Heuristique anti faux-positif : si 1 colonne unique tr√®s longue -> mauvais s√©parateur\n",
    "                if df.shape[1] == 1:\n",
    "                    continue\n",
    "                return True, enc, sep, df.columns.tolist()[:6]\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "    return False, None, None, str(last_err)\n",
    "\n",
    "print(\"\\nüß™ Test de lecture rapide (2 lignes) :\")\n",
    "if FILES:\n",
    "    ok_all = True\n",
    "    for k, p in sorted(FILES.items()):\n",
    "        ok, enc, sep, info = test_preview(p)\n",
    "        if ok:\n",
    "            print(f\"   ‚úÖ {p.name:35s} | enc='{enc}', sep='{sep}' | colonnes: {info}\")\n",
    "        else:\n",
    "            ok_all = False\n",
    "            print(f\"   ‚ùå {p.name:35s} | lecture impossible (dernier message: {info})\")\n",
    "    if ok_all:\n",
    "        print(\"\\nüéâ ETAPE 1 OK : environnement pr√™t, dossiers en place, CSV d√©tect√©s et lisibles.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ETAPE 1 PARTIELLE : certains fichiers ne se lisent pas en preview. On pourra les traiter au cas par cas au nettoyage.\")\n",
    "else:\n",
    "    print(\"   (aucun fichier √† tester)\")\n",
    "\n",
    "# 5) Expose les variables pour les √©tapes suivantes\n",
    "globals()[\"DATA_DIR\"] = DATA_DIR\n",
    "globals()[\"CLEAN_DIR\"] = CLEAN_DIR\n",
    "globals()[\"FILES\"] = FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a950e-64f2-41dc-bf63-b3312ff5a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 2 : D√âCLARATION AUTOMATIQUE DES FICHIERS + SCH√âMAS DE NETTOYAGE ===\n",
    "# Objectif : identifier tous les fichiers de donn√©es dans \"data/\", d√©finir des sch√©mas adapt√©s,\n",
    "# et pr√©parer le pipeline pour le nettoyage (√©tape 4).\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- R√©utilisation des dossiers de l'√©tape 1\n",
    "DATA_RAW = DATA_DIR\n",
    "DATA_CLEAN = CLEAN_DIR\n",
    "\n",
    "# === 1Ô∏è‚É£ D√©tection automatique des fichiers CSV ===\n",
    "FILES = {\n",
    "    f.stem.replace(\"-\", \"_\"): f\n",
    "    for f in DATA_RAW.glob(\"*.csv\")\n",
    "    if not str(f).startswith(str(DATA_CLEAN))  # on ignore les fichiers d√©j√† nettoy√©s\n",
    "}\n",
    "\n",
    "print(\"üìÇ Fichiers d√©tect√©s automatiquement :\")\n",
    "for key, path in sorted(FILES.items()):\n",
    "    print(f\"   ‚Ä¢ {key:30s} ‚Üí {path.name}\")\n",
    "print(f\"\\nTotal : {len(FILES)} fichiers d√©tect√©s dans {DATA_RAW}\\n\")\n",
    "\n",
    "\n",
    "# === 2Ô∏è‚É£ Sch√©mas personnalis√©s pour certaines bases connues ===\n",
    "SCHEMAS_CUSTOM = {\n",
    "    \"dvf_idf\": {\n",
    "        \"rename\": {\n",
    "            \"valeur_fonciere\": \"valeur_fonciere\",\n",
    "            \"surface_reelle_bati\": \"surface_reelle_bati\",\n",
    "            \"type_local\": \"type_local\",\n",
    "            \"nombre_pieces_principales\": \"nb_pieces\",\n",
    "            \"date_mutation\": \"date_mutation\",\n",
    "            \"commune\": \"commune\",\n",
    "        },\n",
    "        \"dtype\": {\n",
    "            \"valeur_fonciere\": \"float\",\n",
    "            \"surface_reelle_bati\": \"float\",\n",
    "            \"nb_pieces\": \"Int64\",\n",
    "        },\n",
    "        \"parse_dates\": [\"date_mutation\"],\n",
    "        \"drop_dupes_on\": [\"code_commune\", \"date_mutation\", \"valeur_fonciere\", \"surface_reelle_bati\"],\n",
    "    },\n",
    "    \"delinquance_communes\": {\n",
    "        \"rename\": {\n",
    "            \"faits_total\": \"faits_total\",\n",
    "            \"population\": \"population\",\n",
    "            \"annee\": \"annee\",\n",
    "            \"commune\": \"commune\",\n",
    "            \"taux_criminalite\": \"taux_criminalite\",\n",
    "        },\n",
    "        \"dtype\": {\n",
    "            \"faits_total\": \"Int64\",\n",
    "            \"population\": \"Int64\",\n",
    "            \"annee\": \"Int64\",\n",
    "        },\n",
    "        \"drop_dupes_on\": [\"code_commune\", \"annee\"],\n",
    "    },\n",
    "    \"air_parif_communes\": {\n",
    "        \"rename\": {\"indice_airparif\": \"indice_airparif\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {\"indice_airparif\": \"float\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"dpe_logement\": {\n",
    "        \"rename\": {\"date_visite_diagnostiqueur\": \"date_dpe\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"insee_dossier_complet\": {\n",
    "        \"rename\": {\n",
    "            \"revenu_median\": \"revenu_median\",\n",
    "            \"taux_chomage\": \"taux_chomage\",\n",
    "            \"population\": \"population\",\n",
    "            \"commune\": \"commune\",\n",
    "        },\n",
    "        \"dtype\": {\"revenu_median\": \"float\", \"taux_chomage\": \"float\", \"population\": \"Int64\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "    \"transports_idf\": {\n",
    "        \"rename\": {\"nb_arrets\": \"nb_arrets\", \"score_connectivite\": \"score_connectivite\", \"commune\": \"commune\"},\n",
    "        \"dtype\": {\"nb_arrets\": \"Int64\", \"score_connectivite\": \"float\"},\n",
    "        \"drop_dupes_on\": [\"code_commune\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# === 3Ô∏è‚É£ Sch√©ma par d√©faut pour toutes les autres bases ===\n",
    "DEFAULT_SCHEMA = {\n",
    "    \"rename\": {},\n",
    "    \"dtype\": {},\n",
    "    \"drop_dupes_on\": [\"code_commune\"],\n",
    "}\n",
    "\n",
    "# === 4Ô∏è‚É£ G√©n√©ration automatique des sch√©mas ===\n",
    "SCHEMAS = {}\n",
    "for key in FILES.keys():\n",
    "    SCHEMAS[key] = SCHEMAS_CUSTOM.get(key, DEFAULT_SCHEMA)\n",
    "\n",
    "print(\"üìò Sch√©mas g√©n√©r√©s :\")\n",
    "for k in sorted(SCHEMAS.keys()):\n",
    "    base_type = \"üéØ personnalis√©\" if k in SCHEMAS_CUSTOM else \"‚öôÔ∏è  g√©n√©rique\"\n",
    "    print(f\"   {k:30s} ‚Üí {base_type}\")\n",
    "print(f\"\\nTotal : {len(SCHEMAS)} sch√©mas charg√©s\\n\")\n",
    "\n",
    "\n",
    "# === 5Ô∏è‚É£ V√©rification rapide de coh√©rence ===\n",
    "missing = [k for k in FILES if k not in SCHEMAS]\n",
    "if missing:\n",
    "    print(\"‚ö†Ô∏è Bases sans sch√©ma associ√© :\", missing)\n",
    "else:\n",
    "    print(\"‚úÖ Toutes les bases d√©tect√©es ont un sch√©ma associ√© (automatique ou personnalis√©).\")\n",
    "\n",
    "# --- Expose les variables pour les √©tapes suivantes\n",
    "globals()[\"FILES\"] = FILES\n",
    "globals()[\"SCHEMAS\"] = SCHEMAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b676b-1654-4236-95b3-9490c6784d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 3 : NETTOYAGE AUTOMATIQUE ULTRA OPTIMIS√â (GROS FICHIERS) ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import csv, time, sys\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "CHUNK_SIZE = 30000  # plus petit = plus fluide\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "print(\"üöÄ Mode optimisation activ√© : lecture par petits paquets + √©criture directe\\n\")\n",
    "\n",
    "def normalize_colnames(cols):\n",
    "    def _norm(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        s = s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "        s = \"_\".join([token for token in s.split(\"_\") if token])\n",
    "        return s\n",
    "    return [_norm(c) for c in cols]\n",
    "\n",
    "def normalize_commune_name(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    df = df.copy()\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c or \"nom\" == c), None)\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def detect_separator(path):\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            sample = f.read(2048)\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "        if sep not in [\";\", \",\", \"\\t\", \"|\"]:\n",
    "            sep = \";\"\n",
    "        return sep\n",
    "    except Exception:\n",
    "        return \";\"\n",
    "\n",
    "def clean_one_lightweight(key, path, schema):\n",
    "    print(f\"\\nüß© === {key.upper()} ===\")\n",
    "    print(f\"üìÑ Lecture du fichier : {path.name}\")\n",
    "\n",
    "    sep = detect_separator(path)\n",
    "    out_path = DATA_CLEAN / f\"{key}_clean.csv\"\n",
    "    first_chunk = True\n",
    "    total_lines = 0\n",
    "    start = time.time()\n",
    "\n",
    "    try:\n",
    "        reader = pd.read_csv(\n",
    "            path, sep=sep, encoding=\"utf-8\", chunksize=CHUNK_SIZE,\n",
    "            engine=\"python\", on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "        for chunk in tqdm(reader, desc=f\"{key}\", unit=\"chunk\"):\n",
    "            total_lines += len(chunk)\n",
    "            chunk.columns = normalize_colnames(chunk.columns)\n",
    "            chunk = add_commune_keys(chunk)\n",
    "\n",
    "            rename_map = {old: new for old, new in schema.get(\"rename\", {}).items() if old in chunk.columns}\n",
    "            chunk = chunk.rename(columns=rename_map)\n",
    "\n",
    "            for col, dtype in schema.get(\"dtype\", {}).items():\n",
    "                if col in chunk.columns:\n",
    "                    try:\n",
    "                        if dtype == \"Int64\":\n",
    "                            chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\").astype(\"Int64\")\n",
    "                        elif dtype == \"float\":\n",
    "                            chunk[col] = pd.to_numeric(chunk[col], errors=\"coerce\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            chunk.to_csv(out_path, index=False, mode=\"a\", header=first_chunk)\n",
    "            first_chunk = False\n",
    "\n",
    "        elapsed = round(time.time() - start, 2)\n",
    "        print(f\"‚úÖ {key} termin√© ({total_lines:,} lignes) en {elapsed}s\")\n",
    "        return {\"base\": key, \"lignes\": total_lines, \"fichier\": str(out_path)}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sur {key}: {e}\")\n",
    "        return {\"base\": key, \"lignes\": 0, \"fichier\": str(path), \"erreur\": str(e)}\n",
    "\n",
    "\n",
    "# --- Boucle principale ---\n",
    "results = []\n",
    "for key, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        print(f\"‚ö†Ô∏è Fichier manquant : {path}\")\n",
    "        continue\n",
    "    schema = SCHEMAS.get(key, {\"rename\": {}, \"dtype\": {}})\n",
    "    res = clean_one_lightweight(key, path, schema)\n",
    "    results.append(res)\n",
    "\n",
    "# --- R√©sum√© global ---\n",
    "print(\"\\nüìä === R√âSUM√â GLOBAL DU NETTOYAGE ===\")\n",
    "summary = pd.DataFrame(results)\n",
    "display(summary)\n",
    "\n",
    "print(\"\\n‚úÖ √âtape 3 termin√©e (mode l√©ger). Aucune surcharge m√©moire d√©tect√©e.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6764e73-5d14-4072-a391-8271fde82dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 4 : NETTOYAGE COMPLET ET CONTR√îL√â ===\n",
    "# Objectif : lire, uniformiser, nettoyer et sauvegarder toutes les bases de donn√©es.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Dossiers\n",
    "DATA_RAW = Path(\"data\")         # üîß fichiers sources\n",
    "DATA_CLEAN = DATA_RAW / \"clean\" # üîß fichiers nettoy√©s\n",
    "DATA_CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Nettoyage du dossier clean avant traitement\n",
    "if DATA_CLEAN.exists():\n",
    "    print(\"üßΩ Nettoyage du dossier 'data/clean'...\")\n",
    "    for f in DATA_CLEAN.glob(\"*.csv\"):\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Impossible de supprimer {f.name} : {e}\")\n",
    "\n",
    "# --- Fichiers √† traiter\n",
    "FILES = {\n",
    "    \"dvf\": DATA_RAW / \"dvf_idf.csv\",\n",
    "    \"delinquance\": DATA_RAW / \"delinquance_communes.csv\",\n",
    "    \"air\": DATA_RAW / \"air_parif_communes.csv\",\n",
    "    \"encadrement\": DATA_RAW / \"encadrement_loyers_idf.csv\",\n",
    "    \"insee\": DATA_RAW / \"insee_dossier.csv\",\n",
    "    \"dpe\": DATA_RAW / \"dpe_logements.csv\",\n",
    "    \"transport\": DATA_RAW / \"transports_idf.csv\",\n",
    "    \"rne\": DATA_RAW / \"fr-en-annuaire-education.csv\",\n",
    "}\n",
    "\n",
    "print(\"üìÇ V√©rification des fichiers pr√©sents :\")\n",
    "for key, path in FILES.items():\n",
    "    print(f\"{key:12s} ‚Üí {path} {'‚úÖ' if path.exists() else '‚ùå'}\")\n",
    "\n",
    "# --- Fonctions utilitaires\n",
    "def normalize_colnames(cols):\n",
    "    \"\"\"Uniformise les noms de colonnes (minuscules, sans accents, snake_case).\"\"\"\n",
    "    def _norm(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        s = s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "        s = \"_\".join([token for token in s.split(\"_\") if token])\n",
    "        return s\n",
    "    return [_norm(c) for c in cols]\n",
    "\n",
    "def normalize_commune_name(s):\n",
    "    \"\"\"Normalise les noms de communes pour √©viter les erreurs de casse ou d'accents.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    \"\"\"Cr√©e les colonnes code_commune et commune_std pour les jointures futures.\"\"\"\n",
    "    df = df.copy()\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c or c == \"nom\"), None)\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def remove_empty_columns(df):\n",
    "    \"\"\"Supprime les colonnes enti√®rement vides.\"\"\"\n",
    "    before = df.shape[1]\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    after = df.shape[1]\n",
    "    if before != after:\n",
    "        print(f\"üßπ {before - after} colonnes enti√®rement vides supprim√©es\")\n",
    "    return df\n",
    "\n",
    "def add_paris_mapping_if_missing(df, key):\n",
    "    \"\"\"Ajoute les codes communes manquants pour les fichiers BruitParif/Paris.\"\"\"\n",
    "    if key not in [\"bruitsparif\", \"bruitsparifs\", \"bruitsparifs_communes\"]:\n",
    "        return df\n",
    "    if \"code_commune\" in df.columns and df[\"code_commune\"].notna().sum() > 0:\n",
    "        return df\n",
    "\n",
    "    paris_mapping = {\n",
    "        \"1er\": 75101, \"2e\": 75102, \"3e\": 75103, \"4e\": 75104, \"5e\": 75105, \"6e\": 75106,\n",
    "        \"7e\": 75107, \"8e\": 75108, \"9e\": 75109, \"10e\": 75110, \"11e\": 75111, \"12e\": 75112,\n",
    "        \"13e\": 75113, \"14e\": 75114, \"15e\": 75115, \"16e\": 75116, \"17e\": 75117,\n",
    "        \"18e\": 75118, \"19e\": 75119, \"20e\": 75120\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].astype(str).str.contains(\"arrondissement|paris\", case=False).any():\n",
    "            df[\"code_commune\"] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.extract(r\"(\\d+)\")\n",
    "                .astype(float)\n",
    "                .astype(\"Int64\")\n",
    "                .map(paris_mapping)\n",
    "            )\n",
    "            df[\"commune_std\"] = \"PARIS\"\n",
    "            print(\"üèôÔ∏è Codes communes ajout√©s via mapping Paris\")\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# --- Sch√©mas de nettoyage\n",
    "SCHEMAS = {\n",
    "    \"dvf\": {\"rename\": {\"valeur_fonciere\": \"valeur_fonciere\", \"surface_reelle_bati\": \"surface_reelle_bati\",\n",
    "                       \"type_local\": \"type_local\", \"nombre_pieces_principales\": \"nb_pieces\",\n",
    "                       \"date_mutation\": \"date_mutation\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"valeur_fonciere\": \"float\", \"surface_reelle_bati\": \"float\", \"nb_pieces\": \"Int64\"},\n",
    "            \"parse_dates\": [\"date_mutation\"],\n",
    "            \"drop_dupes_on\": [\"code_commune\", \"date_mutation\", \"valeur_fonciere\", \"surface_reelle_bati\"]},\n",
    "    \"delinquance\": {\"rename\": {\"faits_total\": \"faits_total\", \"population\": \"population\", \"annee\": \"annee\",\n",
    "                               \"commune\": \"commune\", \"taux_criminalite\": \"taux_criminalite\"},\n",
    "                    \"dtype\": {\"faits_total\": \"Int64\", \"population\": \"Int64\", \"annee\": \"Int64\"},\n",
    "                    \"drop_dupes_on\": [\"code_commune\", \"annee\"]},\n",
    "    \"air\": {\"rename\": {\"indice_airparif\": \"indice_airparif\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"indice_airparif\": \"float\"},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"encadrement\": {\"rename\": {\"zone_encadree\": \"zone_encadree\", \"loyer_ref\": \"loyer_ref\",\n",
    "                               \"loyer_major√©\": \"loyer_majore\", \"commune\": \"commune\"},\n",
    "                    \"dtype\": {},\n",
    "                    \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"insee\": {\"rename\": {\"revenu_median\": \"revenu_median\", \"taux_chomage\": \"taux_chomage\",\n",
    "                         \"pop_18_29\": \"pop_18_29\", \"population\": \"population\",\n",
    "                         \"logements_vacants\": \"logements_vacants\", \"logements_totaux\": \"logements_totaux\",\n",
    "                         \"commune\": \"commune\"},\n",
    "              \"dtype\": {\"revenu_median\": \"float\", \"taux_chomage\": \"float\", \"pop_18_29\": \"Int64\",\n",
    "                        \"population\": \"Int64\", \"logements_vacants\": \"Int64\", \"logements_totaux\": \"Int64\"},\n",
    "              \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"dpe\": {\"rename\": {\"nb_a\": \"nb_a\", \"nb_b\": \"nb_b\", \"nb_c\": \"nb_c\", \"nb_d\": \"nb_d\", \"nb_e\": \"nb_e\",\n",
    "                       \"nb_f\": \"nb_f\", \"nb_g\": \"nb_g\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {k: \"Int64\" for k in [\"nb_a\", \"nb_b\", \"nb_c\", \"nb_d\", \"nb_e\", \"nb_f\", \"nb_g\"]},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"transport\": {\"rename\": {\"nb_arrets\": \"nb_arrets\", \"score_connectivite\": \"score_connectivite\", \"commune\": \"commune\"},\n",
    "                  \"dtype\": {\"nb_arrets\": \"Int64\", \"score_connectivite\": \"float\"},\n",
    "                  \"drop_dupes_on\": [\"code_commune\"]},\n",
    "    \"rne\": {\"rename\": {\"nb_etabs_sup\": \"nb_etabs_sup\", \"superficie_km2\": \"superficie_km2\", \"commune\": \"commune\"},\n",
    "            \"dtype\": {\"nb_etabs_sup\": \"Int64\", \"superficie_km2\": \"float\"},\n",
    "            \"drop_dupes_on\": [\"code_commune\"]},\n",
    "}\n",
    "\n",
    "# --- Fonction principale de nettoyage\n",
    "def clean_one(key, path, schema):\n",
    "    print(f\"\\nüß© === {key.upper()} ===\")\n",
    "    print(f\"üìÑ Lecture du fichier : {path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, sep=\";\", encoding=\"latin-1\", low_memory=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "    print(f\"   ‚û§ {df.shape[0]} lignes, {df.shape[1]} colonnes avant nettoyage\")\n",
    "\n",
    "    # Normalisation et renommage\n",
    "    df.columns = normalize_colnames(df.columns)\n",
    "    rename_map = {old: new for old, new in schema.get(\"rename\", {}).items() if old in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Ajout des cl√©s communes et suppression des colonnes vides\n",
    "    df = add_commune_keys(df)\n",
    "    df = remove_empty_columns(df)\n",
    "    df = add_paris_mapping_if_missing(df, key)\n",
    "\n",
    "    # Typage\n",
    "    for col, dtype in schema.get(\"dtype\", {}).items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == \"Int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "                elif dtype == \"float\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Typage impossible pour {col}: {e}\")\n",
    "\n",
    "    # Suppression des doublons\n",
    "    subset = [c for c in schema.get(\"drop_dupes_on\", []) if c in df.columns]\n",
    "    if subset:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        print(f\"üßπ {before - len(df)} doublons supprim√©s sur {subset}\")\n",
    "\n",
    "    # Export s√©curis√©\n",
    "    out = DATA_CLEAN / f\"{key}_clean.csv\"\n",
    "    try:\n",
    "        df.to_csv(out, index=False, encoding=\"utf-8\")\n",
    "        print(f\"‚úÖ Export√© vers {out} ({df.shape[0]} lignes, {df.shape[1]} colonnes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l‚Äôexport de {out} : {e}\")\n",
    "\n",
    "    return {\"base\": key, \"lignes\": df.shape[0], \"colonnes\": df.shape[1], \"fichier\": str(out)}\n",
    "\n",
    "# --- Boucle principale de traitement\n",
    "results = []\n",
    "for key, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        print(f\"‚ùå Fichier manquant : {path}\")\n",
    "        continue\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.read(1024)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fichier {path.name} inaccessible : {e}\")\n",
    "        continue\n",
    "\n",
    "    schema = SCHEMAS.get(key, {\"rename\": {}, \"dtype\": {}})\n",
    "    res = clean_one(key, path, schema)\n",
    "    results.append(res)\n",
    "\n",
    "# --- R√©sum√© global\n",
    "print(\"\\nüìä === R√âSUM√â GLOBAL DU NETTOYAGE ===\")\n",
    "if results:\n",
    "    summary = pd.DataFrame(results)\n",
    "    summary[\"taille_Ko\"] = summary[\"fichier\"].apply(lambda x: round(os.path.getsize(x) / 1024, 1))\n",
    "    display(summary)\n",
    "    summary.to_csv(DATA_CLEAN / \"rapport_validation.csv\", index=False)\n",
    "    print(f\"üìù Rapport sauvegard√© dans {DATA_CLEAN / 'rapport_validation.csv'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun fichier n‚Äôa √©t√© trait√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcce6f-c74b-4e28-9fe8-e72c023eece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import unicodedata, csv\n",
    "\n",
    "DATA_RAW   = Path(\"data\")          # dvf_75.csv, dvf_77.csv, etc.\n",
    "DATA_CLEAN = Path(\"data/clean\")\n",
    "DATA_CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def normalize_colnames(cols):\n",
    "    def _n(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "        return s.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    return [_n(c) for c in cols]\n",
    "\n",
    "def add_commune_keys(df):\n",
    "    df = df.copy()\n",
    "    if \"code_commune\" in df.columns:\n",
    "        df[\"code_commune\"] = (\n",
    "            df[\"code_commune\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "        )\n",
    "    if \"nom_commune\" in df.columns:\n",
    "        s = df[\"nom_commune\"].astype(str)\n",
    "        s = s.str.normalize(\"NFKD\").str.encode(\"ascii\",\"ignore\").str.decode(\"ascii\")\n",
    "        s = s.str.upper().str.replace(\"-\", \" \", regex=False).str.replace(\"'\", \" \", regex=False)\n",
    "        df[\"commune_std\"] = s.str.split().str.join(\" \")\n",
    "    return df\n",
    "\n",
    "def read_dvf_robuste(src):\n",
    "    try:\n",
    "        return pd.read_csv(src, sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            src, sep=\",\", encoding=\"utf-8\",\n",
    "            engine=\"python\", quotechar='\"', on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "# DVF bruts √† traiter\n",
    "raw_dvf = sorted([p for p in DATA_RAW.glob(\"dvf_*.csv\") if \"_clean\" not in p.name])\n",
    "print(f\"üìÇ DVF bruts d√©tect√©s ({len(raw_dvf)}):\")\n",
    "for p in raw_dvf: print(\"  -\", p.name)\n",
    "\n",
    "for src in raw_dvf:\n",
    "    dest = DATA_CLEAN / (src.stem + \"_clean.csv\")\n",
    "    print(f\"\\nüß© {src.name} ‚Üí {dest.name}\")\n",
    "\n",
    "    df = read_dvf_robuste(src)\n",
    "    df.columns = normalize_colnames(df.columns)\n",
    "    df = add_commune_keys(df)\n",
    "\n",
    "    # ‚úÖ Correction : lineterminator (sans underscore)\n",
    "    df.to_csv(\n",
    "        dest, index=False, encoding=\"utf-8\",\n",
    "        sep=\",\", quoting=csv.QUOTE_MINIMAL, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    chk = pd.read_csv(dest, sep=\",\", nrows=3)\n",
    "    print(f\"   ‚úÖ OK : {chk.shape[1]} colonnes | Aper√ßu colonnes : {list(chk.columns)[:6]}\")\n",
    "\n",
    "print(\"\\nüéâ Recr√©ation des DVF _clean termin√©e (fichiers sains et sans guillemets parasites).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cfc766-f6d3-4ed0-a409-5dbee840f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# === Fonction pour d√©tecter automatiquement le s√©parateur ===\n",
    "def detect_separator(path, sample_size=4096):\n",
    "    \"\"\"D√©tecte le s√©parateur le plus probable dans un fichier CSV.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(sample_size)\n",
    "        sniffer = csv.Sniffer()\n",
    "        sep = sniffer.sniff(sample).delimiter\n",
    "        if sep not in [\",\", \";\", \"\\t\", \"|\"]:\n",
    "            sep = \";\"\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "\n",
    "# === Fonction principale de visualisation ===\n",
    "def check_all_clean_files(folder=\"data/clean\", nrows=5, max_cols=10):\n",
    "    \"\"\"Affiche un aper√ßu des fichiers CSV nettoy√©s (s√©parateur, colonnes, aper√ßu des donn√©es).\"\"\"\n",
    "    clean_dir = Path(folder)\n",
    "    clean_files = list(clean_dir.glob(\"*_clean.csv\"))\n",
    "\n",
    "    if not clean_files:\n",
    "        raise FileNotFoundError(f\"‚ö†Ô∏è Aucun fichier '_clean.csv' trouv√© dans {folder}/\")\n",
    "\n",
    "    print(f\"üìÇ {len(clean_files)} fichiers trouv√©s dans {folder}/\\n\")\n",
    "\n",
    "    for f in clean_files:\n",
    "        sep = detect_separator(f)\n",
    "        print(f\"=== {f.name} ===\")\n",
    "        print(f\"   üîπ S√©parateur d√©tect√© : '{sep}'\")\n",
    "\n",
    "        try:\n",
    "            # Lecture de l'en-t√™te pour conna√Ætre le nombre total de colonnes\n",
    "            header = pd.read_csv(f, sep=sep, nrows=0, encoding=\"utf-8\", low_memory=False)\n",
    "            nb_cols = len(header.columns)\n",
    "\n",
    "            # Lecture partielle : jusqu‚Äô√† 10 colonnes maximum, ou moins si le fichier en a moins\n",
    "            cols_to_read = list(range(min(max_cols, nb_cols)))\n",
    "            df = pd.read_csv(f, sep=sep, nrows=nrows, usecols=cols_to_read, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "            print(f\"   üîπ Colonnes affich√©es : {len(df.columns)} sur {nb_cols} totales\")\n",
    "            display(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur de lecture sur {f.name}: {e}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "\n",
    "# === Appel de la fonction ===\n",
    "check_all_clean_files(\"data/clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19926801-077d-4553-a5f6-ca06b7458ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ac28f-d0ee-449e-83ba-8b68d163a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 5 : TEST DU NETTOYAGE DES BASES DE DONN√âES ===\n",
    "# Objectif : v√©rifier la coh√©rence, la propret√© et la structure de chaque base clean\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Dossier contenant les bases nettoy√©es\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Liste des fichiers √† v√©rifier\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonctions utilitaires\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip().upper()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.replace(\"-\", \" \").replace(\"'\", \" \")\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def detect_separator(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def test_clean_file(path):\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    sep = detect_separator(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture : {e}\\n\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîπ Lignes : {len(df)}, Colonnes : {len(df.columns)}, S√©parateur : '{sep}'\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Harmonisation de format\n",
    "    if \"code_commune\" in df.columns:\n",
    "        df[\"code_commune\"] = df[\"code_commune\"].astype(str).str.extract(r\"(\\d+)\")[0].str.zfill(5)\n",
    "    if \"commune\" in df.columns:\n",
    "        df[\"commune\"] = df[\"commune\"].apply(normalize_text)\n",
    "\n",
    "    # 2Ô∏è‚É£ Doublons\n",
    "    dups = df.duplicated().sum()\n",
    "    print(f\"   üîÅ Doublons d√©tect√©s : {dups}\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Valeurs manquantes\n",
    "    missing = df.isna().mean().round(2)\n",
    "    top_missing = missing[missing > 0].sort_values(ascending=False).head(5)\n",
    "    if not top_missing.empty:\n",
    "        print(\"   ‚ö†Ô∏è Colonnes avec NaN :\", dict(top_missing))\n",
    "    else:\n",
    "        print(\"   ‚úÖ Aucune valeur manquante significative\")\n",
    "\n",
    "    # 4Ô∏è‚É£ Uniformisation noms colonnes\n",
    "    normalized_cols = [unicodedata.normalize(\"NFKD\", c).encode(\"ascii\", \"ignore\").decode(\"utf-8\").lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    if df.columns.tolist() != normalized_cols:\n",
    "        print(\"   ‚öôÔ∏è Correction potentielle des noms de colonnes incoh√©rents\")\n",
    "\n",
    "    # 5Ô∏è‚É£ Types de donn√©es\n",
    "    print(\"   üìä Types d√©tect√©s :\")\n",
    "    print(df.dtypes.head())\n",
    "\n",
    "    # 6Ô∏è‚É£ Normalisation du texte\n",
    "    if \"commune\" in df.columns:\n",
    "        uniques = df[\"commune\"].nunique()\n",
    "        print(f\"   üßæ Communes uniques : {uniques}\")\n",
    "\n",
    "    # 7Ô∏è‚É£ Caract√®res parasites\n",
    "    example_str = df.select_dtypes(include=\"object\").astype(str).apply(lambda x: x.str.contains(\"[‚Ç¨,$,\\t,;]\", regex=True)).any()\n",
    "    if example_str.any():\n",
    "        print(\"   ‚ö†Ô∏è Caract√®res parasites d√©tect√©s dans certaines colonnes texte\")\n",
    "\n",
    "    # 8Ô∏è‚É£ Cl√©s communes\n",
    "    if \"code_commune\" in df.columns:\n",
    "        valid_keys = df[\"code_commune\"].notna().sum()\n",
    "        print(f\"   üß© Cl√© 'code_commune' pr√©sente ({valid_keys} valeurs valides)\")\n",
    "\n",
    "    # 9Ô∏è‚É£ Contr√¥le de coh√©rence simple (ex : surface > 0)\n",
    "    if \"surface_reelle_bati\" in df.columns:\n",
    "        negatives = (df[\"surface_reelle_bati\"] <= 0).sum()\n",
    "        if negatives > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {negatives} valeurs de surface non valides (‚â§0)\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Toutes les surfaces sont positives\")\n",
    "\n",
    "    if set([\"valeur_fonciere\", \"surface_reelle_bati\"]).issubset(df.columns):\n",
    "        df[\"prix_m2\"] = df[\"valeur_fonciere\"] / df[\"surface_reelle_bati\"]\n",
    "        mean_price = df[\"prix_m2\"].mean(skipna=True)\n",
    "        print(f\"   üí∂ Prix moyen estim√© au m¬≤ : {round(mean_price,2)}\")\n",
    "\n",
    "    # 10Ô∏è‚É£ Aper√ßu visuel\n",
    "    print(\"\\n   üßæ Aper√ßu :\")\n",
    "    display(df.head(5).iloc[:, :min(8, len(df.columns))])\n",
    "    print(\"-\" * 120)\n",
    "    return df\n",
    "\n",
    "# --- Lancement du test sur toutes les bases clean\n",
    "for f in files:\n",
    "    test_clean_file(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ad7c7-bd09-4bca-9a04-cccda983cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 1 : D√âTECTION DES ERREURS DE FORMAT ===\n",
    "# Objectif : d√©tecter les probl√®mes de lecture, encodage ou s√©parateur sur toutes les bases clean\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les bases clean\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Recherche de tous les fichiers .csv dans le dossier\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction de test de format\n",
    "def detect_format_issues(path):\n",
    "    report = {\"fichier\": path.name, \"ok\": True, \"erreur\": None, \"colonnes\": 0, \"lignes\": 0, \"sep\": None}\n",
    "\n",
    "    try:\n",
    "        # D√©tection automatique du s√©parateur\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            sample = f.read(4096)\n",
    "        try:\n",
    "            sep = csv.Sniffer().sniff(sample).delimiter\n",
    "        except Exception:\n",
    "            sep = \";\"\n",
    "        report[\"sep\"] = sep\n",
    "\n",
    "        # Lecture test sur 100 premi√®res lignes\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", nrows=100, low_memory=False)\n",
    "        report[\"colonnes\"] = df.shape[1]\n",
    "        report[\"lignes\"] = len(df)\n",
    "\n",
    "        # Test de coh√©rence de structure\n",
    "        if df.shape[1] < 3:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"erreur\"] = \"Trop peu de colonnes (mauvais s√©parateur ou structure)\"\n",
    "        elif df.shape[0] == 0:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"erreur\"] = \"Fichier vide ou mal encod√©\"\n",
    "\n",
    "    except Exception as e:\n",
    "        report[\"ok\"] = False\n",
    "        report[\"erreur\"] = str(e)\n",
    "\n",
    "    return report\n",
    "\n",
    "# --- Lancement du test pour toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    print(f\"üîé V√©rification de {f.name} ...\")\n",
    "    res = detect_format_issues(f)\n",
    "    results.append(res)\n",
    "    if not res[\"ok\"]:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur d√©tect√©e : {res['erreur']}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ OK ({res['colonnes']} colonnes, {res['lignes']} lignes, sep='{res['sep']}')\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# --- R√©sum√© global\n",
    "df_report = pd.DataFrame(results)\n",
    "print(\"\\nüìä === R√âSUM√â DES ERREURS DE FORMAT ===\")\n",
    "display(df_report)\n",
    "\n",
    "# --- Statistiques g√©n√©rales\n",
    "total = len(df_report)\n",
    "ok = df_report[\"ok\"].sum()\n",
    "ko = total - ok\n",
    "\n",
    "print(f\"‚úÖ Fichiers valides : {ok}/{total}\")\n",
    "print(f\"‚ö†Ô∏è Fichiers avec erreurs : {ko}/{total}\")\n",
    "\n",
    "if ko > 0:\n",
    "    print(\"\\nüßæ Liste des fichiers probl√©matiques :\")\n",
    "    display(df_report.loc[df_report[\"ok\"] == False, [\"fichier\", \"erreur\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60cc24-5e93-4471-acca-1f6743d12b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NETTOYAGE GLOBAL + SUPPRESSION DES DOUBLONS (EN PLACE) ===\n",
    "# Objectif :\n",
    "# 1Ô∏è‚É£ Supprimer les fichiers temporaires dans data/clean/\n",
    "# 2Ô∏è‚É£ Supprimer les doublons dans tous les fichiers clean et r√©√©crire directement le fichier\n",
    "# 3Ô∏è‚É£ Fournir un rapport complet et propre\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les bases nettoy√©es\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "print(\"üîç √âtape 1 : Nettoyage du dossier 'data/clean'...\\n\")\n",
    "\n",
    "# --- √âtape 1 : supprimer les fichiers temporaires\n",
    "suffixes_a_supprimer = [\"_nodup.csv\", \"_fixed.csv\", \"_clean_fixed.csv\"]\n",
    "\n",
    "suppr = 0\n",
    "for f in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    if any(suffix in f.name for suffix in suffixes_a_supprimer):\n",
    "        f.unlink()\n",
    "        suppr += 1\n",
    "\n",
    "print(f\"üßπ {suppr} fichiers temporaires supprim√©s.\")\n",
    "print(\"‚úÖ Dossier 'data/clean' revenu √† l'√©tat initial (seuls les *_clean.csv sont conserv√©s).\\n\")\n",
    "\n",
    "# --- √âtape 2 : suppression des doublons dans chaque fichier\n",
    "print(\"‚öôÔ∏è √âtape 2 : Suppression des doublons (fichiers r√©√©crits en place)...\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tection automatique du s√©parateur CSV probable.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def clean_duplicates_in_place(path):\n",
    "    \"\"\"Supprime les doublons et r√©√©crit le fichier directement.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    removed = before - after\n",
    "    pct = round((removed / before) * 100, 2) if before > 0 else 0\n",
    "\n",
    "    # R√©√©criture directe du fichier\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    print(f\"‚úÖ {path.name} : {removed} doublons supprim√©s ({pct}%) ‚Äî Fichier mis √† jour\")\n",
    "    return {\"fichier\": path.name, \"lignes_avant\": before, \"lignes_apres\": after, \"doublons_supprimes\": removed, \"pct\": pct}\n",
    "\n",
    "# --- Application du correctif\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_duplicates_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- √âtape 3 : Synth√®se globale\n",
    "print(\"\\nüìä === SYNTH√àSE FINALE DU NETTOYAGE ===\")\n",
    "df_res = pd.DataFrame(results)\n",
    "display(df_res)\n",
    "\n",
    "total_removed = df_res[\"doublons_supprimes\"].sum()\n",
    "print(f\"\\nüßæ Total de doublons supprim√©s : {total_removed}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© nettoy√©s et mis √† jour sans duplication.\")\n",
    "print(\"‚úÖ Tu peux maintenant relancer ton test de nettoyage pour confirmer qu'il n'y a plus de doublons.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cadff-a448-4a16-9c4b-5e83a6ad259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 2 : D√âTECTION DES DOUBLONS ===\n",
    "# Objectif : rep√©rer les lignes r√©p√©t√©es dans toutes les bases clean\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# --- Dossier contenant les bases nettoy√©es\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "\n",
    "# --- Recherche de tous les fichiers CSV\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def check_duplicates(path):\n",
    "    \"\"\"Analyse un fichier CSV et d√©tecte les doublons.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return {\"fichier\": path.name, \"ok\": False, \"erreur\": str(e), \"doublons\": None}\n",
    "\n",
    "    # --- D√©tection des doublons\n",
    "    nb_total = len(df)\n",
    "    nb_doublons = df.duplicated().sum()\n",
    "    pct = round((nb_doublons / nb_total) * 100, 2) if nb_total > 0 else 0\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    print(f\"üîπ Lignes totales : {nb_total}\")\n",
    "    print(f\"üîÅ Doublons d√©tect√©s : {nb_doublons} ({pct} %)\")\n",
    "    \n",
    "    if nb_doublons > 0:\n",
    "        print(\"üßæ Exemple de doublons :\")\n",
    "        display(df[df.duplicated()].head(5))\n",
    "    else:\n",
    "        print(\"‚úÖ Aucun doublon trouv√©\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    return {\"fichier\": path.name, \"ok\": True, \"lignes\": nb_total, \"doublons\": nb_doublons, \"pourcentage\": pct}\n",
    "\n",
    "# --- Analyse de tous les fichiers\n",
    "results = []\n",
    "for f in files:\n",
    "    res = check_duplicates(f)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_doublons = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DES DOUBLONS ===\")\n",
    "display(df_doublons)\n",
    "\n",
    "# --- Statistiques globales\n",
    "total = len(df_doublons)\n",
    "ok_files = (df_doublons[\"doublons\"] == 0).sum()\n",
    "print(f\"‚úÖ Fichiers sans doublon : {ok_files}/{total}\")\n",
    "print(f\"‚ö†Ô∏è Fichiers contenant des doublons : {total - ok_files}/{total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667f4d0-a998-4998-8ecb-2d60e556a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CORRECTIF √âTAPE 3 : SUPPRESSION DES COLONNES VIDES OU TROP INCOMPL√àTES ===\n",
    "# Objectif :\n",
    "# - Supprimer les colonnes 100% NaN ou avec plus de 50% de valeurs manquantes\n",
    "# - R√©√©crire les fichiers clean existants (pas de duplication)\n",
    "# - Fournir un rapport clair de la r√©duction des colonnes\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction utilitaire pour d√©tecter le s√©parateur\n",
    "def detect_separator(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Nettoyage et r√©√©criture en place\n",
    "def clean_missing_columns_in_place(path, seuil=0.5):\n",
    "    \"\"\"Supprime les colonnes vides ou avec plus de `seuil` de NaN.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = len(df.columns)\n",
    "    before_rows = len(df)\n",
    "\n",
    "    # --- Calcul du taux de valeurs manquantes\n",
    "    missing_ratio = df.isna().mean()\n",
    "\n",
    "    # --- Colonnes √† supprimer\n",
    "    empty_cols = list(missing_ratio[missing_ratio == 1.0].index)\n",
    "    incomplete_cols = list(missing_ratio[missing_ratio > seuil].index)\n",
    "    to_drop = set(empty_cols + incomplete_cols)\n",
    "\n",
    "    # --- Suppression\n",
    "    df = df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "    after_cols = len(df.columns)\n",
    "    removed_cols = before_cols - after_cols\n",
    "    pct_removed = round((removed_cols / before_cols) * 100, 2) if before_cols > 0 else 0\n",
    "\n",
    "    # --- R√©√©criture du fichier propre\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"‚úÖ Colonnes supprim√©es : {removed_cols} ({pct_removed}%)\")\n",
    "    if removed_cols > 0:\n",
    "        print(f\"   üßπ {list(to_drop)[:10]}{' ...' if len(to_drop) > 10 else ''}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Aucune colonne supprim√©e\")\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name} ({after_cols} colonnes conserv√©es, {before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_avant\": before_cols,\n",
    "        \"colonnes_apres\": after_cols,\n",
    "        \"supprim√©es\": removed_cols,\n",
    "        \"pct_supprim√©es\": pct_removed\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_missing_columns_in_place(f, seuil=0.5)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU NETTOYAGE DES COLONNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_removed = df_summary[\"supprim√©es\"].sum()\n",
    "print(f\"üßæ Total de colonnes supprim√©es : {total_removed}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© mis √† jour directement sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ad30f-31db-42d2-9477-81d60432d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 3 : ANALYSE DES VALEURS MANQUANTES ===\n",
    "# Objectif :\n",
    "# - Identifier les colonnes comportant des NaN ou cellules vides\n",
    "# - Afficher des statistiques claires pour chaque base\n",
    "# - Ne pas g√©n√©rer de fichier sur le disque\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s (d√©j√† sans doublons)\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def analyze_missing_values(path):\n",
    "    \"\"\"Analyse le taux de valeurs manquantes d'un fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return {\"fichier\": path.name, \"ok\": False, \"erreur\": str(e)}\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    print(f\"üîπ {len(df)} lignes, {len(df.columns)} colonnes\")\n",
    "\n",
    "    # --- Calcul des NaN par colonne\n",
    "    missing_ratio = df.isna().mean().round(3)\n",
    "    missing_ratio = missing_ratio[missing_ratio > 0].sort_values(ascending=False)\n",
    "\n",
    "    # --- Statistiques globales\n",
    "    nb_col_nan = len(missing_ratio)\n",
    "    taux_moyen_nan = round(missing_ratio.mean() * 100, 2) if nb_col_nan > 0 else 0\n",
    "    taux_median_nan = round(missing_ratio.median() * 100, 2) if nb_col_nan > 0 else 0\n",
    "\n",
    "    if nb_col_nan > 0:\n",
    "        print(f\"‚ö†Ô∏è {nb_col_nan} colonnes avec des valeurs manquantes ({taux_moyen_nan}% en moyenne)\")\n",
    "        print(\"üîù Top 10 des colonnes les plus incompl√®tes :\")\n",
    "        display(missing_ratio.head(10))\n",
    "    else:\n",
    "        print(\"‚úÖ Aucune valeur manquante d√©tect√©e dans ce fichier.\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes\": len(df.columns),\n",
    "        \"lignes\": len(df),\n",
    "        \"nb_colonnes_nan\": nb_col_nan,\n",
    "        \"taux_moyen_nan(%)\": taux_moyen_nan,\n",
    "        \"taux_median_nan(%)\": taux_median_nan,\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = analyze_missing_values(f)\n",
    "    results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_nan = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE GLOBALE DES VALEURS MANQUANTES ===\")\n",
    "display(df_nan)\n",
    "\n",
    "# --- Indicateurs globaux\n",
    "nb_total = len(df_nan)\n",
    "nb_sans_nan = (df_nan[\"nb_colonnes_nan\"] == 0).sum()\n",
    "nb_avec_nan = nb_total - nb_sans_nan\n",
    "taux_moyen_global = round(df_nan[\"taux_moyen_nan(%)\"].mean(), 2)\n",
    "\n",
    "print(f\"‚úÖ Bases sans NaN : {nb_sans_nan}/{nb_total}\")\n",
    "print(f\"‚ö†Ô∏è Bases avec NaN : {nb_avec_nan}/{nb_total}\")\n",
    "print(f\"üìà Taux moyen global de NaN sur l'ensemble des fichiers : {taux_moyen_global}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914b4df-f826-44e2-8475-436cd2339cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 4 : UNIFORMISATION DES NOMS DE COLONNES ===\n",
    "# Objectif :\n",
    "#   - V√©rifier et corriger les noms de colonnes (minuscules, sans accents, underscores)\n",
    "#   - Supprimer les caract√®res sp√©ciaux et doublons\n",
    "#   - R√©√©crire directement les fichiers nettoy√©s\n",
    "#   - Fournir un r√©sum√© clair avant/apr√®s\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tection automatique du s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_column_name(name):\n",
    "    \"\"\"Normalise un nom de colonne : minuscules, pas d'accents, underscores, caract√®res alphanum√©riques uniquement.\"\"\"\n",
    "    name = str(name).strip().lower()\n",
    "    name = \"\".join(c for c in unicodedata.normalize(\"NFKD\", name) if not unicodedata.combining(c))\n",
    "    name = name.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "    name = \"\".join(ch for ch in name if ch.isalnum() or ch == \"_\")\n",
    "    name = \"_\".join([tok for tok in name.split(\"_\") if tok])  # supprime les underscores multiples\n",
    "    return name\n",
    "\n",
    "def clean_column_names_in_place(path):\n",
    "    \"\"\"Uniformise les noms de colonnes et r√©√©crit le fichier en place.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.columns.tolist()\n",
    "\n",
    "    # --- Normalisation des noms de colonnes\n",
    "    normalized_cols = [normalize_column_name(c) for c in before_cols]\n",
    "    df.columns = normalized_cols\n",
    "\n",
    "    # --- Gestion des doublons √©ventuels\n",
    "    if len(set(df.columns)) < len(df.columns):\n",
    "        seen = {}\n",
    "        new_cols = []\n",
    "        for c in df.columns:\n",
    "            if c not in seen:\n",
    "                seen[c] = 1\n",
    "                new_cols.append(c)\n",
    "            else:\n",
    "                seen[c] += 1\n",
    "                new_cols.append(f\"{c}_{seen[c]}\")\n",
    "        df.columns = new_cols\n",
    "        print(\"‚ö†Ô∏è Doublons de colonnes d√©tect√©s et corrig√©s automatiquement.\")\n",
    "\n",
    "    # --- Sauvegarde du fichier corrig√©\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    # --- R√©sum√©\n",
    "    after_cols = df.columns.tolist()\n",
    "    changes = {b: a for b, a in zip(before_cols, after_cols) if b != a}\n",
    "    print(f\"‚úÖ Colonnes renomm√©es : {len(changes)} / {len(before_cols)}\")\n",
    "    if len(changes) > 0:\n",
    "        print(f\"   üßæ Exemples : {list(changes.items())[:5]}{' ...' if len(changes) > 5 else ''}\")\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name} ({len(df.columns)} colonnes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": len(df.columns),\n",
    "        \"colonnes_renommees\": len(changes),\n",
    "        \"pct_renommees\": round((len(changes) / len(df.columns)) * 100, 2) if len(df.columns) > 0 else 0\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_column_names_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU NETTOYAGE DES NOMS DE COLONNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_renamed = df_summary[\"colonnes_renommees\"].sum()\n",
    "print(f\"üßæ Total de colonnes renomm√©es : {total_renamed}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© mis √† jour directement sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8acb6f1-47bb-4b43-a094-a28e56106127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 5 (corrig√©e) : CONTR√îLE ET CORRECTION DES TYPES DE DONN√âES ===\n",
    "# Correction : suppression de 'infer_datetime_format' et suppression des warnings r√©p√©titifs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# D√©sactivation des warnings inutiles de pandas\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pandas\")\n",
    "\n",
    "# --- Dossier contenant les fichiers clean\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def infer_and_fix_types_in_place(path):\n",
    "    \"\"\"Analyse et corrige les types de donn√©es d‚Äôun fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_types = df.dtypes.copy()\n",
    "    conversions = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "\n",
    "        # Tentative de conversion des cha√Ænes vers des types utiles\n",
    "        if series.dtype == \"object\":\n",
    "            # 1Ô∏è‚É£ Conversion num√©rique\n",
    "            num_series = pd.to_numeric(series.astype(str).str.replace(\",\", \".\").str.replace(\" \", \"\"), errors=\"coerce\")\n",
    "            ratio_num = num_series.notna().mean()\n",
    "            if ratio_num > 0.9:  # au moins 90 % de valeurs num√©riques\n",
    "                df[col] = num_series\n",
    "                conversions[col] = \"float\"\n",
    "                continue\n",
    "\n",
    "            # 2Ô∏è‚É£ Conversion date (version corrig√©e)\n",
    "            try:\n",
    "                date_series = pd.to_datetime(series, errors=\"coerce\")\n",
    "                ratio_date = date_series.notna().mean()\n",
    "                if ratio_date > 0.9:\n",
    "                    df[col] = date_series\n",
    "                    conversions[col] = \"datetime\"\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 3Ô∏è‚É£ Conversion float ‚Üí Int64 quand possible\n",
    "        if df[col].dtype == \"float64\" and df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "            conversions[col] = \"Int64\"\n",
    "\n",
    "    # Sauvegarde du fichier corrig√©\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "    after_types = df.dtypes\n",
    "    changed = {col: (before_types[col], after_types[col]) for col in df.columns if before_types[col] != after_types[col]}\n",
    "\n",
    "    print(f\"‚úÖ Colonnes converties : {len(conversions)}\")\n",
    "    if len(conversions) > 0:\n",
    "        print(f\"   üßæ Exemples : {list(conversions.items())[:5]}{' ...' if len(conversions) > 5 else ''}\")\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": len(df.columns),\n",
    "        \"colonnes_converties\": len(conversions),\n",
    "        \"types_chang√©s\": len(changed)\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = infer_and_fix_types_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU CONTR√îLE DES TYPES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_converted = df_summary[\"colonnes_converties\"].sum()\n",
    "print(f\"üßæ Total de colonnes converties : {total_converted}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© v√©rifi√©s et mis √† jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d1a5b-7db0-4544-9a3e-7a384c9c6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 6 : NORMALISATION DES TEXTES ===\n",
    "# Objectif :\n",
    "#   - Nettoyer les colonnes texte pour supprimer les accents, symboles et espaces parasites\n",
    "#   - Uniformiser la casse (majuscule coh√©rente)\n",
    "#   - R√©√©crire les fichiers directement\n",
    "#   - Fournir un r√©sum√© clair du nettoyage\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_text(value):\n",
    "    \"\"\"Nettoie et normalise les cha√Ænes de texte.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    value = str(value).strip()\n",
    "\n",
    "    # Supprimer les accents\n",
    "    value = \"\".join(c for c in unicodedata.normalize(\"NFKD\", value) if not unicodedata.combining(c))\n",
    "\n",
    "    # Supprimer les symboles et caract√®res sp√©ciaux\n",
    "    value = re.sub(r\"[‚Ç¨%$;'\\t\\n\\r]\", \" \", value)\n",
    "\n",
    "    # Remplacer tirets et apostrophes par espace\n",
    "    value = re.sub(r\"[-']\", \" \", value)\n",
    "\n",
    "    # Supprimer les espaces multiples\n",
    "    value = re.sub(r\"\\s+\", \" \", value)\n",
    "\n",
    "    # Mise en majuscule pour les textes courts (ex: communes, cat√©gories)\n",
    "    if len(value) <= 40:\n",
    "        value = value.upper()\n",
    "    else:\n",
    "        value = value.capitalize()\n",
    "\n",
    "    return value.strip()\n",
    "\n",
    "def normalize_text_columns_in_place(path):\n",
    "    \"\"\"Normalise les colonnes texte d‚Äôun fichier CSV et r√©√©crit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.shape[1]\n",
    "    before_rows = df.shape[0]\n",
    "\n",
    "    # Colonnes texte √† traiter\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"üßæ Colonnes texte d√©tect√©es : {len(text_cols)} / {before_cols}\")\n",
    "\n",
    "    if len(text_cols) > 0:\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].apply(normalize_text)\n",
    "\n",
    "        # R√©√©criture directe du fichier\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "        print(f\"‚úÖ Colonnes texte normalis√©es ({len(text_cols)} colonnes trait√©es)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucune colonne texte √† normaliser.\")\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name} ({before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": before_cols,\n",
    "        \"colonnes_texte\": len(text_cols)\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = normalize_text_columns_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU NETTOYAGE TEXTUEL ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_text_cols = df_summary[\"colonnes_texte\"].sum()\n",
    "print(f\"üßæ Total de colonnes texte normalis√©es : {total_text_cols}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© normalis√©s et mis √† jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86881bce-bf55-482b-a7c0-f5c8bf9b7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 7 : SUPPRESSION DES CARACT√àRES PARASITES ===\n",
    "# Objectif :\n",
    "#   - Supprimer les caract√®res invisibles ou non imprimables dans les champs texte\n",
    "#   - Nettoyer les guillemets, tabulations, espaces sp√©ciaux (\\xa0, \\t, etc.)\n",
    "#   - R√©√©crire les fichiers directement\n",
    "#   - Fournir un rapport clair de nettoyage\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def clean_parasites_in_place(path):\n",
    "    \"\"\"Supprime les caract√®res parasites dans les colonnes texte et r√©√©crit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = df.shape[1]\n",
    "    before_rows = df.shape[0]\n",
    "\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    print(f\"üßæ Colonnes texte d√©tect√©es : {len(text_cols)} / {before_cols}\")\n",
    "\n",
    "    # --- Fonction de nettoyage cellule\n",
    "    def clean_cell(value):\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "        value = str(value)\n",
    "\n",
    "        # Supprime les espaces non standard et caract√®res de contr√¥le\n",
    "        value = re.sub(r\"[\\x00-\\x1F\\x7F\\xa0\\u200b\\r\\t]\", \" \", value)\n",
    "\n",
    "        # Supprime guillemets et doubles s√©parateurs\n",
    "        value = value.replace('\"', \"\").replace(\"'\", \"\")\n",
    "\n",
    "        # Supprime les espaces multiples\n",
    "        value = re.sub(r\"\\s+\", \" \", value)\n",
    "\n",
    "        return value.strip()\n",
    "\n",
    "    # --- Application sur toutes les colonnes texte\n",
    "    if len(text_cols) > 0:\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].apply(clean_cell)\n",
    "\n",
    "        # Sauvegarde directe du fichier propre\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "        print(f\"‚úÖ Colonnes nettoy√©es : {len(text_cols)} (fichier r√©√©crit)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucun champ texte √† nettoyer.\")\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name} ({before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"colonnes_totales\": before_cols,\n",
    "        \"colonnes_texte\": len(text_cols)\n",
    "    }\n",
    "\n",
    "# --- Application du nettoyage √† toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = clean_parasites_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU NETTOYAGE DES CARACT√àRES PARASITES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_cleaned = df_summary[\"colonnes_texte\"].sum()\n",
    "print(f\"üßæ Total de colonnes texte nettoy√©es : {total_cleaned}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© nettoy√©s et mis √† jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66912e0c-ea31-4956-9c3e-8b6b35948522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 8 : CR√âATION ET V√âRIFICATION DES CL√âS COMMUNES ===\n",
    "# Objectif :\n",
    "#   - Cr√©er / uniformiser les colonnes code_commune et commune_std\n",
    "#   - Normaliser les noms de communes\n",
    "#   - V√©rifier la coh√©rence et r√©√©crire les fichiers directement\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Utilitaires de normalisation\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def normalize_commune_name(name):\n",
    "    \"\"\"Nettoie et standardise le nom de la commune.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return np.nan\n",
    "    name = str(name).strip().upper()\n",
    "    name = \"\".join(c for c in unicodedata.normalize(\"NFKD\", name) if not unicodedata.combining(c))\n",
    "    name = re.sub(r\"[-']\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "def extract_code_commune(value):\n",
    "    \"\"\"Extrait un code INSEE sur 5 chiffres si possible.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    value = str(value)\n",
    "    match = re.search(r\"\\d{5}\", value)\n",
    "    return match.group(0) if match else np.nan\n",
    "\n",
    "def ensure_commune_keys(path):\n",
    "    \"\"\"Ajoute / v√©rifie code_commune et commune_std puis r√©√©crit le fichier.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_cols = len(df.columns)\n",
    "    before_rows = len(df)\n",
    "\n",
    "    # --- D√©tection de colonnes similaires existantes\n",
    "    code_col = next((c for c in df.columns if \"insee\" in c or \"code_commune\" in c), None)\n",
    "    name_col = next((c for c in df.columns if \"commune\" in c and \"std\" not in c), None)\n",
    "\n",
    "    # --- Cr√©ation / nettoyage du code_commune\n",
    "    if code_col:\n",
    "        df[\"code_commune\"] = df[code_col].apply(extract_code_commune)\n",
    "    else:\n",
    "        df[\"code_commune\"] = np.nan\n",
    "\n",
    "    # --- Cr√©ation / nettoyage du nom de commune\n",
    "    if name_col:\n",
    "        df[\"commune_std\"] = df[name_col].apply(normalize_commune_name)\n",
    "    else:\n",
    "        df[\"commune_std\"] = np.nan\n",
    "\n",
    "    # --- Rapport\n",
    "    valid_codes = df[\"code_commune\"].notna().sum()\n",
    "    valid_names = df[\"commune_std\"].notna().sum()\n",
    "    print(f\"üß© code_commune : {valid_codes} valeurs valides / {before_rows}\")\n",
    "    print(f\"üèôÔ∏è commune_std : {valid_names} valeurs valides / {before_rows}\")\n",
    "\n",
    "    # --- R√©√©criture directe du fichier\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"‚úÖ Fichier mis √† jour : {path.name} ({len(df.columns)} colonnes, {before_rows} lignes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"code_commune_valide\": valid_codes,\n",
    "        \"commune_std_valide\": valid_names,\n",
    "        \"total_lignes\": before_rows\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = ensure_commune_keys(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DES CL√âS COMMUNES ===\")\n",
    "display(df_summary)\n",
    "\n",
    "# --- Indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "avg_code_rate = round((df_summary[\"code_commune_valide\"] / df_summary[\"total_lignes\"]).mean() * 100, 2)\n",
    "avg_name_rate = round((df_summary[\"commune_std_valide\"] / df_summary[\"total_lignes\"]).mean() * 100, 2)\n",
    "\n",
    "print(f\"‚úÖ Moyenne de compl√©tude du code_commune : {avg_code_rate}%\")\n",
    "print(f\"‚úÖ Moyenne de compl√©tude du commune_std : {avg_name_rate}%\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© mis √† jour avec des cl√©s de jointure coh√©rentes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dca695-1ad8-4f49-9de7-b96841af3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 9 : CONTR√îLE ET COH√âRENCE DES DONN√âES ===\n",
    "# Objectif :\n",
    "#   - V√©rifier la coh√©rence logique des valeurs num√©riques\n",
    "#   - Supprimer les lignes incoh√©rentes (surfaces <= 0, valeurs fonci√®res n√©gatives, etc.)\n",
    "#   - Fournir un rapport clair et r√©√©crire les fichiers en place\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "def control_coherence_in_place(path):\n",
    "    \"\"\"V√©rifie et corrige les incoh√©rences de coh√©rence logique des donn√©es.\"\"\"\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de lecture sur {path.name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"=== {path.name} ===\")\n",
    "    before_rows = len(df)\n",
    "    anomalies = {}\n",
    "\n",
    "    # --- 1Ô∏è‚É£ Surfaces n√©gatives ou nulles\n",
    "    if \"surface_reelle_bati\" in df.columns:\n",
    "        anomalies[\"surface<=0\"] = (df[\"surface_reelle_bati\"] <= 0).sum(skipna=True)\n",
    "        df = df[df[\"surface_reelle_bati\"] > 0]\n",
    "\n",
    "    # --- 2Ô∏è‚É£ Valeurs fonci√®res n√©gatives ou nulles\n",
    "    if \"valeur_fonciere\" in df.columns:\n",
    "        anomalies[\"valeur<=0\"] = (df[\"valeur_fonciere\"] <= 0).sum(skipna=True)\n",
    "        df = df[df[\"valeur_fonciere\"] > 0]\n",
    "\n",
    "    # --- 3Ô∏è‚É£ Prix au m¬≤ irr√©alistes (si applicable)\n",
    "    if set([\"valeur_fonciere\", \"surface_reelle_bati\"]).issubset(df.columns):\n",
    "        df[\"prix_m2\"] = df[\"valeur_fonciere\"] / df[\"surface_reelle_bati\"]\n",
    "        anomalies[\"prix_m2<500\"] = (df[\"prix_m2\"] < 500).sum(skipna=True)\n",
    "        anomalies[\"prix_m2>50000\"] = (df[\"prix_m2\"] > 50000).sum(skipna=True)\n",
    "        df = df[(df[\"prix_m2\"] >= 500) & (df[\"prix_m2\"] <= 50000)]\n",
    "\n",
    "    # --- 4Ô∏è‚É£ Valeurs n√©gatives g√©n√©rales sur d‚Äôautres indicateurs\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_cols:\n",
    "        if col not in [\"valeur_fonciere\", \"surface_reelle_bati\", \"prix_m2\"]:\n",
    "            negatives = (df[col] < 0).sum(skipna=True)\n",
    "            if negatives > 0:\n",
    "                anomalies[f\"{col}<0\"] = negatives\n",
    "                df = df[df[col] >= 0]\n",
    "\n",
    "    # --- Calcul des suppressions\n",
    "    after_rows = len(df)\n",
    "    removed = before_rows - after_rows\n",
    "    pct_removed = round((removed / before_rows) * 100, 2) if before_rows > 0 else 0\n",
    "\n",
    "    # --- Sauvegarde du fichier propre\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\", sep=\",\")\n",
    "    print(f\"‚úÖ {removed} lignes incoh√©rentes supprim√©es ({pct_removed}%)\")\n",
    "    if anomalies:\n",
    "        print(\"üìä D√©tails anomalies d√©tect√©es :\", anomalies)\n",
    "    print(f\"   üî∏ Fichier mis √† jour : {path.name} ({after_rows} lignes restantes)\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return {\n",
    "        \"fichier\": path.name,\n",
    "        \"lignes_avant\": before_rows,\n",
    "        \"lignes_apres\": after_rows,\n",
    "        \"supprimees\": removed,\n",
    "        \"pct_supprimees\": pct_removed\n",
    "    }\n",
    "\n",
    "# --- Application √† toutes les bases clean\n",
    "results = []\n",
    "for f in files:\n",
    "    res = control_coherence_in_place(f)\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(\"\\nüìä === SYNTH√àSE DU CONTR√îLE DE COH√âRENCE ===\")\n",
    "display(df_summary)\n",
    "\n",
    "total_removed = df_summary[\"supprimees\"].sum()\n",
    "print(f\"üßæ Total de lignes incoh√©rentes supprim√©es : {total_removed}\")\n",
    "print(\"üéØ Tous les fichiers ont √©t√© nettoy√©s et mis √† jour sans duplication.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a99e5a-18cf-4c8f-8ba9-57d886d66682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPE 10 : V√âRIFICATION STRUCTURELLE ET FORMAT FINAL ===\n",
    "# Objectif :\n",
    "#   - V√©rifier la structure, le format et la propret√© finale de chaque fichier clean\n",
    "#   - Contr√¥ler encodage, s√©parateur, coh√©rence colonnes / lignes, taille et caract√®res parasites\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "# --- Fonction de d√©tection du s√©parateur\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- V√©rification structurelle\n",
    "def verify_structure(path):\n",
    "    report = {\n",
    "        \"fichier\": path.name,\n",
    "        \"encodage\": \"UTF-8\",\n",
    "        \"sep\": None,\n",
    "        \"colonnes\": 0,\n",
    "        \"lignes\": 0,\n",
    "        \"caract√®res_parasites\": False,\n",
    "        \"taille_ko\": round(os.path.getsize(path) / 1024, 2),\n",
    "        \"statut\": \"‚úÖ Conforme\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        report[\"sep\"] = sep\n",
    "\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "        report[\"colonnes\"] = len(df.columns)\n",
    "        report[\"lignes\"] = len(df)\n",
    "\n",
    "        # V√©rification des crit√®res de base\n",
    "        if report[\"colonnes\"] < 5 or report[\"lignes\"] < 10:\n",
    "            report[\"statut\"] = \"‚ö†Ô∏è Structure suspecte (trop peu de donn√©es)\"\n",
    "\n",
    "        # Recherche de caract√®res parasites\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "        if any(char in content for char in [\"\\x00\", \"\\x1f\", \"\\xa0\", \"\\u200b\", \"; ;\", \", ,\"]):\n",
    "            report[\"caract√®res_parasites\"] = True\n",
    "            report[\"statut\"] = \"‚ö†Ô∏è Caract√®res parasites d√©tect√©s\"\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        report[\"encodage\"] = \"‚ö†Ô∏è Non UTF-8\"\n",
    "        report[\"statut\"] = \"‚ö†Ô∏è Encodage incorrect\"\n",
    "    except Exception as e:\n",
    "        report[\"statut\"] = f\"‚ùå Erreur de lecture : {e}\"\n",
    "\n",
    "    return report\n",
    "\n",
    "# --- Application de la v√©rification √† toutes les bases\n",
    "results = []\n",
    "for f in files:\n",
    "    res = verify_structure(f)\n",
    "    results.append(res)\n",
    "    print(f\"üîé V√©rification {f.name} ‚Üí {res['statut']} ({res['colonnes']} colonnes, {res['lignes']} lignes)\")\n",
    "\n",
    "# --- Synth√®se globale\n",
    "df_final = pd.DataFrame(results)\n",
    "print(\"\\nüìä === RAPPORT FINAL DE VALIDATION ===\")\n",
    "display(df_final)\n",
    "\n",
    "# --- Statistiques globales\n",
    "nb_total = len(df_final)\n",
    "nb_valid = (df_final[\"statut\"].str.contains(\"‚úÖ\")).sum()\n",
    "nb_warnings = (df_final[\"statut\"].str.contains(\"‚ö†Ô∏è\")).sum()\n",
    "nb_errors = (df_final[\"statut\"].str.contains(\"‚ùå\")).sum()\n",
    "\n",
    "print(f\"\\n‚úÖ Fichiers conformes : {nb_valid}/{nb_total}\")\n",
    "print(f\"‚ö†Ô∏è Fichiers √† v√©rifier : {nb_warnings}/{nb_total}\")\n",
    "print(f\"‚ùå Fichiers en erreur : {nb_errors}/{nb_total}\")\n",
    "\n",
    "if nb_errors == 0 and nb_warnings == 0:\n",
    "    print(\"\\nüéØ Tous les fichiers sont pr√™ts pour l‚Äôanalyse finale et la fusion inter-bases !\")\n",
    "else:\n",
    "    print(\"\\nüìå Certains fichiers n√©cessitent une v√©rification manuelle avant l‚Äô√©tape d‚Äôanalyse.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca4f76-19d3-441f-a057-74c34eb013bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === R√âCAPITULATIF GLOBAL DES BASES APR√àS NETTOYAGE ===\n",
    "# Objectif :\n",
    "#   - Afficher nombre de lignes / colonnes par base\n",
    "#   - Montrer un aper√ßu (5 lignes √ó 8 colonnes)\n",
    "#   - V√©rifier coh√©rence et propret√© globale\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Lecture et r√©sum√© global\n",
    "summaries = []\n",
    "\n",
    "for path in files:\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "        # Informations de base\n",
    "        lignes, colonnes = df.shape\n",
    "        nan_rate = round(df.isna().mean().mean() * 100, 2)\n",
    "        numeric_cols = len(df.select_dtypes(include=[\"number\"]).columns)\n",
    "        text_cols = len(df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "        summaries.append({\n",
    "            \"Fichier\": path.name,\n",
    "            \"Lignes\": lignes,\n",
    "            \"Colonnes\": colonnes,\n",
    "            \"Colonnes num√©riques\": numeric_cols,\n",
    "            \"Colonnes texte\": text_cols,\n",
    "            \"Taux NaN (%)\": nan_rate\n",
    "        })\n",
    "\n",
    "        print(f\"=== {path.name} ===\")\n",
    "        print(f\"üîπ {lignes} lignes | {colonnes} colonnes\")\n",
    "        print(f\"   üìä {numeric_cols} num√©riques | {text_cols} texte | NaN moyen : {nan_rate}%\")\n",
    "        print(\"   üî∏ Aper√ßu des premi√®res colonnes :\")\n",
    "        display(df.head(5).iloc[:, :min(8, colonnes)])  # 5 lignes √ó 8 colonnes max\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de lire {path.name} : {e}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# --- Synth√®se finale\n",
    "print(\"\\nüìä === SYNTH√àSE GLOBALE DES BASES ===\")\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "display(df_summary.sort_values(\"Fichier\").reset_index(drop=True))\n",
    "\n",
    "# --- Quelques indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "total_lignes = df_summary[\"Lignes\"].sum()\n",
    "total_colonnes = df_summary[\"Colonnes\"].sum()\n",
    "\n",
    "print(f\"\\nüìà Nombre total de bases : {nb_total}\")\n",
    "print(f\"üìä Total de lignes (toutes bases confondues) : {total_lignes:,}\")\n",
    "print(f\"üß± Total de colonnes cumul√©es : {total_colonnes}\")\n",
    "print(\"üéØ V√©rification visuelle effectu√©e : tu peux maintenant confirmer que tout est coh√©rent avant les fusions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c3233-b551-4e5c-9660-7033165f0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === R√âCAPITULATIF GLOBAL DES BASES APR√àS NETTOYAGE ===\n",
    "# Objectif :\n",
    "#   - Afficher nombre de lignes / colonnes par base\n",
    "#   - Montrer un aper√ßu (5 lignes √ó 8 colonnes)\n",
    "#   - V√©rifier coh√©rence et propret√© globale\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Dossier contenant les fichiers nettoy√©s\n",
    "CLEAN_DIR = Path(\"data/clean\")\n",
    "files = sorted(CLEAN_DIR.glob(\"*.csv\"))\n",
    "print(f\"üìÇ {len(files)} fichiers d√©tect√©s dans {CLEAN_DIR}\\n\")\n",
    "\n",
    "def detect_separator(path):\n",
    "    \"\"\"D√©tecte automatiquement le s√©parateur probable (, ou ;)\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        sample = f.read(4096)\n",
    "    try:\n",
    "        sep = csv.Sniffer().sniff(sample).delimiter\n",
    "    except Exception:\n",
    "        sep = \";\"\n",
    "    return sep\n",
    "\n",
    "# --- Lecture et r√©sum√© global\n",
    "summaries = []\n",
    "\n",
    "for path in files:\n",
    "    try:\n",
    "        sep = detect_separator(path)\n",
    "        df = pd.read_csv(path, sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "        # Informations de base\n",
    "        lignes, colonnes = df.shape\n",
    "        nan_rate = round(df.isna().mean().mean() * 100, 2)\n",
    "        numeric_cols = len(df.select_dtypes(include=[\"number\"]).columns)\n",
    "        text_cols = len(df.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "        summaries.append({\n",
    "            \"Fichier\": path.name,\n",
    "            \"Lignes\": lignes,\n",
    "            \"Colonnes\": colonnes,\n",
    "            \"Colonnes num√©riques\": numeric_cols,\n",
    "            \"Colonnes texte\": text_cols,\n",
    "            \"Taux NaN (%)\": nan_rate\n",
    "        })\n",
    "\n",
    "        print(f\"=== {path.name} ===\")\n",
    "        print(f\"üîπ {lignes} lignes | {colonnes} colonnes\")\n",
    "        print(f\"   üìä {numeric_cols} num√©riques | {text_cols} texte | NaN moyen : {nan_rate}%\")\n",
    "        print(\"   üî∏ Aper√ßu des premi√®res colonnes :\")\n",
    "        display(df.head(5).iloc[:, :min(8, colonnes)])  # 5 lignes √ó 8 colonnes max\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible de lire {path.name} : {e}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# --- Synth√®se finale\n",
    "print(\"\\nüìä === SYNTH√àSE GLOBALE DES BASES ===\")\n",
    "df_summary = pd.DataFrame(summaries)\n",
    "display(df_summary.sort_values(\"Fichier\").reset_index(drop=True))\n",
    "\n",
    "# --- Quelques indicateurs globaux\n",
    "nb_total = len(df_summary)\n",
    "total_lignes = df_summary[\"Lignes\"].sum()\n",
    "total_colonnes = df_summary[\"Colonnes\"].sum()\n",
    "\n",
    "print(f\"\\nüìà Nombre total de bases : {nb_total}\")\n",
    "print(f\"üìä Total de lignes (toutes bases confondues) : {total_lignes:,}\")\n",
    "print(f\"üß± Total de colonnes cumul√©es : {total_colonnes}\")\n",
    "print(\"üéØ V√©rification visuelle effectu√©e : tu peux maintenant confirmer que tout est coh√©rent avant les fusions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5559b-5b46-47e4-b792-42c33ac71e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
